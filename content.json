{"meta":{"title":"Notes","subtitle":"Quick Notes","description":null,"author":"Funky Gao","url":"http://funkygao.github.io"},"pages":[{"title":"tags","date":"2017-05-15T23:20:30.000Z","updated":"2017-05-19T00:26:43.000Z","comments":true,"path":"tags/index.html","permalink":"http://funkygao.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2 phase commit failures","slug":"2PC-failures","date":"2017-06-14T08:34:59.000Z","updated":"2017-06-14T09:15:52.000Z","comments":true,"path":"2017/06/14/2PC-failures/","link":"","permalink":"http://funkygao.github.io/2017/06/14/2PC-failures/","excerpt":"","keywords":[],"text":"Node Failure Models fail-stopcrash and never recover fail-recovercrash and later recover byzantine failure Cases2 phase commit，n个节点，那么需要3n个消息交换 coordinator发送proposal后crash 有的node收到，有的没收到 收到Proposal的node被block forever，它可能已经vote commit了不能简单地timeout/abort，因为coordinator可能随时recover并启动phase2 commit这个txn就只能blocked by coordinator，cannot make any progress 解决办法引入coordinator的watchdog机制，它发现coordinator crash后，接管Phase1. 先询问每个participants，已经vote commit还是vote abort还是没有votePhase2. 通知每个participant Commit/Abort但仍有局限，如果有个participant crash了，那么Phase1无法确认 worse casecoordinator本身也是participant","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"graph database","slug":"graph-database","date":"2017-06-14T06:33:50.000Z","updated":"2017-06-14T06:34:26.000Z","comments":true,"path":"2017/06/14/graph-database/","link":"","permalink":"http://funkygao.github.io/2017/06/14/graph-database/","excerpt":"","keywords":[],"text":"graph indexgraph operations","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"HDD Stats Q1 2017","slug":"HDD-Stats-Q1-2017","date":"2017-06-14T03:14:03.000Z","updated":"2017-06-14T06:05:54.000Z","comments":true,"path":"2017/06/14/HDD-Stats-Q1-2017/","link":"","permalink":"http://funkygao.github.io/2017/06/14/HDD-Stats-Q1-2017/","excerpt":"","keywords":[],"text":"Backblaze Storage Pod把45块SATA盘存放到一台4U机器里，其中15块搞RAID6，ext4，可以空间是裸盘空间的87%，所有访问通过tomcat HTTPS没有iSCSI，没有NFS，没有Fibre Channel180TB成本$9,305，即每GB $0.0517 Backblze Vaults cloud backup service99.999999% annual durability已经存储150PB，由1000个Storage Pod组成，40,000块盘，每天有10块盘损坏 每个Vault由20个Storage Pod组成，每个Pod有45块盘，即每个Vault有900块盘，一块盘如果是4TB，那么每个Vault可以存3.6PB每个磁盘使用ext4文件系统，每个Vault有个7位数字id，例如555-1021，前3位代表data center id，后4位是vault id有个类似name service的服务，client备份前先request name service获取vault id，之后client直接与相应的vault进行backup IO(https) 每个文件被分成20 shards = 17 data shars + 3 parity shards，存放在ext4每个shard有checksum，如果损坏，可以从其他17个shards恢复如果某个Vault有1个pod crash了，backup write的parity会变成2，如果3个pod坏了，那么也可以写，但parity就不存在了，如果此时再坏一个pod，数据无法恢复了 Referenceshttps://www.backblaze.com/blog/vault-cloud-storage-architecture/https://www.backblaze.com/blog/hard-drive-failure-rates-q1-2017/","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"OpenStack storage","slug":"OpenStack-storage","date":"2017-06-14T02:43:35.000Z","updated":"2017-06-14T03:10:55.000Z","comments":true,"path":"2017/06/14/OpenStack-storage/","link":"","permalink":"http://funkygao.github.io/2017/06/14/OpenStack-storage/","excerpt":"","keywords":[],"text":"对象存储 Swift 块存储 EBS/RBD Cinder, Ceph, Sheepdog RDS Trove","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"},{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Reed-Solomon erasure code","slug":"erasure-coding","date":"2017-06-14T01:20:19.000Z","updated":"2017-06-14T05:28:03.000Z","comments":true,"path":"2017/06/14/erasure-coding/","link":"","permalink":"http://funkygao.github.io/2017/06/14/erasure-coding/","excerpt":"","keywords":[],"text":"Intro纠错码在RAID、备份、冷数据、历史数据存储方面使用广泛也有人利用它把一份数据分散到多个cloud provider(e,g. S3,Azure,Rackspace)，消除某个供应商的依赖: cloud of cloud Usage一份文件，大小x，分成n个数据块+k个校验块，能容忍任意k个数据块或者校验块错误，即至少要n个块是有效的1234567891011121314151617181920212223242526$encode -data 4 -par 2 README.md # n=4 k=2Opening README.mdFile split into 6 data+parity shards with 358 bytes/shard.Writing to README.md.0Writing to README.md.1Writing to README.md.2Writing to README.md.3Writing to README.md.4 # README.md.4 and README.md5 are parity shardsWriting to README.md.5$rm -f README.md.4 README.md.5 # remove the 2 parity shards$decode -out x README.mdOpening README.md.0Opening README.md.1Opening README.md.2Opening README.md.3Opening README.md.4Error reading file open README.md.4: no such file or directoryOpening README.md.5Error reading file open README.md.5: no such file or directoryVerification failed. Reconstructing dataWriting data to x$diff x README.md$ # all the same Algorithm文件内容‘ABCDEFGHIJKLMNOP’，4+2给定n和k，encoding矩阵是不变的 Referenceshttp://pages.cs.wisc.edu/~yadi/papers/yadi-infocom2013-paper.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"},{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"kafka ext4","slug":"kafka-ext4","date":"2017-06-12T02:03:31.000Z","updated":"2017-06-12T03:05:10.000Z","comments":true,"path":"2017/06/12/kafka-ext4/","link":"","permalink":"http://funkygao.github.io/2017/06/12/kafka-ext4/","excerpt":"","keywords":[],"text":"big file我目前使用ext4作为kafka存储，每个segment 1GB Ext4针对大文件顺序访问的主要优化 replace indirect blocks with extent(盘区) ext3采用多层间接地址映射，操作大文件时产生很多随机访问 one extra block read(seek) every 1024 blocks(4MB) ext4 extent是一组连续的数据块 123456789101112+---------+--------+----------+| logical | length | physical |+---------+--------+----------++ 0 | 1000 | 200 |+---------+--------+----------+type ext4_extent struct &#123; block uint32 // first logical block extent covers len uint16 // number of blocks covered by extent start_hi uint16 // high 16 bits of physical block start uint32 // low 32 bits of physical block&#125; inode preallocinode有很好的locality，同一目录下文件的inode尽量存放一起，加速了目录寻址性能 Multiblock Allocator(MBAlloc) ext3每次只能分配一个4KB(block size)的block ext4支持一次性分配多个block 延迟分配defer block allocation to writeback time online defregmentation e4defrag12345allocate more contiguous blocks in a temporary inoderead a data block from the original inodemove the corresponding block number from the temporary inode to the original inodewrite out the page Benchmark Summary","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"fs atomic and ordering","slug":"fs-atomic-and-ordering","date":"2017-06-12T01:46:26.000Z","updated":"2017-06-12T03:37:17.000Z","comments":true,"path":"2017/06/12/fs-atomic-and-ordering/","link":"","permalink":"http://funkygao.github.io/2017/06/12/fs-atomic-and-ordering/","excerpt":"","keywords":[],"text":"应用层通过POSIX系统调用接口访问文件系统，但POSIX只规定了结果，底层的guarantee并没有提供，开发者只能去猜。不同的文件系统的实现不同，那些不“可见”的部分，会造成很大的差异；同时，每个文件系统又有些配置，这些配置也造成理解的困难。 Example12write(f1, &quot;pp&quot;)write(f2, &quot;qq&quot;) 如果write不是原子的，那么可能造成文件大小变化了，但内容没变(State#A)乱序造成State#C size atomicity content atomicity calls out of order Facts atomicity atomic single-sector overwrite目前绝大部分文件系统提供了atomic single-sector overwrite，主要是底层硬盘就已经提供了 atomic append 需要原子地修改inode+data block ext3/ext4/reiserfs writeback不支持 multi-block append目前绝大部分文件系统没有支持 rename/link这类directory operation，基本上是原子的 ordering","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"Wake on LAN","slug":"WOL","date":"2017-06-09T00:11:10.000Z","updated":"2017-06-09T00:45:20.000Z","comments":true,"path":"2017/06/09/WOL/","link":"","permalink":"http://funkygao.github.io/2017/06/09/WOL/","excerpt":"","keywords":[],"text":"WOL是个链路层协议，通过发送UDP广播 magic packet远程power upBIOS可以打开/关闭该功能 1234type MagicPacket struct &#123; header [6]byte // 0xFF payload [16]MACAddress&#125;","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Swinging Door Trending","slug":"SDT","date":"2017-06-08T06:52:01.000Z","updated":"2017-06-09T00:46:57.000Z","comments":true,"path":"2017/06/08/SDT/","link":"","permalink":"http://funkygao.github.io/2017/06/08/SDT/","excerpt":"","keywords":[],"text":"SDT旋转门流式压缩，有损算法，对于变化频率不高的时间序列数据，压缩比很高，但可能丢到峰值、波谷数据通过一条由起点和终点确定的直线代替一系列连续数据点通过线性差值“还原”数据，pref sample &lt;–插值–&gt; next sample vs facebook gorillahttp://www.vldb.org/pvldb/vol8/p1816-teller.pdf","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"socket recv buffer","slug":"socket-recv-buffer","date":"2017-06-08T03:03:16.000Z","updated":"2017-06-08T03:33:53.000Z","comments":true,"path":"2017/06/08/socket-recv-buffer/","link":"","permalink":"http://funkygao.github.io/2017/06/08/socket-recv-buffer/","excerpt":"","keywords":[],"text":"1BDP * 4 / 3 use iperf for testing12iperf -s -w 200K # serveriperf -c localhost # client 1234ethtool -k eth0 # segment offload makes tcpdump see a packet bigger than MTUnet.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_congestion_controlnet.ipv4.tcp_moderate_rcvbuf","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"golang pkg github pull request","slug":"golang-pkg-github-pull-request","date":"2017-06-08T00:33:57.000Z","updated":"2017-06-08T00:53:08.000Z","comments":true,"path":"2017/06/08/golang-pkg-github-pull-request/","link":"","permalink":"http://funkygao.github.io/2017/06/08/golang-pkg-github-pull-request/","excerpt":"","keywords":[],"text":"12345git remote add upstream https://github.com/funkygao/myrepo.gitgit fetch upstreamgit checkout -b mybranchgit commit -agit push upstream mybranch","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"ElasticSearch internals","slug":"ElasticSearch","date":"2017-06-05T05:35:53.000Z","updated":"2017-06-13T08:24:20.000Z","comments":true,"path":"2017/06/05/ElasticSearch/","link":"","permalink":"http://funkygao.github.io/2017/06/05/ElasticSearch/","excerpt":"","keywords":[],"text":"BasicsStorage Model一种特殊的LSM Tree123456ES LSM======== ==========translog WALbuffer MemTablesegment SSTablemerge merge &amp; compaction Defaults123456node.master: truenode.data: trueindex.number_of_shards: 5index.number_of_replicas: 1path.data: /path/to/data1,/path/to/data2http.max_content_length: 100mb Indexing translog(WAL) 每个shard一个translog，即每个index一个translog 它提供实时的CRUD by docID，先translog然后再segments 默认5s刷盘 translog存放在path.data，可以stripe，但存在与segments竞争资源问题1234$ls nodes/0/indices/twitter/1/drwxr-xr-x 3 funky wheel 102B 6 8 17:23 _statedrwxr-xr-x 5 funky wheel 170B 6 8 17:23 indexdrwxr-xr-x 3 funky wheel 102B 6 8 17:23 translog refresh scheduled periodically(1s)，也可手工 /_refresh 在此触发merge逻辑 refresh后，那么在此之前的所有变更就可以搜索了，in-memory buffer的数据是不能搜索的 产生新segment，但不fsync(close不会触发fsync)，然后open(the new segment) 不保证durability，那是由flush保证的 in-memory buffer清除 flush 30s by default，会触发commit Any docs in the in-memory buffer are written to a new segment The buffer is cleared A commit point is written to diskcommit point is a file that contains list of segments ready for search The filesystem cache is flushed with an fsync The old translog is deleted commit 把还没有fsync的segments，一一fsync merge policy tiered(default) log_byte_size log_doc 可能把committed和uncommitted segments一起merge 异步，不影响indexing/query 合并后，fsync the merged big segment，之前的small segments被删除 ES内部有throttle机制控制merge进度，防止它占用过多资源: 20MB/s updatedelete, then insert。mysql内部的变长字段update也是这么实现的 Query local search1234translog.search(q)for s in range segments &#123; s.search(q)&#125; deep paginationsearch(from=50000, size=10)，那么每个shard会创建一个优先队列，队列大小=50010，从每个segment里取结果，直到填满而coordinating node需要创建的优先队列 number_of_shards * 50010 利用scan+scroll可以批量取数据(sorting disabled)，例如在reindex时使用 ClusterIntro所有node彼此相连，n*(n-1)个连接shard = MurMurHash3(document_id) % (num_of_primary_shards) Node1节点用数据的_id计算出数据应该存储在shard0上，通过cluster state信息发现shard0的主分片在Node3节点上，Node1转发请求数据给Node3,Node3完成数据的索引Node3并行转发(PUSH model replication)数据给分配有shard0的副本分片Node1和Node2上。当收到任一节点汇报副本分片数据写入成功以后，Node3即返回给初始的接受节点Node1，宣布数据写入成功。Node1成功返回给客户端。 Scale Rebalance123456789101112131415// 查看shard分布情况curl -XGET http://localhost:9200/_cat/shards// 手工rebalancecurl-XPOST &apos;http://localhost:9200/_cluster/reroute&apos; -d&apos;&#123; &quot;commands&quot;: [ &#123; &quot;move&quot;: &#123; &quot;index&quot;: &quot;mylog-2016-02-08&quot;, &quot;shard&quot;: 6, &quot;from_node&quot;: &quot;192.168.0.1&quot;, &quot;to_node&quot;: &quot;192.168.0.2&quot; &#125; &#125; ]&#125;&apos; 新节点加入过程12345678910// get master node and eligible master nodesfor host = range discovery.zen.ping.unicast.hosts &#123; PingResponse = ZendPing.send(host)&#125;send(&apos;internal:discovery/zen/join&apos;) to mastermaster.reply(&apos;internal:discovery/zen/join/validate&apos;)// 2PC joinmaster.update(ClusterState) and broadcast to all nodes, and wait for minimum_master_nodes ackClusterState change committed and confirmation sent Master fault detection默认，每个node每1s ping master，ping_timeout=30s，ping_retries=3失败后，会触发new master election Concurrency每个document有version，乐观锁 Consistency quorum(default) one all Shardallocation1234weightindex(node, index) = indexBalance * (node.numShards(index) – avgShardsPerNode(index))weightnode(node, index) = shardBalance * (node.numShards() – avgShardsPerNode)weightprimary(node, index) = primaryBalance * (node.numPrimaries() – avgPrimariesPerNode)weight(node, index) = weightindex(node, index) + weightnode(node, index) + weightprimary(node, index) 如果计算最后的weight(node, index)大于threshold， 就会发生shard迁移。 在一个已经创立的集群里，shard的分布总是均匀的。但是当你扩容节点的时候，你会发现，它总是先移动replica shard到新节点。这样就导致新节点全部分布的全是副本，主shard几乎全留在了老的节点上。 cluster.routing.allocation.balance参数，比较难找到合适的比例。 初始化1234master通过ClusterState分配一个新shardnode初始化一个空shard，并notify mastermaster mark the shard as startedif this is the first shard with a specific id, it is marked as primary Consensuszen discovery(unicast/multicast)，存在脑裂问题，没有去解决，而是通过3台专用master机器，优化master逻辑，减少由于no reponse造成的partition可能性12node1, node2(master), node32-3不通，但1-2, 2-3都通，minimum_master_nodes=2，node3会重新选举自己成为master，而1同意了，2个master 默认ping_interval=1s ping_timeout=3s ping_retries=3 join_timeout=20*ping_interval 没有使用Paxos(zk)的原因: This, to me, is at the heart of our approach to resiliency.Zookeeper, an excellent product, is a separate external system, with its own communication layer.By having the discovery module in Elasticsearch using its own infrastructure, specifically the communication layer, it means that we can use the “liveness” of the cluster to assess its health.Operations happen on the cluster all the time, reads, writes, cluster state updates.By tying our assessment of health to the same infrastructure, we can actually build a more reliable more responsive system.We are not saying that we won’t have a formal Zookeeper integration for those who already use Zookeeper elsewhere. But this will be next to a hardened, built in, discovery module. ClusterStatemaster负责更改，并广播到机器的每个节点，每个节点本地保存如果有很多index，很多fields，很多shard，很多node，那么它会很大，因此它提供了增量广播机制和压缩 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;version&quot; : 11, &quot;master_node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;blocks&quot; : &#123; &#125;, &quot;nodes&quot; : &#123; &quot;sIh5gQcFThCcz3SO6txvvQ&quot; : &#123; &quot;name&quot; : &quot;Max&quot;, &quot;transport_address&quot; : &quot;inet[/162.245.23.194:9301]&quot;, &quot;attributes&quot; : &#123; &#125; &#125;, &quot;-mq1SRuuQoeEq-3S8SdHqw&quot; : &#123; &quot;name&quot; : &quot;Llyron&quot;, &quot;transport_address&quot; : &quot;inet[/162.245.23.194:9300]&quot;, &quot;attributes&quot; : &#123; &#125; &#125; &#125;, &quot;metadata&quot; : &#123; &quot;templates&quot; : &#123; &#125;, &quot;indices&quot; : &#123; &quot;blog&quot; : &#123; &quot;state&quot; : &quot;open&quot;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;uuid&quot; : &quot;UQMz5vbXSBqFU_8U3u4gYQ&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;number_of_shards&quot; : &quot;5&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;1030099&quot; &#125; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;user&quot; : &#123; &quot;properties&quot; : &#123; &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125; &#125; &#125; &#125;, &quot;aliases&quot; : [ ] &#125; &#125; &#125;, &quot;routing_table&quot; : &#123; &quot;indices&quot; : &#123; &quot;blog&quot; : &#123; &quot;shards&quot; : &#123; &quot;4&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;0&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;3&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;1&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;2&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ] &#125; &#125; &#125; &#125;, &quot;routing_nodes&quot; : &#123; &quot;unassigned&quot; : [ ], &quot;nodes&quot; : &#123; &quot;sIh5gQcFThCcz3SO6txvvQ&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;-mq1SRuuQoeEq-3S8SdHqw&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ] &#125; &#125;, &quot;allocations&quot; : [ ]&#125; Referenceshttps://www.elastic.co/blog/resiliency-elasticsearchhttps://github.com/elastic/elasticsearch/issues/2488http://blog.mikemccandless.com/2011/02/visualizing-lucenes-segment-merges.htmlhttp://blog.trifork.com/2011/04/01/gimme-all-resources-you-have-i-can-use-them/https://github.com/elastic/elasticsearch/issues/10708","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Kafka vs Kinesis vs Redis","slug":"Kafka-vs-Kinesis-Redis","date":"2017-06-05T03:15:08.000Z","updated":"2017-06-05T03:29:07.000Z","comments":true,"path":"2017/06/05/Kafka-vs-Kinesis-Redis/","link":"","permalink":"http://funkygao.github.io/2017/06/05/Kafka-vs-Kinesis-Redis/","excerpt":"","keywords":[],"text":"vs KinesisUS East region，要支持10万/秒的吞吐量，Kinesis需要的费用是4787美元/月 https://www.quora.com/Amazon-Kinesis-versus-Apache-Kafka-which-of-them-is-the-most-proven-and-high-performance-oriented vs Redis PubSub redis OOM kafka replay messages, horizontal scale, HA, async Pub","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"facebook Mystery Machine","slug":"facebook-Mystery-Machine","date":"2017-06-05T01:49:00.000Z","updated":"2017-06-05T02:58:05.000Z","comments":true,"path":"2017/06/05/facebook-Mystery-Machine/","link":"","permalink":"http://funkygao.github.io/2017/06/05/facebook-Mystery-Machine/","excerpt":"","keywords":[],"text":"Data Flowconsistent sampling 1local log --&gt; Scribe --&gt; Hive --&gt; UberTrace --&gt; UI Log Schema1234request idhost idhost-local timestampunique event label(event name, task name) Timestamp Normalize 不考虑local clock skew 假设client/server间的RTT是对称的 123456789101112131415Client Server 1|------------&gt;| | |--+1.1 | | | logic | |&lt;-+1.2 2|&lt;------------|1.2 - 1.1 = 0.12 - 1 = 1.0RTT = (1.0 - 0.1)/2 = 0.45clock(1.1) = 1 + 0.45 = 1.45clock(1.2) = 1.45 + 0.1 = 1.55RTT是个经验值，根据大量的trace后稳定下来: 使用最小值","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"storm acker","slug":"storm-ack","date":"2017-06-02T01:03:52.000Z","updated":"2017-06-02T06:36:42.000Z","comments":true,"path":"2017/06/02/storm-ack/","link":"","permalink":"http://funkygao.github.io/2017/06/02/storm-ack/","excerpt":"","keywords":[],"text":"Acker对于Spout产生的每一个tuple, storm都会进行跟踪，利用RotatingMap存放内存，但有保护措施，不会打爆 当Spout触发fail动作时，storm不会自动重发失败的tuple，只是向Spout发送fail消息，触发Spout.fail回调，真正的重发需要在Spout.fail里实现 tuple tree，中间任意一个edge fail，会理解触发Spout的fail，但后面的Bolt的执行不受影响。做无用功？ Spout Executor Bolt Executor","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"monkey patch golang","slug":"monkey-patch-golang","date":"2017-06-02T00:08:25.000Z","updated":"2017-06-02T00:09:22.000Z","comments":true,"path":"2017/06/02/monkey-patch-golang/","link":"","permalink":"http://funkygao.github.io/2017/06/02/monkey-patch-golang/","excerpt":"","keywords":[],"text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( &quot;syscall&quot; &quot;unsafe&quot;)func a() int &#123; return 1 &#125;func b() int &#123; return 2 &#125;func getPage(p uintptr) []byte &#123; return (*(*[0xFFFFFF]byte)(unsafe.Pointer(p &amp; ^uintptr(syscall.Getpagesize()-1))))[:syscall.Getpagesize()]&#125;func rawMemoryAccess(b uintptr) []byte &#123; return (*(*[0xFF]byte)(unsafe.Pointer(b)))[:]&#125;func assembleJump(f func() int) []byte &#123; funcVal := *(*uintptr)(unsafe.Pointer(&amp;f)) return []byte&#123; 0x48, 0xC7, 0xC2, byte(funcVal &gt;&gt; 0), byte(funcVal &gt;&gt; 8), byte(funcVal &gt;&gt; 16), byte(funcVal &gt;&gt; 24), // MOV rdx, funcVal 0xFF, 0x22, // JMP rdx &#125;&#125;func replace(orig, replacement func() int) &#123; bytes := assembleJump(replacement) functionLocation := **(**uintptr)(unsafe.Pointer(&amp;orig)) window := rawMemoryAccess(functionLocation) page := getPage(functionLocation) syscall.Mprotect(page, syscall.PROT_READ|syscall.PROT_WRITE|syscall.PROT_EXEC) copy(window, bytes)&#125;func main() &#123; replace(a, b) print(a())&#125; Referencehttps://software.intel.com/en-us/articles/introduction-to-x64-assemblyhttps://www.hopperapp.com/","categories":[],"tags":[{"name":"hacking","slug":"hacking","permalink":"http://funkygao.github.io/tags/hacking/"}]},{"title":"KafkaSpout","slug":"KafkaSpout","date":"2017-06-01T10:04:04.000Z","updated":"2017-06-02T06:14:53.000Z","comments":true,"path":"2017/06/01/KafkaSpout/","link":"","permalink":"http://funkygao.github.io/2017/06/01/KafkaSpout/","excerpt":"","keywords":[],"text":"configTopology config TOPOLOGY_WORKERS 整个topology在所有节点上的java进程总数 例如，设置成25，parallelism=150，那么每个worker进程会创建150/25=6个线程执行task TOPOLOGY_ACKER_EXECUTORS = 20 不设或者为null，it=TOPOLOGY_WORKERS，即one acker task per worker 设置为0，表示turn off ack/reliability TOPOLOGY_MAX_SPOUT_PENDING = 5000 123(defn executor-max-spout-pending [storm-conf num-tasks] (let [p (storm-conf TOPOLOGY-MAX-SPOUT-PENDING)] (if p (* p num-tasks)))) max in-flight(not ack or fail) spout tuples on a single spout task at once 如果不指定，默认是1 TOPOLOGY_BACKPRESSURE_ENABLE = false TOPOLOGY_MESSAGE_TIMEOUT_SECS30s by default KafkaSpout config123fetchSizeBytes = 1024 * 1024 * 2 // 1048576=1M by default FetchRequestfetchMaxWait = 10000 // by defaultforceFromStart = false emit/ack/fail flow1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class PartitionManager &#123; SortedSet&lt;Long&gt; _pending, failed = new TreeSet(); LinkedList&lt;MessageAndRealOffset&gt; _waitingToEmit = new LinkedList(); func EmitState next(SpoutOutputCollector collector) &#123; if (this._waitingToEmit.isEmpty()) &#123; // 如果内存里数据都发出，就调用kafka consumer一次性批量填充内存_waitingToEmit // 填充时，如果发现failed里有东西，那么就从head of failed(offset) FetchRequest: 重发机制 this.fill(); &#125; // 从LinkedList _waitingToEmit里取一条消息 MessageAndRealOffset toEmit = (MessageAndRealOffset)this._waitingToEmit.pollFirst(); // emit时指定了messageID // BasicBoltExecutor.execute会通过template method自动执行_collector.getOutputter().ack(input) // 即KafkaSpout.ack -&gt; PartitionManager.ack collector.emit(tup, new PartitionManager.KafkaMessageId(this._partition, toEmit.offset)); // 该tuple处于pending state &#125; // Note: a tuple will be acked or failed by the exact same Spout task that created it func ack(Long offset) &#123; this._pending.remove(offset) &#125; func fail(Long offset) &#123; this.failed.add(offset); // kafka consumer会reset offset to the failed msg，重新消费 &#125;&#125;class TopologyBuilder &#123; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125;&#125;class BasicBoltExecutor &#123; public void execute(Tuple input) &#123; _collector.setContext(input); try &#123; _bolt.execute(input, _collector); _collector.getOutputter().ack(input); &#125; catch(FailedException e) &#123; _collector.getOutputter().fail(input); &#125; &#125;&#125; Bolt ackKafkaSpout产生的每个tuple，Bolt必须进行ack，否则30s后KafkaSpout会认为emitted tuple tree not fully processed，进行重发123456class MyBolt &#123; public void execute(Tuple tuple) &#123; _collector.emit(new Values(foo, bar)) _collector.ack(tuple) &#125;&#125; OOM如果消息处理一直不ack，累计的unacked msg越来越多，会不会OOM?NOKafkaSpout只保留offset，不会保存每条emitted but no ack/fail msg spout throttle1.0.0之前，只能用TOPOLOGY_MAX_SPOUT_PENDING控制但这个参数很难控制，它有一些与其他参数配合使用才能生效的机制，而且如果使用Trident语义又完全不同1.0.0之后，可以通过backpressure Storm messaging intra-workerDisruptor inter-worker0MQ/Netty Referenceshttp://www.michael-noll.com/blog/2013/06/21/understanding-storm-internal-message-buffers/http://jobs.one2team.com/apache-storms/http://woodding2008.iteye.com/blog/2335673","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"aliware","slug":"aliware","date":"2017-06-01T07:34:25.000Z","updated":"2017-06-01T08:37:15.000Z","comments":true,"path":"2017/06/01/aliware/","link":"","permalink":"http://funkygao.github.io/2017/06/01/aliware/","excerpt":"","keywords":[],"text":"https://www.aliyun.com/aliware EDAS Enterprise Distributed Application Service RPC framework + elasticjob + qconf + GTS + Dapper + autoscale 鉴权数据下放到服务机器，避免性能瓶颈 MQ DRDS TDDL proxy avg() =&gt; sum()/count() 扩容，切换时client可能会失败，需要client retry 通过/ TDDL: XXX/ sql注释来实现特定SQL路由规则 可以冷热数据分离 ARMSstreaming processing，具有nifi式visualized workflow editor GTS 1234567@GtsTransaction(timeout=1000*60)public void transfer(DataSource db1, DataSource db2) &#123; // 强一致，但涉及的事务比较大时，性能下降非常明显 // 通过soft state MQ的最终一致性吞吐量要好的多 db1.getConnection().execute(&quot;sql1&quot;) db2.getConnection().execute(&quot;sql2&quot;)&#125; SchedulerX CSBcloud service bus，相当于api gateway，包含协议转换","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"TiDB KV Mapping","slug":"TiDB-KV-Mapping","date":"2017-06-01T06:30:58.000Z","updated":"2017-06-01T06:37:51.000Z","comments":true,"path":"2017/06/01/TiDB-KV-Mapping/","link":"","permalink":"http://funkygao.github.io/2017/06/01/TiDB-KV-Mapping/","excerpt":"","keywords":[],"text":"Data12Key： tablePrefix_rowPrefix_tableID_rowIDValue: [col1, col2, col3, col4] Unique Index12Key: tablePrefix_idxPrefix_tableID_indexID_indexColumnsValueValue: rowID Non-Unique Index12Key: tablePrefix_idxPrefix_tableID_indexID_ColumnsValue_rowIDValue: nil 12345var ( tablePrefix = []byte&#123;&apos;t&apos;&#125; recordPrefixSep = []byte(&quot;_r&quot;) indexPrefixSep = []byte(&quot;_i&quot;))","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"SSD vs HDD","slug":"ssd-vs-hdd","date":"2017-06-01T03:27:37.000Z","updated":"2017-06-02T08:17:20.000Z","comments":true,"path":"2017/06/01/ssd-vs-hdd/","link":"","permalink":"http://funkygao.github.io/2017/06/01/ssd-vs-hdd/","excerpt":"","keywords":[],"text":"http://www.pcgamer.com/hard-drive-vs-ssd-performance/2/ 123 RandRead RandWrite SeqRead SeqWriteSSD 50 200 2300 1300HDD 0.6 1.0 200 130","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"scale","slug":"scale","date":"2017-05-31T23:48:41.000Z","updated":"2017-06-02T08:17:07.000Z","comments":true,"path":"2017/06/01/scale/","link":"","permalink":"http://funkygao.github.io/2017/06/01/scale/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"gnatsd","slug":"gnatsd","date":"2017-05-31T07:33:01.000Z","updated":"2017-06-05T03:26:09.000Z","comments":true,"path":"2017/05/31/gnatsd/","link":"","permalink":"http://funkygao.github.io/2017/05/31/gnatsd/","excerpt":"","keywords":[],"text":"2011年用Ruby完成了一个版本，后来用golang重写，目前只维护golang版本NATS = Not Another Tibco Server 协议类似redis持久化通过上层的NATS Streaming完成 竞品是0mq/nanomsg/aeron https://github.com/real-logic/Aeron","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"microservice gateway","slug":"microservice-gateway","date":"2017-05-31T06:11:12.000Z","updated":"2017-05-31T06:24:37.000Z","comments":true,"path":"2017/05/31/microservice-gateway/","link":"","permalink":"http://funkygao.github.io/2017/05/31/microservice-gateway/","excerpt":"","keywords":[],"text":"需要的功能 traffic management failure recovery A/B testing, canary releases discovery, load balance throttling rate limit fault injection observability metrics, monitor, alert, insights policy enforcement access control service identity and security end-to-end authz","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Database as a Service","slug":"Database-as-a-Service","date":"2017-05-31T05:37:27.000Z","updated":"2017-05-31T05:43:41.000Z","comments":true,"path":"2017/05/31/Database-as-a-Service/","link":"","permalink":"http://funkygao.github.io/2017/05/31/Database-as-a-Service/","excerpt":"","keywords":[],"text":"Trove https://wiki.openstack.org/wiki/Trove","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"DB Schema Management","slug":"DB-Schema-Management","date":"2017-05-31T03:06:12.000Z","updated":"2017-05-31T03:15:29.000Z","comments":true,"path":"2017/05/31/DB-Schema-Management/","link":"","permalink":"http://funkygao.github.io/2017/05/31/DB-Schema-Management/","excerpt":"","keywords":[],"text":"https://github.com/skeema/skeema 统一控制dev/test/staging/prod等环境的scheme 12345678910111213141516$skeemaSkeema is a MySQL schema management tool. It allows you to export a databaseschema to the filesystem, and apply online schema changes by modifying files.Usage: skeema [&lt;options&gt;] &lt;command&gt;Commands: add-environment Add a new named environment to an existing host directory diff Compare a DB instance&apos;s schemas and tables to the filesystem help Display usage information init Save a DB instance&apos;s schemas and tables to the filesystem lint Verify table files and reformat them in a standardized way pull Update the filesystem representation of schemas and tables push Alter tables on DBs to reflect the filesystem representation version Display program version 与mysql在线alter table设计不同，它是higher level的，底层仍旧需要OSC支持:1alter-wrapper=&quot;/usr/local/bin/pt-online-schema-change --execute --alter &#123;CLAUSES&#125; D=&#123;SCHEMA&#125;,t=&#123;TABLE&#125;,h=&#123;HOST&#125;,P=&#123;PORT&#125;,u=&#123;USER&#125;,p=&#123;PASSWORDX&#125;&quot; Referenceshttps://www.percona.com/live/17/sessions/automatic-mysql-schema-management-skeema","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"Cassandra vs ScyllaDB","slug":"Cassandra","date":"2017-05-31T00:16:23.000Z","updated":"2017-05-31T08:53:14.000Z","comments":true,"path":"2017/05/31/Cassandra/","link":"","permalink":"http://funkygao.github.io/2017/05/31/Cassandra/","excerpt":"","keywords":[],"text":"CassandraCassandra 项目诞生于 Facebook，后来团队有人跳到 Amazon 做了另外一个 NoSQL 数据库 DynamoDB。 Dynamo论文发表于2007年，用于shopping cartCassandra在2008年被facebook开源，用于inbox search Features CQL(Cassandra Query Language) 1,000 node clusters multi-data center out-of-the-box replication ecosystem with Spark Internals LSM Tree Gossip P2P DHT consistency same as DynamoDB ONE QUORUM ALL read repair Thrift ScyllaDBKVM核心人员用C++写的Cassandra(Java) clone，单机性能提高了10倍，主要原因是： DPDK, bypass kernel O_DIRECT IO, bypass pagecache, cache由scylla自己管理 pagecahce的格式必须是文件的格式(sstable)，而app level cache更有效，更terse compaction的时候，pagecache讲是个累赘，它可能造成很多热点数据的淘汰 把一个node看做是多个cpu core组成的cluster, share nothing sharding at the cpu core instead of node更充分利用多核，减少contention，充分利用cpu cache, NUMA friendly 在需要core间交换数据时，使用explicit message passing avoid JVM GC Referenceshttps://db-engines.com/en/rankinghttps://github.com/scylladb/scyllahttp://www.scylladb.com/http://www.seastar-project.org/https://www.reddit.com/r/programming/comments/3lzz56/scylladb_cassandra_rewritten_in_c_claims_to_be_up/https://news.ycombinator.com/item?id=10262719","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"etcd3 vs zookeeper","slug":"etcd3-vs-zookeeper","date":"2017-05-25T09:07:34.000Z","updated":"2017-06-06T01:54:53.000Z","comments":true,"path":"2017/05/25/etcd3-vs-zookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/25/etcd3-vs-zookeeper/","excerpt":"","keywords":[],"text":"etcd v3独有的特性 get and watch by prefix, by interval lease based TTL for key sets runtime reconfiguration point in time backup extensive metrics 获取历史版本数据(这个非常有用)multi-version mini transation DSL 123456789Tx.If( Compare(Value(&quot;foo&quot;), &quot;&gt;&quot;, &quot;bar&quot;), Compare(Value(Version(&quot;foo&quot;), &quot;=&quot;, 2), ...).Then( Put(&quot;ok&quot;, &quot;true&quot;)...).Else( Put(&quot;ok&quot;, &quot;false&quot;)...).Commit() leases 1234l = CreateLeases(15*second)Put(foo, bar, l)l.KeepAlive()l.Revoke() watcher功能丰富 streaming watch 支持index参数，不会lose event recursive off-heap内存中只保留index，大部分数据通过mmap映射到boltdb file incremental snapshot zk独有的特性 ephemeral znode non-blocking full fuzzy snapshotToo busy to snap, skipping key支持在N Millions on-heap etcd2 etcd2的key支持在10K量级，etcd3支持1M量级 原因在于snapshot成本，可能导致0 qps，甚至reelection comparisonmemory footprint2M 256B keys123etcd2 10GBzk 2.4GBetcd3 0.8GB Referenceshttps://coreos.com/blog/performance-of-etcd.html","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"2017 kafka report","slug":"2017-kafka-report","date":"2017-05-25T01:30:27.000Z","updated":"2017-05-25T01:54:10.000Z","comments":true,"path":"2017/05/25/2017-kafka-report/","link":"","permalink":"http://funkygao.github.io/2017/05/25/2017-kafka-report/","excerpt":"","keywords":[],"text":"调查来自47个国家的388个组织(公司)26%受访者年销售额10亿美金以上15%受访者每天处理10亿消息/天43%受访者在公有云上使用kafka，其中60%是AWS Referenceshttps://www.confluent.io/wp-content/uploads/2017-Apache-Kafka-Report.pdf","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"zookeeper processor","slug":"zookeeper-processor","date":"2017-05-25T00:39:35.000Z","updated":"2017-05-26T07:11:03.000Z","comments":true,"path":"2017/05/25/zookeeper-processor/","link":"","permalink":"http://funkygao.github.io/2017/05/25/zookeeper-processor/","excerpt":"","keywords":[],"text":"Chain of Responsibility为了实现各种服务器的代码结构的高度统一，不同角色的server对应不同的processor chain 123interface RequestProcessor &#123; void processRequest(Request request) throws RequestProcessorException;&#125; LeaderZooKeeperServer.java FollowerZooKeeperServer.java ZooKeeperServer.java1234567func processPacket() &#123; submitRequest()&#125;func submitRequest(req) &#123; firstProcessor.processRequest(req)&#125;","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"kafka redesign","slug":"kafka-redesign","date":"2017-05-24T02:21:06.000Z","updated":"2017-05-24T08:15:10.000Z","comments":true,"path":"2017/05/24/kafka-redesign/","link":"","permalink":"http://funkygao.github.io/2017/05/24/kafka-redesign/","excerpt":"","keywords":[],"text":"Goals support many topics needle in haystack IO optimization R/W isolation index file leads to random sync write","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"apache bookeeper","slug":"apache-bookeeper","date":"2017-05-23T09:04:32.000Z","updated":"2017-05-24T08:13:14.000Z","comments":true,"path":"2017/05/23/apache-bookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/23/apache-bookeeper/","excerpt":"","keywords":[],"text":"Features 没有topic/partition概念，一个stream是由多个ledger组成的，每个ledger是有边界的createLedger(int ensSize, int writeQuorumSize, int ackQuorumSize) ledger只有int id，没有名字 每个entry(log)是有unique int64 id的 striped write: 交错存储 各个存储节点bookie之间没有明确的主从关系 shared WAL single writer Quorum投票复制，通过存储在zk里的LastAddConfirmedId确保read consistency bookie does not communicate with other bookies，由client进行并发broadcast/quorum VS Kafka createLedger时，客户端决定如何placement(org.apache.bookkeeper.client.EnsemblePlacementPolicy)，然后存放在zookeeper例如，5个bookie，createLedger(3, 3, 2) IO Model 读写分离 Disk 1: Journal(WAL) Device {timestamp}.txn Disk 2: Ledger Device 数据存放在多个ledger目录 LastLogMark表示index+data在此之前的都已经持久化到了Ledger Device，之前的WAL可以删除 异步写 而且是顺序写 所有的active ledger共用一个entry logger 读的时候利用ledger index/cache [Disk 3]: Index Device 默认Disk2和Disk3是在一起的 在写入Memtable后，就可以向client ack了 IO被分成4种类型，分别优化 sync sequential write: shared WAL async random write: group commit from Memtable tail read: from Memtable random read: from (index + os pagecache) Referenceshttps://github.com/ivankelly/bookkeeper-tutorialhttps://github.com/twitter/DistributedLog","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"SSD","slug":"SSD","date":"2017-05-22T06:31:39.000Z","updated":"2017-06-13T09:31:50.000Z","comments":true,"path":"2017/05/22/SSD/","link":"","permalink":"http://funkygao.github.io/2017/05/22/SSD/","excerpt":"","keywords":[],"text":"http://codecapsule.com/2014/02/12/coding-for-ssds-part-6-a-summary-what-every-programmer-should-know-about-solid-state-drives/http://www.open-open.com/lib/view/open1423106687217.html","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"oklog","slug":"oklog","date":"2017-05-22T03:22:58.000Z","updated":"2017-05-23T01:35:42.000Z","comments":true,"path":"2017/05/22/oklog/","link":"","permalink":"http://funkygao.github.io/2017/05/22/oklog/","excerpt":"","keywords":[],"text":"injecter负责write优化(WAL)，让storage node负责read优化与RocketMQ类似: CQRS injecter = commit log storage node = consume log 不同在于：storage node是通过pull mode replication机制实现，可以与injecter位于不同机器而RocketMQ的commit log与consume log是在同一台broker上的 kafka couples R/W，无法独立scale CQRS decouples R/W，可以独立scale produceProducer通过forwarder连接到多个injecter上，injecter间通过gossip来负载均衡，load高的会通过与forwarder协商进行redirect distribution queryscatter-gather","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"db trigger","slug":"db-trigger","date":"2017-05-22T02:40:14.000Z","updated":"2017-05-22T02:50:00.000Z","comments":true,"path":"2017/05/22/db-trigger/","link":"","permalink":"http://funkygao.github.io/2017/05/22/db-trigger/","excerpt":"","keywords":[],"text":"触发器的缺陷 如何监控 代码的版本控制 test 部署 性能损耗 多租户 资源隔离 无法频繁发布，如何应付频繁的需求变更","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"materialized view","slug":"materialized-view","date":"2017-05-22T02:05:02.000Z","updated":"2017-05-22T02:57:51.000Z","comments":true,"path":"2017/05/22/materialized-view/","link":"","permalink":"http://funkygao.github.io/2017/05/22/materialized-view/","excerpt":"","keywords":[],"text":"物化试图，可以理解为cache of query results, derived result觉得用“异构表”可能更贴切 与试图不同，它是物理存在的，并由数据库来确保与主库的一致性它是随时可以rebuilt from source store，应用是从来不会更新它的: readonly MySQL没有提供该功能，但通过dbus可以方便构造materialized viewPostgreSQL提供了materialized view Referenceshttps://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"cache invalidation","slug":"cache-invalidation","date":"2017-05-22T01:52:37.000Z","updated":"2017-05-22T02:59:27.000Z","comments":true,"path":"2017/05/22/cache-invalidation/","link":"","permalink":"http://funkygao.github.io/2017/05/22/cache-invalidation/","excerpt":"","keywords":[],"text":"1234567891011// read dataval = cache.get(key)if val == nil &#123; val = db.get(key) cache.put(key, val)&#125;return val// write datadb.put(key, val)cache.put(key, val) 这会造成dual write conflict 如果需要的只是eventaul consistency，那么通过dbus来进行cache invalidation是最有效的 https://martinfowler.com/bliki/TwoHardThings.html","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"Linked Esprosso","slug":"Linked-Esprosso","date":"2017-05-22T00:07:48.000Z","updated":"2017-05-23T01:50:05.000Z","comments":true,"path":"2017/05/22/Linked-Esprosso/","link":"","permalink":"http://funkygao.github.io/2017/05/22/Linked-Esprosso/","excerpt":"","keywords":[],"text":"WhatDistributed Document Store RESTful API MySQL作为存储 Helix负责集群 Databus异步replicate不同数据中心commit log Schema存放在zookeeper，通过Avro的兼容性实现schema evolution Referenceshttps://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store https://nonprofit.linkedin.com/content/dam/static-sites/thirdPartyJS/github-gists?e_origin=https://engineering.linkedin.com&amp;e_channel=resource-iframe-embed-4","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"Multi-Data Center Consistency","slug":"Multi-Data-Center-Consistency","date":"2017-05-19T07:18:33.000Z","updated":"2017-05-19T08:09:06.000Z","comments":true,"path":"2017/05/19/Multi-Data-Center-Consistency/","link":"","permalink":"http://funkygao.github.io/2017/05/19/Multi-Data-Center-Consistency/","excerpt":"","keywords":[],"text":"MDCC提供了跨机房的分布式数据库强一致性模型 Referenceshttp://mdcc.cs.berkeley.edu/","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"asynchronous distributed snapshot","slug":"distributed-snapshot","date":"2017-05-19T02:32:48.000Z","updated":"2017-05-19T09:31:46.000Z","comments":true,"path":"2017/05/19/distributed-snapshot/","link":"","permalink":"http://funkygao.github.io/2017/05/19/distributed-snapshot/","excerpt":"","keywords":[],"text":"如何给分布式系统做个全局逻辑一致的快照?Node State + Channel State 发送规则12345node.recordState()for conn in allConns &#123; // before any conn&apos;s outbound msg conn.send(marker)&#125; 接收规则123456789101112msg = conn.recv()if msg.isMarker() &#123; t1 = now() if !node.stateRecorded() &#123; node.recordState() Channel(conn) = [] &#125; else &#123; Channel(conn) = msgsBetween(now(), t1) // in-flight msgs not applied on state node.state.apply(msgs before the marker) &#125;&#125; Demo 12345678910111213141516171819202122232425a)P为自己做快照P(red, green, blue)在Channel(PQ)上 send(marker)b)P把绿球送给Q，这个消息是在marker后面以此同时，Q把自己的橙色球送给P，此时Q(brown, pink)c) Q在Channel(PQ)上收到marker // Q是接收者Q为自己做快照Q(brown, pink)Channel(PQ) = []// 因为之前Q把自己的橙色球送给了P，因此Q也是发送者在Channel(QP)上 send(marker)d)P收到橙色球，然后是marker由于P已经记录了state, Channel(QP)=[orange, ]最终的分布式系统的snapshot:P(red, green, blue)Channel(PQ) []Q(brown, pink)Channel(QP) = [orange, ] FAQ如何发起发起global distributed snapshot的节点，可以是一台，也可以多台并发 如何结束所有节点上都完成了snapshot 用途故障恢复 与Apache Storm的基于记录的ack不同，Apache Flink的failure recovery采用了改进的Chandy-Lamport算法checkpoint coordinator是JobManager data sources periodically inject markers into the data stream.123val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(4)env.enableCheckpointing(1000) // 数据源每1s发送marker(barrier) Whenever an operator receives such a marker, it checkpoints its internal state.1234567891011121314151617181920212223242526272829303132class StateMachineMapper extends FlatMapFunction[Event, Alert] with Checkpointed[mutable.HashMap[Int, State]] &#123; private[this] val states = new mutable.HashMap[Int, State]() override def flatMap(t: Event, out: Collector[Alert]): Unit = &#123; // get and remove the current state val state = states.remove(t.sourceAddress).getOrElse(InitialState) val nextState = state.transition(t.event) if (nextState == InvalidTransition) &#123; // 报警 out.collect(Alert(t.sourceAddress, state, t.event)) &#125; else if (!nextState.terminal) &#123; // put back to states states.put(t.sourceAddress, nextState) &#125; &#125; override def snapshotState(checkpointId: Long, timestamp: Long): mutable.HashMap[Int, State] = &#123; // barrier(marker) injected from data source and flows with the records as part of the data stream // // snapshotState()与flatMap()一定是串行执行的 // 此时operator已经收到了barrier(marker) // 在本方法返回后，flink会自动把barrier发给我的output streams // 再然后，保存states(默认是JobManager内存，也可以HDFS) states &#125; override def restoreState(state: mutable.HashMap[Int, State]): Unit = &#123; // 出现故障后，flink会停止dataflow，然后重启operator(StateMachineMapper) states ++= state &#125;&#125; Referenceshttp://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdfhttps://arxiv.org/abs/1506.08603https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.htmlhttps://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"https","slug":"https","date":"2017-05-19T00:21:27.000Z","updated":"2017-06-02T00:17:45.000Z","comments":true,"path":"2017/05/19/https/","link":"","permalink":"http://funkygao.github.io/2017/05/19/https/","excerpt":"","keywords":[],"text":"curl https://baidu.comHow the 270ms passed1234567891011121314151617181920212223242526272829303132333435363738394041424344451 1 0.0721 (0.0721) C&gt;S Handshake ClientHello Version 3.1 cipher suites TLS_EMPTY_RENEGOTIATION_INFO_SCSV TLS_DHE_RSA_WITH_AES_256_CBC_SHA TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 TLS_DHE_DSS_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA256 TLS_DHE_RSA_WITH_AES_128_CBC_SHA TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 TLS_DHE_DSS_WITH_AES_128_CBC_SHA TLS_RSA_WITH_RC4_128_SHA TLS_RSA_WITH_RC4_128_MD5 TLS_RSA_WITH_AES_128_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA256 TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA TLS_RSA_WITH_3DES_EDE_CBC_SHA compression methods NULL1 2 0.1202 (0.0480) S&gt;C Handshake ServerHello Version 3.1 session_id[32]= b3 ea 99 ee 5a 4c 03 e8 e0 74 95 09 f1 11 09 2a 9d f5 8f 2a 26 7a d3 7f 71 ff dc 39 62 66 b0 f9 cipherSuite TLS_RSA_WITH_AES_128_CBC_SHA compressionMethod NULL1 3 0.1205 (0.0002) S&gt;C Handshake Certificate1 4 0.1205 (0.0000) S&gt;C Handshake ServerHelloDone1 5 0.1244 (0.0039) C&gt;S Handshake ClientKeyExchange1 6 0.1244 (0.0000) C&gt;S ChangeCipherSpec1 7 0.1244 (0.0000) C&gt;S Handshake1 8 0.1737 (0.0492) S&gt;C ChangeCipherSpec1 9 0.1737 (0.0000) S&gt;C Handshake1 10 0.1738 (0.0001) C&gt;S application_data1 11 0.2232 (0.0493) S&gt;C application_data1 12 0.2233 (0.0001) C&gt;S Alert1 0.2234 (0.0000) C&gt;S TCP FIN1 0.2709 (0.0475) S&gt;C TCP FIN","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"hybrid logical clock","slug":"hlc","date":"2017-05-18T23:51:28.000Z","updated":"2017-05-27T00:07:48.000Z","comments":true,"path":"2017/05/19/hlc/","link":"","permalink":"http://funkygao.github.io/2017/05/19/hlc/","excerpt":"","keywords":[],"text":"分布式事务，为了性能，目前通常提供SI/SSI级别的isolation，通过乐观冲突检测而非2PC悲观方式实现，这就要求实现事务的causality，通常都是拿逻辑时钟实现total order例如vector clock就是一种，zab里的zxid也是；google percolator里的total order算是另外一种逻辑时钟，但这种方法由于有明显瓶颈，也增加了一次消息传递 但逻辑时钟无法反应物理时钟，因此有人提出了混合时钟，wall time + logical time，分别是给人看和给机器看，原理比较简单，就是在交互消息时，接收方一定sender event happens before receiver 但wall time本身比较脆弱，例如一个集群，有台机器ntp出现问题，管理员调整时间的时候出现人为错误，本来应该是2017-09-09 10:00:00，结果typo成2071-09-09 10:00:00，后果是它会传染给集群内所有机器，hlc里的wall time都会变成2071年，人工无法修复，除非允许丢弃历史数据，只有等到2071年那一天系统会自动恢复，wall time部分也就失去了意义 要解决这个问题，可以加入epoch1234HLC+-------+-----------+--------------+| epoch | wall time | logical time |+-------+-----------+--------------+ 修复2071问题时，只需把epoch+1","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"可靠性金字塔","slug":"SRE","date":"2017-05-18T05:55:21.000Z","updated":"2017-06-02T08:17:15.000Z","comments":true,"path":"2017/05/18/SRE/","link":"","permalink":"http://funkygao.github.io/2017/05/18/SRE/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"MySQL B+ Tree","slug":"MySQL-B-Tree","date":"2017-05-18T01:32:32.000Z","updated":"2017-05-18T01:34:07.000Z","comments":true,"path":"2017/05/18/MySQL-B-Tree/","link":"","permalink":"http://funkygao.github.io/2017/05/18/MySQL-B-Tree/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"batch insert(mysql)","slug":"batch-insert","date":"2017-05-18T00:46:17.000Z","updated":"2017-05-18T00:58:45.000Z","comments":true,"path":"2017/05/18/batch-insert/","link":"","permalink":"http://funkygao.github.io/2017/05/18/batch-insert/","excerpt":"","keywords":[],"text":"1234567// case1INSERT INTO T(v) VALUES(1), (2), (3), (4), (5)// case2for i=1; i&lt;=5; i++ &#123; INSERT INTO T(v) VALUES(i);&#125; case1和2有什么影响？假设auto_commit 好处 减少与mysql server的交互 减少SQL解析(如果statement则没区别) query cache打开时，只会invalidate cache一次，提高cache hit 坏处 可能变成一个大事务batch insert的时候，batch不能太大","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"cannot have exactly-once delivery","slug":"cannot-have-exactly-once-delivery","date":"2017-05-17T02:46:26.000Z","updated":"2017-05-18T06:12:44.000Z","comments":true,"path":"2017/05/17/cannot-have-exactly-once-delivery/","link":"","permalink":"http://funkygao.github.io/2017/05/17/cannot-have-exactly-once-delivery/","excerpt":"","keywords":[],"text":"http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"RocketMQ解读","slug":"RocketMQ","date":"2017-05-17T02:38:32.000Z","updated":"2017-05-18T00:29:34.000Z","comments":true,"path":"2017/05/17/RocketMQ/","link":"","permalink":"http://funkygao.github.io/2017/05/17/RocketMQ/","excerpt":"","keywords":[],"text":"Features Producer Group发送事务消息时，作为TC，要多机，保存事务状态表{offset: P/C/R} Broker tag-based message filter 定时消息，不支持任意精度，只是特定level: 5s, 10s, 1m等 queueID=delayLevel-1因此，应该不支持message revoke 区分commit log和consume log，有点类似WAL和table关系可以把它们放在不同FS下，但没有更细粒度的增加了一个分发步骤的好处：可以不分发 Commit Log1234$&#123;rocketmq.home&#125;\\store\\commitlog\\$&#123;fileName&#125;fileName[n] = fileName[n-1] + mappedFileSize为了保证mappedFileSize相同，在每个file tail加padding，默认1GB 每条消息12345678QueueOffset针对普通消息，存的是consume log里的offset；如果事务消息，是事务状态表的offset+---------+-------+-----+---------+------+-------------+----------------+----------------+| MsgSize | Magic | CRC | QueueID | Flag | QueueOffset | PhysicalOffset | SysFlag(P/C/R) |+---------+-------+-----+---------+------+-------------+----------------+----------------++--------------+------------------+-----------+---------------+----+------+-------+------+| ProducedTime | ProduderHostPort | StoreTime | StoreHostPort | .. | Body | Topic | Prop |+--------------+------------------+-----------+---------------+----+------+-------+------+ 每次append commit log，会同步调用dispatch分发到consume queue和索引服务1234567new DispatchRequest(topic, queueId, result.getWroteOffset(), result.getWroteBytes(), tagsCode, msg.getStoreTimestamp(), result.getLogicsOffset(), msg.getKeys(), // Transaction msg.getSysFlag(), msg.getPreparedTransactionOffset()); queue仅仅是逻辑概念，可以通过它来参与producer balance，类似一致哈希里的虚拟节点每台broker上的commitlog被本机所有的queue共享，不做任何区分 1234567broker1: queue0, queue2broker2: queue0, then, topicA has 3 queues:broker1_queue0, broker1_queue2, broker2_queue0producer.selectOneMessageQueue(&quot;topicA&quot;, &quot;broker1&quot;, &quot;queue0&quot;) 消息的局部顺序由producer client保证 Question 如何实现retention by topic: 没有实现仅仅根据commit log file的mtime来判断是否过期，虽然里面混杂多topics 如何I/O balancing 如何压缩 如果CRC出错，那么所有topic都受影响? 为什么要存StoreHostPort？如何迁移topic：无法迁移 写commit log需要加锁，这个锁粒度太大，相当于db level lock，而非table level broker的脑裂问题 failover topic的commit log是分散在所有broker上的 Consume Queue1$&#123;rocketmq.home&#125;/store/consumequeue/$&#123;topicName&#125;/$&#123;queueId&#125;/$&#123;fileName&#125; 读一条消息，先读consume queue(类似mysql的secondary index)，再读commit log(clustered index) 没有采用sendfile，而是通过mmap：因为random read 123+---------------------+-----------------+------------------------+| CommitLogOffset(8B) | MessageSize(4B) | MessageTagHashcode(8B) |+---------------------+-----------------+------------------------+ 虽然消费时，consume queue是顺序的，但接下来的commit log几乎都是random read，此外如何优化压缩？光靠pagecache+readahead是远远不够的 Producer123TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); // from local cache or name serverMessageQueue mq = topicPublishInfo.selectOneMessageQueue(lastBrokerName);sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); Transaction123456789101112131415// 2PC，2 messages// Phase1producer group write redologproducer group send a message(type=TransactionPreparedType) to brokerbroker append it to CommitLog and return MessageIdbroker will not append it to consume queue// Phase2producer group write redologproducer group send a message(type=TransactionCommitType, msgId=$msgId) to brokerbroker find the message with msgId in CommitLog and clone it and append it to CommitLog(type=TransactionCommitType|TransactionRollbackType)if type == TransactionCommitType &#123; broker append commit log offset to consume queue&#125; State Table保存在broker，默认1m扫一次 1234567824B, mmap+-----------------+------+-----------+-----------------------+--------------+| CommitLogOffset | Size | Timestamp | ProducerGroupHashcode | State(P/C/R) |+-----------------+------+-----------+-----------------------+--------------+prepare消息，insert tablecommit/rollback消息，update table 对于未决事务，根据随机向Producer Group里的一台发请求CHECK_TRANSACTION_STATEProducer Group根据redolog(mmap)定位状态Producer Group信息存放在namesvr Problems Producer不再是普通的client，它已经变成server(TC)，而且要求不能随便shutdown Producer Group里写redolog的机器死了怎么办","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"architecture design checklist","slug":"architecture-design-checklist","date":"2017-05-17T01:00:00.000Z","updated":"2017-06-02T08:16:55.000Z","comments":true,"path":"2017/05/17/architecture-design-checklist/","link":"","permalink":"http://funkygao.github.io/2017/05/17/architecture-design-checklist/","excerpt":"","keywords":[],"text":"archeck","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"kateway replay messages","slug":"kateway-replay-messages","date":"2017-05-16T23:52:34.000Z","updated":"2017-05-17T00:27:21.000Z","comments":true,"path":"2017/05/17/kateway-replay-messages/","link":"","permalink":"http://funkygao.github.io/2017/05/17/kateway-replay-messages/","excerpt":"","keywords":[],"text":"Issueconsumer有需求回放/快进消息，目前kateway具有该功能：用户在web console上把offset设置到指定位置 但由于机器里kateway正在消费源源不断的消息，checkpoint会overwrite这个指定的offset这就要求用户先关闭消费进程，然后web console上操作，再启动消费进程: not user friendly在不影响性能前提下，对其进行改进 Solution12345678910111213141516171819202122232425_, stat, err := cg.kz.conn.Get(path)if cg.lastVer == -1 &#123; // 第一次commit offset cg.lastVer = stat.Version&#125; else if cg.lastVer != stat.Version &#123; // user manually reset the offset checkpoint return ErrRestartConsumerGroup&#125;// 也可能在Get后，用户恰好操作了“回放”，通过CAS解决这个问题switch err &#123;case zk.ErrNoNode: return cg.kz.create(path, data, false)case nil: newStat, err := cg.kz.conn.Set(path, data, stat.Version) if err != nil &#123; cg.lastVer = newStat.Version &#125; return errdefault: return err&#125;","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"how DBMS works","slug":"how-DBMS-works","date":"2017-05-16T08:31:29.000Z","updated":"2017-06-01T06:09:13.000Z","comments":true,"path":"2017/05/16/how-DBMS-works/","link":"","permalink":"http://funkygao.github.io/2017/05/16/how-DBMS-works/","excerpt":"","keywords":[],"text":"Basics Undo log Oracle和MySQL机制类似 MS SQL Server里，称为transaction log PostgreSQL里没有undo log，它通过mvcc系统表实现，每一行存储多个版本 Redo log Oracle和MySQL机制类似 MS SQL Server里，称为transaction log PostgreSQL里称为WAL Query Optimization大部分是基于Selinger的论文，动态规划算法，把这个问题拆解成3个子问题 cost estimation以I/O和CPU的成本衡量 relational equivalences that define a search space cost-based search Concurrency ControlGray论文 区分细粒度和粗粒度的锁数据库是个分层结构 hierarchical structure 提出了多种隔离级别最初都是2PL实现的serializable isolation Database RecoveryIBM的ARIES算法(1992)，Algorithm for Recovery and Isolation Exploiting SemanticsARIES can only update the data in-place after the log reaches storage确保在恢复时，已经commit的事务要redo，未commit的事务要undoredo log是物理的，undo log是逻辑的 No Force, Steal database need not write dirty pages to disk at commit time由于有redo log，update pages are written to disk lazily after commitNo Force database can flush dirty pages to disk at any time由于有undo log，uncommitted(dirty) pages can be written to disk by the buffer managerSteal ARIES为each page保存LSN，disk page是数据管理和恢复的基本单位，page write是原子的 ARIES crash recovery分成3步 analysis phase从前向后，determine winners &amp; losers redo phase如果是Force(在commit前刷dirty pages)，就不需要redo stage了repeat history undo phase从后向前，undo losers ARIES数据结构 xaction table dirty page table checkpoint Example123456789101112131415161718192021222324252627282930After a crash, we find the following log:0 BEGIN CHECKPOINT5 END CHECKPOINT (EMPTY XACT TABLE AND DPT)10 T1: UPDATE P1 (OLD: YYY NEW: ZZZ)15 T1: UPDATE P2 (OLD: WWW NEW: XXX)20 T2: UPDATE P3 (OLD: UUU NEW: VVV)25 T1: COMMIT30 T2: UPDATE P1 (OLD: ZZZ NEW: TTT)Analysis phase:Scan forward through the log starting at LSN 0.LSN 5: Initialize XACT table and DPT to empty.LSN 10: Add (T1, LSN 10) to XACT table. Add (P1, LSN 10) to DPT.LSN 15: Set LastLSN=15 for T1 in XACT table. Add (P2, LSN 15) to DPT.LSN 20: Add (T2, LSN 20) to XACT table. Add (P3, LSN 20) to DPT.LSN 25: Change T1 status to &quot;Commit&quot; in XACT tableLSN 30: Set LastLSN=30 for T2 in XACT table.Redo phase:Scan forward through the log starting at LSN 10.LSN 10: Read page P1, check PageLSN stored in the page. If PageLSN&lt;10, redo LSN 10 (set value to ZZZ) and set the page&apos;s PageLSN=10.LSN 15: Read page P2, check PageLSN stored in the page. If PageLSN&lt;15, redo LSN 15 (set value to XXX) and set the page&apos;s PageLSN=15.LSN 20: Read page P3, check PageLSN stored in the page. If PageLSN&lt;20, redo LSN 20 (set value to VVV) and set the page&apos;s PageLSN=20.LSN 30: Read page P1 if it has been flushed, check PageLSN stored in the page. It will be 10. Redo LSN 30 (set value to TTT) and set the page&apos;s PageLSN=30.Undo phase:T2 must be undone. Put LSN 30 in ToUndo.Write Abort record to log for T2LSN 30: Undo LSN 30 - write a CLR for P1 with &quot;set P1=ZZZ&quot; and undonextLSN=20. Write ZZZ into P1. Put LSN 20 in ToUndo.LSN 20: Undo LSN 20 - write a CLR for P3 with &quot;set P3=UUU&quot; and undonextLSN=NULL. Write UUU into P3. ARIES是为传统硬盘设计的，顺序写，但成本也明显：修改1B，需要redo 1B+undo 1B+page 1B=3Bwhat if in-place update with SSD? 分布式mid-1970s 2PC 一票否决 Referenceshttps://blog.acolyer.org/2016/01/08/aries/http://cseweb.ucsd.edu/~swanson/papers/SOSP2013-MARS.pdfhttps://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"scalability papers","slug":"scalability-papers","date":"2017-05-16T08:28:29.000Z","updated":"2017-06-02T08:17:57.000Z","comments":true,"path":"2017/05/16/scalability-papers/","link":"","permalink":"http://funkygao.github.io/2017/05/16/scalability-papers/","excerpt":"","keywords":[],"text":"http://www.perfdynamics.com/Manifesto/USLscalability.html","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"PostgreSQL MVCC","slug":"PostgreSQL-MVCC","date":"2017-05-16T03:54:13.000Z","updated":"2017-05-16T08:23:45.000Z","comments":true,"path":"2017/05/16/PostgreSQL-MVCC/","link":"","permalink":"http://funkygao.github.io/2017/05/16/PostgreSQL-MVCC/","excerpt":"","keywords":[],"text":"InternalsMySQL通过undo log记录uncommitted changes，与此不同，PostgreSQL store all row versions in table data structure. 每个row有2个隐藏字段 Tmin insert时的trx id Tmax delete时的trx id INSERT DELETE DELETE操作并不会马上物理删除，而是VACUUM进程调度进行purge UPDATE","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"VNC protocol","slug":"VNC-protocol","date":"2017-05-16T02:32:46.000Z","updated":"2017-05-17T00:51:09.000Z","comments":true,"path":"2017/05/16/VNC-protocol/","link":"","permalink":"http://funkygao.github.io/2017/05/16/VNC-protocol/","excerpt":"","keywords":[],"text":"WebRTCWebRTC提供了direct data and media stream transfer between two browsers without external server involved: P2P 浏览器上点击“Screen share”按钮后 12345678// sender利用OS的API获取screenshot，并以一定的FPS来进行发送// 优化：把屏幕分成chunk，在把timer之间有变化的chunk生成frame发送时，frame被编码成H.264或VP8通过HTTPS发送// receiver对接收到的frame解码并显示 通过WebRTC实现的是只读的屏幕分享，receiver不能控制sender屏幕 实现123456789101112131415161718192021222324252627&lt;body&gt; &lt;p&gt;&lt;input type=&quot;button&quot; id=&quot;share&quot; value=&quot;Screen share&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;video id=&quot;video&quot; autoplay /&gt;&lt;/p&gt;&lt;/body&gt;&lt;script&gt;navigator.getUserMedia = navigator.webkitGetUserMedia || navigator.getUserMedia;$(&apos;#share&apos;).click(function() &#123; navigator.getUserMedia(&#123; audio: false , video: &#123; mandatory: &#123; chromeMediaSource: &apos;screen&apos; , maxWidth: 1280 , maxHeight: 720 &#125; , optional: [ ] &#125; &#125;, function(stream) &#123; // we&apos;ve got media stream // so the received stream can be transmitted via WebRTC the same way as web camera and easily played in &lt;video&gt; component on the other side document.getElementById(&apos;video&apos;).src = window.URL.createObjectURL(stream); &#125; , function() &#123; alert(&apos;Error. Try in latest Chrome with Screen sharing enabled in about:flags.&apos;); &#125;)&#125;)&lt;/script&gt; VNCRemote Frame Buffer，支持X11, Windows, Mac远程终端用户使用机器（比如显示器、键盘、鼠标）的叫做客户端，提供帧缓存变化的被称为服务器 显示协议pixel(x, y) =&gt; 像素数据编码 C-&gt;S消息类型SetEncodingsRaw, CopyRect, RRE, Hextile, TRLE, ZRLE FramebufferUpdateRequest最重要的显示消息，表示client要server传回哪些区域的图像 1234client.send(messageType, incremental, x, y, width, height) =&gt; server// incremental&gt;0，表示该区域内容变化了才发给client；没有变化，就不用发server.reply(messageType, rectangleN, [&#123;x, y, with, height, color&#125;, ...]) =&gt; client KeyEventclient端的键盘动作 PointerEventclient端的鼠标动作 vnc browserhttp://guacamole.incubator.apache.org/https://github.com/novnc/noVNC Referenceshttp://www.tuicool.com/articles/Rzqumuhttps://github.com/macton/htermchrome://flags/#enable-usermedia-screen-capture","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"LSM for SSD","slug":"LSM","date":"2017-05-16T01:33:59.000Z","updated":"2017-06-01T11:37:56.000Z","comments":true,"path":"2017/05/16/LSM/","link":"","permalink":"http://funkygao.github.io/2017/05/16/LSM/","excerpt":"","keywords":[],"text":"Basics (immutable)memtable: sorted skiplist SSTable: sorted string table L0里的SSTable们，key可能会overlap，因为他们是直接从immutable memtable刷出来的 SSTable file123+-------------------+-------------------------+------------+| index block(16KB) | bloom-filter block(4KB) | data block |+-------------------+-------------------------+------------+ Get(key)12345678910111213locate key in memtableif found then returnlocate key in immutable memtableif found then returnfor level:=0; level&lt;=6; level++ &#123; // 对于level0，需要遍历所有SSTable，因为keys overlap // 但leveldb为了解决这个问题，除了Bloomfilter，也限制了L0的文件数量，一旦超过8，就compact(L0-&gt;L1) // 对于其他level，由于已经sorted，可以直接定位SSTable // // 最坏情况下，Get(key)需要读8个L0以及L1-L6，共14个文件 locate key in SSTable in $level if found then return&#125; SSD主流SSD，例如Samsung 960 Pro，可以提供440K/s random readwith block size=4KB LSM是为传统硬盘设计的，在SSD下，可以做优化，不必过分担心随机读 优化LSM-Tree的主要成本都在compaction(merge sort)，造成IO放大(50倍) 读很多文件到内存 排序 再写回到磁盘 要优化compaction，可以把LSM Tree变小，RocksDB是通过压缩实现的 在SSD下，可以考虑把key和value分离，在LSM Tree里只保存sorted key和pointer(value)，value直接保存在WAL里 key: 16Bpointer(value): 16B 2M个k/v，需要64MB2B个k/v，需要64GB Referencehttps://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"InnoDB MVCC","slug":"InnoDB-MVCC","date":"2017-05-16T00:17:17.000Z","updated":"2017-06-02T07:44:59.000Z","comments":true,"path":"2017/05/16/InnoDB-MVCC/","link":"","permalink":"http://funkygao.github.io/2017/05/16/InnoDB-MVCC/","excerpt":"","keywords":[],"text":"BasicInnoDB中通过Undo log实现了txn rollback和MVCC，而并发控制(isolation)通过锁来实现Undo log分为insert undo和update undo(delete是一种特殊的update)，回滚时 insert只要把insert undo log丢弃即可 update需要通过DB_ROLL_PTR DB_TRX_ID找到事务修改前的版本并恢复 与redo log不同的是，磁盘上不存在单独的undo log文件，所有的undo log均存放在主ibd数据文件中（表空间），即使客户端设置了每表一个数据文件也是如此 内部存储InnoDB为每行row都reserved了隐藏字段(system column) DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 12345typedef ib_uint64_t ib_id_t;typedef ib_id_t row_id_t;typedef ib_id_t trx_id_t;typedef ib_id_t roll_ptr_t; Undo log实现方式 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） Demo Innodb中存在purge线程，它会查询那些比现在最老的活动事务还早的undo log，并删除它们 Issues回滚成本当事务正常提交时Innbod只需要更改事务状态为COMMIT即可，不需做其他额外的工作而Rollback如果事务影响的行非常多，回滚则可能成本很高 write skewInnoDB通过Undo log实现的MVCC在修改单行记录是没有问题的，但多行时就可能出问题1234begin;update table set col1=2 where id=1; // 成功，创建了undo logupdate table set col2=3 where id=2; // 失败rollback; 回滚row(id=1)时，由于它没有被lock，此时可能已经被另外一个txn给修改了，这个回滚会破坏已经commit的事务 如果要解决这个问题，需要应用层进行控制 另外一个例子123456国家规定一个家庭最多养3只宠物(constraint)，Alice和Bob是一家人，他们现在有dog(1)+cat(1)如果并发地，Alice再买一只猫，而Bob再买一只狗，这2个事务就会write skew因为repeatable read：一个事务提交之后才visible by other txnTxnAlice TxnBobcat=cat+1 dog=dog+1这2个事务都成功了，但却破坏了constraint 锁RR隔离级别下，普通select不加锁，使用MVCC进行一致性读取，即snapshot readupdate, insert, delete, select … for update, select … lock in share mode都会进行加锁，并且读取的是当前版本: READ COMMITTED读除了lock in share mode是S锁，其他都是X锁 Referenceshttps://dev.mysql.com/doc/refman/5.7/en/innodb-locks-set.html","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"microservice transaction","slug":"microservice-transaction","date":"2017-05-15T23:02:29.000Z","updated":"2017-05-16T02:27:33.000Z","comments":true,"path":"2017/05/16/microservice-transaction/","link":"","permalink":"http://funkygao.github.io/2017/05/16/microservice-transaction/","excerpt":"","keywords":[],"text":"Referenceshttps://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"zookeeper internals","slug":"zookeeper","date":"2017-05-15T02:26:14.000Z","updated":"2017-06-07T23:57:00.000Z","comments":true,"path":"2017/05/15/zookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/15/zookeeper/","excerpt":"","keywords":[],"text":"BasicsModelFile api without partial R/WNo rename operation zab通过TCP+zxid实现事务的totally order ImplementationZKDatabase1234567// zk的内存数据库class ZKDatabase &#123; DataTree dataTree LinkedList&lt;Proposal&gt; committedLog FileTxnSnapLog snapLog ConcurrentHashMap&lt;Long, Integer&gt; sessionsWithTimeouts // &#123;sessionID: timeout&#125;&#125; DataTree12345678910111213141516171819202122class DataTree &#123; ConcurrentHashMap&lt;String, DataNode&gt; nodes // &#123;path: znode&#125;, flat ConcurrentHashMap&lt;Long, HashSet&lt;String&gt;&gt; ephemerals // &#123;sessionID: [path, ]&#125; WatchManager dataWatches, childWatches func createNode(path, data) &#123; parent = nodes.get(parentPath) parent.Lock() // check NodeExistsException // set stat of the new znode and its parent child = new DataNode(parent, data, stat) // the new znode parent.addChild(child) nodes.put(path, child) if ephemeralOwner != nil &#123; ephemerals.get(ephemeralOwner).add(path) &#125; parent.Unlock() dataWatches.triggerWatch(path, NodeCreated) childWatches.triggerWatch(parentPath, NodeChildrenChanged) &#125;&#125; DataNode1234567class DataNode &#123; DataNode parent Set&lt;String&gt; children StatPersisted stat []byte data&#125; n(n-1)/2 conns只允许id比较大的server发起主动连接：由于任意server在启动时都会主动向其他server发起连接，如果这样，任意两台server之间就拥有两条连接，这明显是没有必要的123456789======= ======= ======= ======= ======= ========sid 1 2 3 4 5======= ======= ======= ======= ======= ========1 &lt;&gt; &lt; &lt; &lt; &lt;2 &lt;&gt; &lt; &lt; &lt;3 &lt;&gt; &lt; &lt;4 &lt;&gt; &lt;5 &lt;&gt;======= ======= ======= ======= ======= ======== 成为 leader 的条件 选epoch最大的 epoch相等，选 zxid 最大的 epoch和zxid都相等，选择server id最大的 1234(newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId)))) 何时选举进入LOOKING状态 刚启动时 稳定运行中，任何的异常都会让本机进入LOOKING态123catch (Exception e) &#123; setPeerState(LOOKING)&#125; ConstraintsMany ZooKeeper write requests are conditional in nature: a znode can only be deleted if it does not have any children a znode can be created with a name and a sequence number appended to it a change to data will only be applied if it is at an expected version Quorum1234567891011121314func isQuorum(type) &#123; // zk的请求有2种 1. 事务请求 2. 只读请求 switch (type) &#123; case OpCode.exists, getACL, getChildren, getChildren2, getData: // 本地执行，不需要proposal return false case OpCode.error, closeSession, create, createSession, delete, setACL, setData, check, multi: return true default: return false &#125;&#125; 注意：session部分，也会走txn multi是原子操作，multi里的每个op都使用相同的zxid WatchWatches are maintained locally at the ZooKeeper server to which the client is connected.它是不走proposal quorum的 Watcher只会告诉客户端发生了什么类型的事件，而不会说明事件的具体内容例如，NodeDataChanged，watcher只会通知client：在你watch的path上，发生了NodeDataChanged这个事件但最新的数据是什么，不在event里，而需要client主动重新去get Watch的通知，由WatchManager完成，它先从内存里删除这个watcher，然后回调watcher.process后者在NIOServerCnxn，即watches are sent asynchronously to watchers(client).But ZooKeeper guarantees that a client will see a watch event for a znode it is watching before seeing the new data that corresponds to that znode. 1234567891011121314151617181920212223class WatchManager &#123; HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable func triggerWatch(path, event) &#123; synchronized(this) &#123; watchers = watchTable.remove(path) // so one time trigger &#125; // Watch机制本身是非常轻量级的，对服务器不会有多大开销： // 它都是local zk server在内存中处理 // 但如果一个path的watcher很多，那么这个O(n)循环 for watcher = range watchers &#123; w.process(event) &#125; &#125;&#125;func process(WatchedEvent event) &#123; h = new ReplyHeader(-1, -1L, 0) sendResponse(h, event) // if IOException, close the client conn // sock.write(非阻塞) async write // sendResponse对同一个client是串行的，顺序的&#125; 顺序123456client.get(path, watch=true)// 此时数据发生变化zk保证的顺序：client先拿到watch event，之后才能看到最新的数据client如果watch很多时间，那么得到这些event的顺序与server端发生的顺序是完全一致的 watch的保持zk client连接zk1, 并get with watch，突然zk1 crash，client连接到zk2，那么watch是如何保持的?这是client端实现的，内存里记录watches，在pick new server后，sendSetWatches watch的事件会丢吗client2能获取到每次的set事件吗?12client1不停地set(node, newValue)client2 get with watch 不一定：因为是one time trigger获取event后，要重新watch，在此过程中可能产生新的事件: 期间事件lost 此外，zk与client的conn断开后，client会连接下一个zk，在此期间的事件lost例如，watch NodeCreated事件，在client重新连接期间，该node created，那么client将永远无法获取该事件 watch同一个znode多次，会收到几条event?由于WatchManager的实现，相同类型的watch在一个path上被set多次，只会触发一次 12create(&quot;/foo&quot;, EPHEMERAL_SEQUENTIAL)exists(&quot;/foo&quot;, watcher) // 那么这个watch事件是永远不会trigger的，因为path不同，见WatchManager的实现 ClientrecvLoop里任意的错误，都会pick next server and authentication，进入新的循环 conn reset by peer conn EOF receive packet timeout session expire time conn.recvTimeout = sessionTimeout * 2 / 3 ping interval = sessionTimeout / 3 例如，sessionTimeout=30s，那么client在等待20s还得不到response，就会try next server恰好赶在ping的窗口期 10+20=30 Q/Aclient ping是如何保持住session的?client连接s1，定期ping，但s1 crash后client连接s2，为什么session能保持住?123456connect(s2)send(ConnectRequest&#123;LastZxidSeen, SessionID&#125;) // SessionID是s1当时分配的var r ConnectResponse = recv()if r.SessionID == 0 &#123; // session expire&#125; createSession会通过txn，因此client可以failoverserver在sessionTimeout内没有收到ping，就会closeSession，它也通过txn session idzk session id assigned by server, global unique 123456func initializeNextSession(id=1) &#123; long nextSid = 0; nextSid = (System.currentTimeMillis() &lt;&lt; 24) &gt;&gt;&gt; 8; nextSid = nextSid | (id &lt;&lt;56); return nextSid;&#125; 后面的session id就是这个种子基础上 increment by 1 SnapshotdataLogDir(txn log) and dataDir(snapshot) should be placed in 2 disk devices如果txn log和snapshot处于同一块硬盘，异步的snapshot可能会block txn log，连锁反应就是把proposal阻塞，进而造成follower重新选举 when System.getProperty(“zookeeper.snapCount”), 默认值100,000 takeSnapshot的时间在50,000 ~ 100,0000 之间的随机值 txn数量超过snapCount+随机数 roll txn log 创建一个线程，异步执行takeSnapshot。但前面的takeSnapshot线程未完成，则放弃Too busy to snap, skipping 1234567891011121314151617181920212223Request si = getRequest()if (zks.getZKDatabase().append(si)) &#123; // txn log ok logCount++; // logCount就是txn的数量 if (logCount &gt; (snapCount / 2 + randRoll)) &#123; randRoll = r.nextInt(snapCount/2); // 为了防止集群内所有节点同时takeSnapshot加入随机 zks.getZKDatabase().rollLog(); // txn log will roll if (snapInProcess != null &amp;&amp; snapInProcess.isAlive()) &#123; LOG.warn(&quot;Too busy to snap, skipping&quot;); &#125; else &#123; snapInProcess = new Thread(&quot;Snapshot Thread&quot;) &#123; public void run() &#123; try &#123; zks.takeSnapshot(); &#125; catch(Exception e) &#123; LOG.warn(&quot;Unexpected exception&quot;, e); &#125; &#125; &#125;; snapInProcess.start(); &#125; logCount = 0; &#125;&#125; size每个znode meta data至少76+path+data，如果1M znodes，平均size(path+data)=100，那么snapshot文件长度至少200MB我的一个生产环境zk，znode 3万，snapshot文件15MB；即，如果300万个znodes，那么snapshot文件将是1.5GB 123456path(len4, path)node data(len4, data) acl8 meta60 czxid8, mzxid8, ctime8, mtime8, version4, cversion4, aversion4, ephemeralOwner8, pzxid8 checksumAdler32 Edge casesleader electionLOOKING后，把自己的zxid广播，是得到大多数同意就成为leader?是，不需要等全部ack async commit[S1(leader), S2, S3]S1 Propose set(a)=5，在得到majority Ack(proposal)后，向所有机器发送Commit，Q1. S1需要在得到majority Ack(commit)后才return OK to client?Q2. 如果S1发送Commit给所有机器前恰好挂了，new leader会恢复这个事务吗？ leader在得到majority Ack(proposal)后，majority servers已经记录下了txnlog，leader发送Commit只是为了让serversmake the txn visibile to client，Commit消息是不会记录txnlog的leader处理Commit是异步的，不需要等待Commit的ack，即Q1: no，Q2: yes ZAB makes the guarantee that a proposal which has been logged by a quorum of followers will eventually be committedany uncommited proposals from a previous epoch seen by a new leader will be committed by that leader before it becomes active. 换个角度看这个问题：S1得到请求set(a)=5，commit locally，但commit还没有发送给S2，S3，crash!这时候，一个client可能发请求 get(a)，如果它连接的是S1，在S1 crash前，get(a)=2所以，这个commit必须让新leader知道 sync proposal[S1(leader), S2, S3, S4, S5]S1 Propose set(a)=2，发送给了S2 Proposal，但S3-5还没有收到Proposal，此时S1 crash，那么这个proposal在new leader上会被恢复吗? 即重新选举后，get(a)=2? 不一定！ 1234567S1(leader), S2, S3, S4, S5现在propose set(a)=b，S1确认了，但其他还没有确认，此时全部crash然后启动S2-S5，等一会儿再启动S1，那么S2-S5他们的txid相同，会选出S5 leader等S1启动时，它的txid是最大的，a=b可能会丢：如果S1启动慢了200ms内，可能不会丢；否则，例如慢了1分钟，则丢了，S1变成follower后会把该txid truncate: txnlog seek FastLeaderElection.java123456789101112131415161718192021222324252627282930finalizeWait=200ms // 在得到majority确认后，但还没有得到全部确认，wait before make leadersendNotifications()for i.am.looking &#123; n = recvqueue.poll() switch n.state &#123; case LOOKING: compare my proposal with n and update my proposal if every node agrees &#123; // got the leader! return &#125; if quorum agrees &#123; // Verify if there is any change in the proposed leader for &#123; n = recvqueue.poll(200ms) if n == nil &#123; break &#125; &#125; &#125; case FOLLOWING, LEADING: if leader is not me &#123; // confirm I have recv notification from leader // stating that he is leader &#125; &#125;&#125; Referenceshttps://issues.apache.org/jira/browse/ZOOKEEPER-1813https://issues.apache.org/jira/browse/ZOOKEEPER-417https://issues.apache.org/jira/browse/ZOOKEEPER-1674https://issues.apache.org/jira/browse/ZOOKEEPER-1642http://blog.csdn.net/pwlazy/article/details/8080626","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"delay and schedule message delivery","slug":"delay-and-schedule-message-delivery","date":"2017-05-15T00:11:24.000Z","updated":"2017-05-15T00:26:43.000Z","comments":true,"path":"2017/05/15/delay-and-schedule-message-delivery/","link":"","permalink":"http://funkygao.github.io/2017/05/15/delay-and-schedule-message-delivery/","excerpt":"","keywords":[],"text":"使用场景 业务需要 通过它可以实现XA的prepare/commit/rollback，从而实现与其他系统的原子提交 实现kateway通过mysql作为WAL，并通过background worker(actor)来实现调度/commit/rollback 优先队列以message due time作为优先级进行存储，配合workermessage rollback可以通过发送一个tombstone message实现但由于worker的async，无法在rollback时判断是否真正rollback成功：一条消息要5分钟后发送，在5分钟到达时，client可能恰好要取消，这时候，rollback与worker之间存在race condition，需要正常处理这个一致性：要么，取消失败，消息被发出要么，取消成功，消息不发出不能，取消成功，消息被发出1234567891011// workerfor &#123; if msg := peek(queue); msg.due() &#123; msg = pop(queue) if msg.isTombstone() &#123; // the msg is cancelled &#125; else &#123; publish(msg) &#125; &#125;&#125;","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"mysql repeatable read write skew","slug":"repeatable-read-write-skew","date":"2017-05-12T01:18:24.000Z","updated":"2017-05-15T09:25:14.000Z","comments":true,"path":"2017/05/12/repeatable-read-write-skew/","link":"","permalink":"http://funkygao.github.io/2017/05/12/repeatable-read-write-skew/","excerpt":"","keywords":[],"text":"Isolation教科书里的4种isolation read uncommitted: 即dirty read，可能读到其他rollback的数据 read committed: 即non-repeatable read，同一个txn内读一条数据多次，结果可能不同 repeatable read: 一个txn内读一条数据多次结果相同，但不保证读多个数据的时候也相同(phantom read) serialized 但它只是从锁的实现来描述的，不适用于MVCC。各个数据库产品，虽然采用了这些isolation名字，但语义各不相同，很多与教科书里的定义不符 MySQL不会出现phantom read。MySQL里的RR其实是Snapshot Isolation，只有Serialized是完全基于锁 PostgreSQL实际上只有2个隔离级别：Read Committed和Serialized而Serialized是基于MVCC的无锁实现，即Serialized Snapshot MVCC Snapshot存在write skew问题 甲在一个银行有两张信用卡，分别是A和B。银行给这两张卡总的信用额度是2000，即A透支的额度和B透支的额度相加必须不大于2000：A+B&lt;=2000。 A账号扣款1234567begin;a = select credit from ab = select credit from bif (a + b) + amount &lt;= 2000 &#123; update a set credit = credit + amount&#125;commit B账号扣款1234567begin;a = select credit from ab = select credit from bif (a + b) + amount &lt;= 2000 &#123; update b set credit = credit + amount&#125;commit 假设现在credit(a)=1000, credit(b)=500, 1500&lt;=2000甲同时用a账号消费400，b账号消费300在mysql RR下，2个事务都成功，但2个事务结束后credit(a)=1400, credit(b)=700, 2100&gt;2000 如果是serialized隔离级别，则没有问题：一个事务会失败 在mysql RR下，可以通过应用层加约束来避免write skew 结论for mysql不能期望加了一个事务就万事大吉，而要了解每种隔离级别的语义。 涉及单行数据事务的话，只要 Read Committed ＋ 乐观锁就足够保证不丢写 涉及多行数据的事务的话，Serializable 隔离环境的话不会出错，但是你不会开 如果开 Repeatable Read （Snapshot）隔离级别，那么可能会因为 Write Skew 而丢掉写 如果是金融业务，尽量不要用MySQL","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"dual write conflict","slug":"dual-write-conflict","date":"2017-05-11T23:58:30.000Z","updated":"2017-06-14T07:27:18.000Z","comments":true,"path":"2017/05/12/dual-write-conflict/","link":"","permalink":"http://funkygao.github.io/2017/05/12/dual-write-conflict/","excerpt":"","keywords":[],"text":"Issues race condition partial failure","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"shard scales","slug":"shard-scales","date":"2017-05-11T23:58:16.000Z","updated":"2017-06-02T00:16:47.000Z","comments":true,"path":"2017/05/12/shard-scales/","link":"","permalink":"http://funkygao.github.io/2017/05/12/shard-scales/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"mysql group replication","slug":"mysql-group-replication","date":"2017-05-11T08:50:06.000Z","updated":"2017-05-15T02:12:36.000Z","comments":true,"path":"2017/05/11/mysql-group-replication/","link":"","permalink":"http://funkygao.github.io/2017/05/11/mysql-group-replication/","excerpt":"","keywords":[],"text":"简介GR是个mysql插件，通过原子广播协议、乐观事务冲突检测实现了高可用的多master集群每个master都有全量数据，client side load balance write workload或者使用ProxySQL读事务都是本地执行的有2种模式 单主，自动选主 多主，active active master 与PXC是完全的竞争产品 Requirements and Limitations InnoDB engine only, rollback uncommitted changes turn on binlog RBR GTID enabled each table MUST have a primary key或者not null unique key no concurrent DDL 至少3台master，至多9台，不需要slave auto_increment字段通过offset把各个master隔离开，避免冲突 cascading foreign key not supported 只是校验write set，serializable isolation NOT supported 存在stale read问题，如果write/read不在一台member savepoints可能有问题 Performancehttp://mysqlhighavailability.com/an-overview-of-the-group-replication-performance/ 80% throughput of a standalone MySQL server Internals XCOMeXtended COMmunications，一个Paxos系统 确保消息在所有member上相同顺序分发 动态成员，成员失效检测 理论基础 Database State Machine 事务的Update操作都在一个成员上执行，在Commit时把write-set以total order发送消息给每个成员；每个成员上的certification进程检查事务冲突(first commit wins)，完成最终提交或回滚 Commit时的Paxos有2个作用 certification，检测事务冲突 propagate Group Replication ensures that a transaction only commits after a majority of the members in a group have received itand agreed on the relative order between all transactions that were sent concurrently. 与multi-paxos不同，XCOM是multi-leader/multi-proposer：每个member都是leader of its own slots Certificationgroup_replication_group_name就是GTID里的UUIDGTID就是database versionmysql&gt; select @@global.gtid_executed transaction write set: [{updated_row_pk: GTID_EXECUTED}, {updated_row_pk: GTID_EXECUTED}, …] GTID是由certification模块负责的，由它来负责GTID GNO的increment所有member会定期交换GTID_EXECUTED，所有member已经committed事务的交集：Stable Set. Transaction Distributed Recovery向group增加新成员的过程: 获取missing data，同时cache正在发生的新事务，最后catch up 123456789101112131415161718192021222324252627282930// 从现有member里通过mysql backup工具(mysqldump等)搞个backup instance// phase 0: joinJoiner.join(), 通过total order broadcast发给每个member生成view change binlog event: $viewIDgroup里每个member(包括Joiner)都会收到该view event每个online member会把该binlog event排队到现有transaction queue里// phase 1: row copyJoiner pick a live member from the group as Donor // Donor可能会有lagDoner transmits all data up to the joining moment: master/slave connectionfor &#123; if binlog.event.view_id == $viewID &#123; Joiner.SQLThread.stop() break &#125; if Doner.dies &#123; reselect Donner goto restart &#125;&#125;// phase 2: catch upjoining moment后发生的binlogDonor发给Joiner，Joiner applycatch up同步完成后，declare Joiner online，开始对外服务// Joiner.leave()类似的过程// crash过程会被detector发现，自动执行Joiner.leave() 这个过程与mysql在线alter table设计原理类似 binlog view change markersgroup里member变化，会产生一种新的binlog event: view change log event.view id就是一种logicl clock，在member变化时inrement123+-----------------+| epoch | counter |+-----------------+ epoch在第一个加入group的member生成，作用是为了解决all members crash问题: avoid dup counter certification based replication通过group communication和total order transaction实现synchronous replication 事务在单节点乐观运行，在commit时，通过广播和冲突检测实现全局数据一致性它需要 transactional database来rollback uncommitted changes primary keys to generate broadcast write-set atomic changes global ordering replication events Config123456789101112131415[mysqld]log-binbinlog-format=rowbinlog-checksum=NONEgtid-mode=ONenforce-gtid-consistencylog-slave-updatesmaster-info-repository=TABLErelay-log-info-repository=TABLEtransaction-write-set-extraction=MURMUR32// GRgroup_replication_group_name=&quot;da7bad5b-daed-da7a-ba44-da7aba5e7ab&quot;group_replication_local_address=&quot;host2:24901&quot;group_replication_group_seeds=&quot;host1:24901,host2:24901,host3:24901&quot; FAQGR是同步还是异步？replication分为5步12345master locally applymaster generate binlog eventmaster sending the event to slave(s)slave IO thread add event to relay logslave SQL thread apply the event from relay log GR下，只有3是同步的: 把write set广播并得到majority certify confirm广播时发送消息是同步的，但apply write set还是异步的:12345678910member1: DELETE FROM a; // a table with many rowsmember1: 产生一个非常大的binlog eventmember1: group communicate the binlog event to all members(包括它自己)其他member确认ok，那么member1就返回ok给clientclient访问member1，那么数据是一致的但其他member在异步apply binlog event，可能花很长时间，这时候client访问member2，可能不一致：delete的数据仍然能读出来 Referenceshttp://lefred.be/content/mysql-group-replication-about-ack-from-majority/http://lefred.be/content/mysql-group-replication-synchronous-or-asynchronous-replication/http://lefred.be/content/galera-replication-demystified-how-does-it-work/http://www.tocker.ca/2014/12/30/an-easy-way-to-describe-mysqls-binary-log-group-commit.htmlhttp://mysqlhighavailability.com/tag/mysql-group-replication/http://mysqlhighavailability.com/mysql-group-replication-transaction-life-cycle-explained/","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"mysql在线alter table设计","slug":"osc","date":"2017-05-11T07:42:46.000Z","updated":"2017-05-31T03:05:36.000Z","comments":true,"path":"2017/05/11/osc/","link":"","permalink":"http://funkygao.github.io/2017/05/11/osc/","excerpt":"","keywords":[],"text":"主要逻辑1234567891011121314151617181920212223// 合法性检查// 包括：是否有外键、用户权限、表是否合法、是否有唯一键等// 创建变更记录表CREATE /* changelog table */ TABLE _tbs_c// 创建影子表CREATE /* shadow table */ TABLE _tbl_s LIKE tbl// 在影子表上应用alter语句ALTER TABLE _tbl_s STATEMENT// 开始行拷贝线程 tbl -&gt; _tbl_s// 开始binlog接收和应用线程 binlog -&gt; _tbl_s// 等待行拷贝线程完成// 通知binlog线程收工// 等待binlog线程结束// 开始切换LOCK TABLES tbl WRITERENAME TABLE tbl TO _tbl_old, _tbl_s TO tbl UNLOCK TABLES 确定行拷贝chunk范围123select id from (select id from a where id&gt;=0 and id&lt;=3001 order by id asc limit 1000) select_osc_chunk order by id desc limit 1; 行拷贝in chunk1234567begin;insert ignore into `a`.`_a_gho` (`id`, `value`) (select `id`, `value` from `a`.`a` force index (`PRIMARY`) where (((`id` &gt; ?) or ((`id` = ?))) and ((`id` &lt; ?) or ((`id` = ?)))) lock in share mode )commit; 关键点async binlog worker如何判断所有数据变更已经完成binlog worker向changelog table发一行记录，在收到这个记录时，即表示完成 RENAME race condition with DMLmysql内部保证，LOCK TABLE后，如果有DML与RENAME并发操作，那么在UNLOCK TABLES时，RENAME一定获取最高优先级，即：RENAME一定会先执行。否则，会丢数据:1234567LOCK TABLE WRITEINSERT // blockedRENAME // blockedUNLOCK TABLEINERT // 如果INSERT先执行，那么它会插入原表RENAME // 原表被rename to tbl_old，刚才INSERT的数据丢失: 存放在了tbl_old LOCK, RENAME如果在一个mysql连接内执行LOCK; RENAME，那么会失败解决办法：创建2个mysql连接，分别执行LOCK和RENAME","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"两段锁 2PL","slug":"2PL","date":"2017-05-11T07:33:41.000Z","updated":"2017-05-16T08:51:42.000Z","comments":true,"path":"2017/05/11/2PL/","link":"","permalink":"http://funkygao.github.io/2017/05/11/2PL/","excerpt":"","keywords":[],"text":"事务开始后就处于加锁阶段，一直到执行ROLLBACK和COMMIT之前都是加锁阶段。ROLLBACK和COMMIT使事务进入解锁阶段 事务遵守两段锁协议是可串行化调度的充分条件，而不是必要条件 MS SQL Server默认采用2PL实现isolation，Oralce/PostgreSQL/MySQL InnoDB默认使用MVCC MySQL 2PL提供2种锁 shared(read) exclusive(write) 读写互斥，但读读不互斥 MySQL serialized isolation","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]}]}