{"meta":{"title":"Notes","subtitle":"Quick Notes","description":null,"author":"Funky Gao","url":"http://funkygao.github.io"},"pages":[{"title":"tags","date":"2017-05-15T23:20:30.000Z","updated":"2017-05-19T00:26:43.000Z","comments":true,"path":"tags/index.html","permalink":"http://funkygao.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"知名公司功能模块的实现笔记","slug":"Misc","date":"2017-12-14T02:06:48.000Z","updated":"2017-12-14T08:43:26.000Z","comments":true,"path":"2017/12/14/Misc/","link":"","permalink":"http://funkygao.github.io/2017/12/14/Misc/","excerpt":"","keywords":[],"text":"Storage微信支付的交易记录之前kv，每个用户一个key(相当于redis list)，这样问题是： value会大 无法根据条件filter value 改进后：没有用户多个value，其中1个root value，保存metadata，其他value为data多value解决了以前单value大的问题，但： 多了一次请求先root，再value root成为新瓶颈 可以把root也变成一个link list按照时间倒排，新的是head，老的是tail 但越以前的数据，越慢 算法biparties graph(二部图/二分图)对多种查询条件进行归并缓存，提高缓存命中率 查询条件： 1234(row1, row5) =&gt; (field3, field7)(row2) =&gt; (field2, field4)(row3) =&gt; (field3, field5)(row4, row6) =&gt; (field1, field4, field8) 利用二部图把图切分，把零散查询归并为少数集中的查询","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"},{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"OtterTune","slug":"OtterTune","date":"2017-12-11T01:28:11.000Z","updated":"2017-12-14T09:36:16.000Z","comments":true,"path":"2017/12/11/OtterTune/","link":"","permalink":"http://funkygao.github.io/2017/12/11/OtterTune/","excerpt":"","keywords":[],"text":"DB tuning问题传统方法：DBA把生产库copy到另外一个机器，回放sql，根据metrics尝试调节某一个参数看效果，再尝试多个 数据库的调优参数过多，通常都是数百个，而且新的版本会增加更多参数 同一个db的参数之间是互相依赖的 依赖于运行环境例如，增加innobuffer，在某些条件下是正回馈，但如果物理机内存开始swap，增加它会起副作用 跟业务数据有关 OtterTune解决办法通过controller收集数据库的参数和负载metrics以及hardware profile，定期向中央报告。分析系统从中央取得原始数据，进行分析，提供参数配置建议，生效后，持续分析比较，找出合适的参数配置 target workload latency throughput Controller就是agent,定期把通过配置文件配置的数据库实例的paramters &amp; metrics通过HTTP POST以JSON格式上传同时，它是trial and error的执行者，对参数不断的尝试取得样本数据，它必须有修改数据库参数权限，甚至restart db PostgreSQL SELECT version() parametersSHOW ALL metrics select * from pg_stat_archiver/pg_stat_bgwriter/pg_stat_database/pg_statio_user_indexes/… 分析 通过factor analysis(FA)对收集来的metrics进行降维例如，read_in_bytes/read_in_kbytes 通过k-means对metrics进行聚类 通过Lasso线性回归，来发现哪些参数对target workload有重大影响 Referenceshttp://db.cs.cmu.edu/papers/2017/p1009-van-aken.pdfhttps://github.com/cmu-db/ottertune","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"TimescaleDB","slug":"TimescaleDB","date":"2017-09-28T00:42:41.000Z","updated":"2017-12-14T08:36:05.000Z","comments":true,"path":"2017/09/28/TimescaleDB/","link":"","permalink":"http://funkygao.github.io/2017/09/28/TimescaleDB/","excerpt":"","keywords":[],"text":"time series数据的现有方案问题 传统RDBMS 无法支持high ingest rate，尤其index无法完全放入内存后 delete的成本高时序数据是有retention的 NoSQL和time series databaseCassandar, MongoDBOpenTSDB, InfluxDB 通常缺乏丰富的查询接口，复杂查询是高延时 Hadoop/Sparkingest rate可以高，但查询慢 TimescaleDB解决办法IntroPostgreSQL上的一个插件，目前只支持单机部署，cluster功能还在开发因此，PQ具备的功能它都有，此外还针对tsdb提供了一些方便的函数 hypertable相当于多个chunk(物理)上的逻辑统一 chunk相当于shard，物理属性，通过time interval和[partition key]进行路由chunk就是PQ的table 解决方法high inject rate传统mysql/postgresql，受限于index，如果无法放到内存，性能会大大降低 在100亿条数据的hypertable，单机单磁盘，仍然可以享受10万插入/秒(in batch). TimescaleDB solves this through its heavy utilization of time-space partitioning, even when running on a single machine.So all writes to recent time intervals are only to tables that remain in memory, and updating any secondary indexes is also fast as a result. retention删除数据时，不是delete by row，而是delete by chunk，把整个chunk(table)删除就快了 SELECT drop_chunks(interval &apos;7 days&apos;, &apos;conditions&apos;); chunk partition目前不支持adaptive time intervals，需要在创建hypertable时手工指定chunk_time_interval(默认1个月)","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"blockchain","slug":"block-chain","date":"2017-09-11T00:25:49.000Z","updated":"2017-09-15T02:13:27.000Z","comments":true,"path":"2017/09/11/block-chain/","link":"","permalink":"http://funkygao.github.io/2017/09/11/block-chain/","excerpt":"","keywords":[],"text":"Overview目前公开的bitcoin只能支持每秒7笔的吞吐量，一般对于大额交易来说，安全的交易确认时间为1小时(6个block生成时间) 中本聪设计比特币时，为区块设置了1MB的容量限制，使每一个区块只能容纳4096个交易；同时，工作量证明机制使得确认交易并将交易记录到区块链中需要约10分钟 bitcoin是一种通货紧缩货币，到2140年到达最大值：2100万个比特币 blockchain的所谓去中心化，实际上是建立一个少数服从多数的分布式权威网络，只要这个网络上节点足够多，就是可以信赖的以前，用户trust银行；现在，用户trust majority nodes in blockchain network Bitcoin accounts: addresspublic key: 1MKe24pNsLmFYk9mJd1dXHkKj9h5YhoEeyprivate key: 5KkKR3VAjjPbHPzi3pWEHVQWrVa3C4fwD4PjR9wWgSV2D3kdmeM Currently, the top 10 mining pools consistently create about 90% of the blocks, and China-based pools create more than 60% of the blocks.所以，虽然bitcoin设计上是去中心的，但目前的现状是实际上控制在少数人手里 bitcoin Blocks481,823 Total BTC16.523M Difficulty923233068449 Network total7,983,858 trillion hash per second Blocks/hour7.25 Key DesignBlockChain是以牺牲效率和性能为代价但换来了可靠的安全的最终一致性的分布式数据库 与数据库不同，它不保存状态，只保存commit log，而通过commit log来计算出状态bitcoin的blockchain里并不保存balance，只有[]transaction(inputs, outputs, sender, receiver) no range query blockchain可以想象成只有1个table的replicated database each transaction must commit before next transaction can enter proposal phase throughput bottleneck 关键技术 P2P节点发现和通信 hash, 非对称加密sender: sign(privatekey_of_sender, publickey_of_receiver) merkle tree wallet app Block Merkle Tree每一个block有一个merkle树，叶子节点是transaction，每个中间节点有2个指针(2叉树)如果修改某一个transaction叶子，就会传递到tree root merkle tree，目前还没有在bitcoin中使用，但它的作用是：确认一个交易是否被confirm，只要下载block header+merkle tree，而不用下载整个block Proof of Worksha256(‘pay me, joe, 25 bitcoins. 000001’) =&gt; 4699f0e443b73ddd6fdc14d4662395361fa21f54e647c64e643a49c54fef511csha256(‘pay me, joe, 25 bitcoins. 000013’) =&gt; 0ac09823cf2309c63d425c21b1c3d83f6dbc4b8acfb80b37b2db3d544192b7e9 猜到第13个nonce，就得到1个0开头的hashbitcoin目前的难度是需要14个0开头，实际上每增加1个0，就把原来的计算量提高了16倍 bitcoin sha256(input + nonce)，其中input包括 computer code receiving address(public key) transactions timestamp prev hash Sybil Attack女巫攻击，来源是70年代的一部叫做《Sybil》的美国系列片。片中的女主角人格混乱，扮演着16个角色。 Transaction1234567891. client send payment to replica nodes, proposing new log entries2. proposal会在replica nodes间进行广播(unconfirmed transaction)，进行leader election: proposal phase transaction临时保存在node(miner)的内存3. 通过PoW，leader胜出，本地append到blockchain，并广播到其他replicas miner之间是竞争关系，都在正确尽快solve the puzzle to be leader miner会把unconfirmed transactions进行合并(batch)，生成到一个block(一个block最多4096个transactions)4. 所有replicas append to local blockchain，transaction confirmedblock chain实际上实现了global transaction time order Block Acceptance收到miner发来的新block后，node会very block, verify each transaction, verify not spent都成功，就加入local blockchain 如果local blockchain没有与main blockchain同步，那么verify block会失败，它会通过P2P请求同步，把unsynced blocks追上： 1234node1: block 1, 2, 3, 4main blockchain: 1, 2, 3, 4, 5, 6, 7此时node2成功创建了block8，并广播给node1node1发现block 5-7还没有同步过来，先进行同步，然后再把block8 append to local blockchain Confirm确认数 = 该交易所在的block在blockchain中后面的block数量 + 1如果确认数 &gt;= 6，那么就可以认为无法逆转了 Consensus通过PoW，每次生成block时进行一次leader选举，由leader生产new block，猜数成功的node自认为成为了leader，然后通过P2P广播(gossip)由于猜数比较困难，多节点同时成为leader并且在接收到其他leader广播前猜成功的可能性非常小，但还是存在可能性，即多主，这就需要解决冲突 miner一旦recv a new block，就意识到在这个block的race上自己输了，它会立即把一些pending transactions和这个收到的block作为prev hash来构建下一个block Conflict Resolve采用longest chain rule，发现冲突时，block chain fork branch，在发现longest chain时，把short chain删除：但可能会造成confirmed transaction lost Double Spent Public keys as identitiesAlice pays Bob，Alice会把这个交易通过private key进行签名，广播给miners Bob不需要参与 交易信息里只包含Alice和Bob的public keys(address) You can create a new identity at any time by generating a new key pairwith no central authority or registry 验证身份verify 比特币的所有权是通过数字密钥、比特币地址、数字签名来确立的密钥实现了去中心化的信任，所有权认证 创建交易时，payer利用自己的private key给transaction签名，同时把自己的public key存放在消息里：(payer_public_key, signature)miners通过这个信息，就可以verify这个transaction确实是payer发出的同时，transaction里也包含了payee的public key信息，只有payee利用他的private key才能解开 key pair，非对称加密的特性： 用私钥加密的内容，只能用公钥解密如果能通过(payer_public_key, signature)能解密，就证明了payer的身份 用公钥加密的内容，只能用私钥解密只有payee才能用他的私钥解密交易每个交易必须有效的签名才能被存入ledger当payee花销这笔钱时，他必须给这个交易签名，由于这笔钱已经在ledger里记录了payee的公钥，只有payee才能签名 公钥用于收钱，私钥用于花钱时生成数字签名 通过私钥能计算出公钥，但反过来不行只要私钥不丢，公钥丢了也没关系 如果我知道Alice的public key(X)，然后创建一笔交易：X支付给me 1BTC，系统是怎么知道有诈的?首先我不知道X对应的私钥，只能拿我的私钥对交易加签名，miner通过X和signature就能验证：invalid signature Upgradehttps://github.com/bitcoin/bips BIP(bitcoin improvement proposal)在github上发布，miners自愿地下载、启动，但不是强制的 Q &amp; A为什么blocks通过hash进行chain，而不是通过普通的编号？因为hash具有不可篡改性，hash本身包含了内容的指纹 同时有很多client进行交易，full node一直在瞎忙活？T1, T2, Tn被分发到n个full node，那么每个node都开始猜数，其中一个node(N1)猜中，开始广播，此时N2~Nn还在为自己的Tn进行猜数，发现new block，就停下手中的活，重新生成新的block，并重新猜数如果N1在广播时，消息还没有传到Nx，而此时Nx的猜数工作是不会停的；如果此时Nx也猜数成功，那么在它还没有收到N1广播前，就会广播自己的new block，此时fork出现对于某一个full node，它可能会并发接收多个交易请求，它会进行串行化 如果识别一个account通过wallet程序，每次交易都可以生成新的public/private key pair，由wallet管理并保存。如果wallet的数据丢了，那么bitcoin就无法证明是你的了 如何动态维护PoW的难度？每个节点计算最近2016个solution，因为频率应该是10分钟一个，因此2016个，应该是2016/(6*24)=14天而每个block里都有timestamp1234elapsed = timestamp(block(-2016)) - timestamp(block(-1)) // 例如，假如elapsed = 7 day，那么说明难度系统太低了// 即，移动平均数difficulty = difficulty * 2 如何保证bitcoin address唯一？Bitcoin address是由34个alphanumerical组成的字符串，但排除了O/T/l/0，它的空间是58^34，即904798310844700775313327215140493940623303545877497699631104但里面有几位用于checksum，它的实际空间是2^160，即1461501637330902918203684832716283019655932542976 一个恶意node可以做什么，不能做什么？它能 Refuse to relay valid transactions to other nodes但其他节点会replay Attempt to create blocks that include or exclude specific transactions of his choosing Attempt to create a ‘longer chain’ of blocks that make previously accepted blocks become ‘orphans’ and not part of the main chain 它不能 Steal bitcoins from your account Make payments on your behalf or pretend to be you 为什么建议每次transaction都新生成key pair？公钥虽然没有安全问题，但有隐私，毕竟它属于某一个人，如果我的交易都用一个key pair，那么最终这些交易可以发现是一个人的行为 如果写错了接受者payee公钥，这笔钱能回来吗？首先，公钥(bitcoin address)是有校验位的，写错的时候，基本上就可以在提交时离线发现如果恰巧校验一致，而地址是不存在的，那么交易会成功，但由于那个payee不存在，也就不存在对应的private key，也就无法spend it：黑洞，那笔钱永远消失了 如果本来我想给A钱，却输入时写成了B的address，那么：Bitcoin transactions are not reversible. Sending to the wrong person cannot be undone. Block timestamp如果做假怎么办？A timestamp is accepted as valid if it is greater than the median timestamp of previous 11 blocks, and less than the network-adjusted time + 2 hours.“Network-adjusted time” is the median of the timestamps returned by all nodes connected to you. Whenever a node connects to another node, it gets a UTC timestamp from it, and stores its offset from node-local UTC. The network-adjusted time is then the node-local UTC plus the median offset from all connected nodes. Network time is never adjusted more than 70 minutes from local system time, however. 如果真有人控制了51%计算能力，会发生什么？attacker只能把他刚花的钱payment取消掉，即double spent 据说bitcoin最大吞吐量是7 TPS，怎么计算来的？每个block最大1MB，4096个transaction，10分钟产生一个block 4096/(10*60) = 6.83 = 7 miner做恶的惩罚miner竞争成功后，创建一个block，它会获得奖励；如果它发布一个非法的block，那么大部分miners会拒绝；并在下一个block时，该非法block被取消，同时它的奖励也被取消。除非，它能保证它一直是竞争成功 seed nodes?hard codedhttps://github.com/bitcoin/bitcoin/blob/863e995b79ec388bf292d80f181912d01e20e2e5/src/net.cpp#L1198 12345678unsigned int pnSeed[] =&#123; 0x959bd347, 0xf8de42b2, 0x73bc0518, 0xea6edc50, 0x21b00a4d, 0xc725b43d, 0xd665464d, 0x1a2a770e, 0x27c93946, 0x65b2fa46, 0xb80ae255, 0x66b3b446, 0xb1877a3e, 0x6ee89e3e, 0xc3175b40, 0x2a01a83c, 0x95b1363a, 0xa079ad3d, 0xe6ca801f, 0x027f4f4a, 0x34f7f03a, 0xf790f04a, 0x16ca801f, 0x2f4d5e40, 0x3a4d5e40, 0xc43a322e, 0xc8159753, 0x14d4724c, 0x7919a118, 0xe0bdb34e, 0x68a16b2e, 0xff64b44d, // 列出了500多个节点&#125; Referenceshttps://bitcoincharts.com/https://blockchain.info/https://en.bitcoin.it/wiki/Vocabularyhttp://www.cs.rice.edu/Conferences/IPTPS02/101.pdfhttps://www.cryptocompare.com/wallets/guides/how-to-create-a-bitcoin-address-from-a-public-key/","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"perceptual hash","slug":"perceptual-hash","date":"2017-08-23T23:43:24.000Z","updated":"2017-09-13T00:57:36.000Z","comments":true,"path":"2017/08/24/perceptual-hash/","link":"","permalink":"http://funkygao.github.io/2017/08/24/perceptual-hash/","excerpt":"","keywords":[],"text":"这种算法的优点是简单快速，不受图片大小缩放的影响，缺点是图片的内容不能变更。如果在图片上加几个文字，它就认不出来了。所以，它的最佳用途是根据缩略图，找出原图。 实际应用中，往往采用更强大的pHash算法和SIFT算法，它们能够识别图片的变形。只要变形程度不超过25%，它们就能匹配原图。这些算法虽然更复杂，但是原理与上面的简便算法是一样的，就是先将图片转化成Hash字符串，然后再进行比较。 比较图片的相似度图片尺寸可以不同，不同的长宽比例，小范围的颜色不同(亮度、对比度等)对旋转都不具有鲁棒性：但可以sample argument 12345678910111213img = resize(img, (8, 8))img.grayscalize(level=64)avg = img.average() // e,g. 78for i, b = range img &#123; // (8, 8) =&gt; img[0...63] if b &gt;= avg &#123; img[i] = 1 &#125; else &#123; img[i] = 0 &#125;&#125;// img现在就是一个64位的二进制整数, 这就是这张图片的指纹HanmingDistance(img1, img2) // 汉明距离，如果小于5就说明两副图相似","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"GFS evolution and BigTable","slug":"GFS","date":"2017-07-25T06:39:34.000Z","updated":"2017-07-31T00:31:39.000Z","comments":true,"path":"2017/07/25/GFS/","link":"","permalink":"http://funkygao.github.io/2017/07/25/GFS/","excerpt":"","keywords":[],"text":"GFSGFS，2001年开发出来，3个人，1年，论文发表于2003BigTable，2004年开发出来，论文发表于2006 master replicated operation(redo) log with checkpoint to shadow masters 当时没有自动failover，全是人工操作DNS alias read请求可以使用shadow masters 生成checkpoint时，master切换到新的redo log file，创建新线程生成checkpoint在此过程中发生的变化，记录到新redo log file operation log保证了mutation的全局有序 save checkpoint很可能是通过mmap+msync bottleneck? 当时部署了多个cluster，根据业务属性最大的超过1000节点，300TB 过高的OPS由于redo log需要复制，master的tps也就在1万左右 client cache何时invalidate and recall master? primary unreachable primary reponse: no lease client batch request lease 内存容量 prefix compression to reduce memory footprint 每个chunk(64MB)，metadata占内存64B如果保存1百万个文件(64TB)，需要64MB内存如果保存10亿个文件(64PB)，需要64GB内存实际上是到了5千万个文件，10PB的时候，master已经到达瓶颈了 metadata1234567891011type metadata &#123; filename string // prefix compression owner uid perm acl chunks []chunk &#123; chunk_id int64 version int refCount int // for snapshot COW chunkservers []string // ip addr &#125;&#125; deleterename to hidden file，保留3天，之后真正删除 write/mutation operation log replicated between master and shadows client write data flow to pipelined chain chunkservers primary write control flow to secondary chunkservers read(filename, offset, size)123456789client根据(offset, size)计算出需要哪些chunks，假设chunk1, chunk2client把(filename, chunk index [1, 2])类似batch一样发送mastermaster返回&#123;chunk1: &#123;chunkId1, [ip1, ip2, ip3]&#125;, chunk2: &#123;chunkId2, [ip4, ip5, ip6]&#125;&#125;client.putInCache(metadata)client顺序地获取chunk1, chunk2对于chunk1，根据ip1-3与client ip的距离，找最近的，取chunk；如果失败，找下一个最近的chunkserverclient.sendToIp1(chunkId1, offset1, size1)// stale read是存在的，read的时候chunkserver可能发现data corruption 磁盘错误的解决file -&gt; chunk -&gt; block 在每个chunkserver上执行，通过checksum检测 每个chunk由多个64KB的block组成，每个block有一个32位的checksum read repair，如果chunkserver发现了一个非法block，会返回client err，同时向master汇报 client会从其他replica read master会从其他有效的replica把这整个chunk clone到另外一个chunkserver，然后告诉有问题的chunkserver删除那个chunk Questionschunk为什么64MB那么大? 减少master内存的占用 减少client与master的交互同一个chunk的R/W，client只需要与master交互一次 可以很容易看到机器里哪些机器磁盘快满了，而做迁移 可以减少带宽的hotspot如果没有chunk，那么1TB的文件就只能从一个replica读有了chunk，可以从多个replica读 sharding the load 加快recovery时间每个chunk，可以很快地被clone到一台新机器 如果一个file只有1MB，那么实际存储空间是1MB，而不是64MB但它会增加master file count问题 可以独立replicate 文件的一部分损坏，可以迅速从好的replica恢复 支持更细粒度的placement 支持超大文件 chunk hotspot问题MapReduce时，需要把代码发布到GFS，很可能小于64MB，即只有1个chunk，当很多mapper时，这个chunkserver就成为hotspot解决办法是：增加replication factor 但最终的解决方案是：client间P2P，互相读 master为什么不用Paxos?master通过redo log sync replication来提高可靠性，但没有election过程，都是完全手工进行failover 我猜，chubby当时还没有启动，chubby论文发表于2006 master为什么不持久化chunk location?其他的metadata是有redo log replication并持久化的，他们的变化，都是主动产生的，例如创建一个文件，增加一个chunk而由于chunkserver随时可能crash，不受控制，因此通过heartbeat来计算并存放内存，通过heartbeat，master又可以修正chunkserver的一些错误，例如orphan chunk Data flow为什么pipelined chain，而不并发?为了避免产生网络瓶颈，同时为了更充分利用high latency links通过ip地址，感知chunkserver直接的距离 Total Latency = (B/T) + (R*L) 2PC，避免了client(coordinator) crash问题，因为primary成为了coordinator，而它是有failover的12345678910111213client负责把一个write请求分成多个chunk的请求Phase1: data flow client -&gt; chained chunkservers相当于prepare，但数据不落盘client由近及远地chain把数据写入相应chunkserver的LRU buffer这个顺序跟primary在哪无关Phase2: control flow client -&gt; primary -&gt; secondary chunkservers相当于commit，数据visible确定mutation orderPhase1出错，则等master来修复，把crashed chunkserver摘除Phase2出错，primary-&gt;secondary，这个阶段，那么primary返回client err，client会重试，此时可能出现不一致的状态，但最终master会修复 为什么搞个primary角色，而不让master做?为了减轻master负担，所以搞了个二级调度:跨chunk，master负责；chunk内部，primary负责 master如何revoke primary lease?在lease expire后，master可能什么都不做在lease expire前，master会sendto(primary)让它取消；如果sendto失败，那么只能等expire 为什么data flow和control flow分开?如果不分开，那么所有的数据都是client-&gt;primary-&gt;secondary分开后，比较轻量级的control flow必须走primary扩散；重量级的data flow可以根据物理拓扑进行优化 GFS vs Ceph 论文2003 vs 2006 chunk(64MB) vs Object(4MB)object size可配 master vs mon(Paxos) chunkserver vs osd replication GFS2PC, decouple data/control flow Cephclient &lt;-&gt; osd Ceph通过PG+crunch提高了扩展性GFS通过allocation table的方式 GFS上直接跑MapReduce计算向存储locality Ceph更通用，latency更好GFS通过lease提高扩展性，但遇到错误时只能等expire 节点的变化 GFSchunkserver向master汇报，自动加入，完全不需要人工参与 Ceph需要通过ceph osd命令，手工执行 namespaceGFS是directory，Ceph是flat object id 2009 GFS回顾GFS在使用了10年的过程中，发现了一些问题，对这些问题，有的是通过上层的应用来解决的，有的是修改GFS解决的 master ops压力最开始的设计，考虑的是支撑几百TB，几百万个文件。但很快，到了几十PB，这对master有了压力 master在recover的时候，也变慢 master要维护的数据更多 client与master的交互变慢每次open，client都要请求masterMapReduce下，可能突然多出很多task，每个都需要open，master处理能力也就是每秒几千个请求解决办法是在应用层垂直切分，弄多个cluster，应用通过静态的NameSpace找自己的master，同时提升单个master能力到数万ops 随着GFS的内部推广，越来越多的千奇百怪的上层应用连接进来 最开始是爬虫和索引系统 然后QA和research组用GFS来保存large data sets 再然后，就有50多个用户了 在此过程中GFS不断地调整以满足新use case file-count问题很早就发现了，例如： 前端机上要把log发到GFS保存以便MapReduce分析，前端机很多，每个log每天会logrotate，log的种类也越来越多 gmail需要保存很多小文件解决办法是把多个文件合并，绕开file-count问题，同时增加quota功能，限制file-count和storage space长远的办法：在开发distributed multi-master系统，一个cluster可以有上百个master，每个master可以存1亿个文件，但如果都是小文件，会有新的问题出现：more seek再后来，建立在GFS之上的BigTable推出了，帮助GFS直接面对应用对小文件、多文件的需求，BigTable层给解决了，BigTable在使用GFS时，仍然是大文件、少文件 latency问题GFS设计是只考虑吞吐率，而少考虑latency error recovery慢如果write一个文件，需要写到3个chunkserver，如果其中一个卡住了或crash，master会发觉(heartbeat)，它会开新的一个chunkserver replica从其他chunkserver pullmaster会把这个lock，以便新的client不能write(等恢复后再unlock)而这个pullchunk操作，为了防止bandwidth burst，是有throttle的，限制在5-10MB/s，即一个64MB chunk，需要10s左右等恢复到3个ok的时候再返回给client，client再继续write在此过程中，client一直是block的 master failover慢刚开始master failover完全靠人工，可能需要1h；后来增加了自动master failover，需要几分钟；再改进，可以在几秒钟内完成master自动切换 为吞吐量而设计的batch增加latency解决办法BigTable是无法忍受那么高的延时的，它的transaction log是最大的瓶颈，存储在GFS：2个log(secondary log)，一个慢，就切换到另外一个，这2个log任意时刻只有1个active，并且log entry里有sequence号，以便replay时防重google使用这个request redundancy or timeout方法很广泛，为了解决search long tail latency，一样思路 Gmail是多机房部署的，一个卡了，切到另外机房 consistencyclient一直push the write till it succeeds但如果中途client crash了，会造成中间状态：不同client读同一个文件，可能发现文件长度不同解决办法：使用append，offset统一由primary管理 但append由于没有reply保护机制，也有问题：client write，primary分配一个offset，并call secondary，有的secondary收到有的没收到，此时primary crashmaster会选另外一个作为primary，它可能会分配一个新的offset，造成该数据重复如果为此再设计一套consensus，得不偿失解决办法：single writer，让上层应用保证不并发写 ColossusColossus is specifically designed for BigTable, 没有GFS那么通用In other words, it was built specifically for use with the new Caffeine search-indexing system, and though it may be used in some form with other Google services, it is not the sort of thing that is designed to span the entire Google infrastructure. automatically sharded metadata layer EC client driven replication metadata space has enabled availability analysis BigTable 在有了GFS和Chubby后，Google就可以在上面搭建BigTable了，一个GFS的索引服务但BigTable论文对很多细节都没有提到：SSTable的实现、tabletserver的HA，B+数的metadata table算法 为了管理巨大的table，按照row key做sharding，每个shard称为tablet(100-200MB，再大就split)，每台机器存储100-1000个tabletrow key是一级索引，column是二级索引，版本号(timestamp)是三级索引 tabletserver没有任何的持久化数据，只是操作memtable，真正的数据存放在哪里只有GFS知道，那为什么需要master在chubby上分配tablet给tabletserver?因为memtable是有状态的: level0 tabletserver的HA?通过chubby ephemeral node，死了master会让别的server接管，通过GFS上的redo log恢复memtable为了保证强一致性系统，同一时刻同一份数据只能一台tabletserver服务，tabletserver对每个tablet是没有备份的当它crash，由于只需要排序很少的操作日志并且加载服务的tablet的索引，宕机恢复可以做到一分钟以内；在此期间，一部分rowkey不可用 split and migration?在没有crash情况下，只需要修改metadata和从sstable加载索引数据，效率很高 与GFS的对应 commit log每台机器一个commit log文件，与GFS File一一对应 sstableHBase中Column Family的名称会被作为文件系统中的目录名称，每个CF存储成一个HDFS的HFile据google工作过的人说：Column Families are stored in their own SSTable，应该是这样sstable对应一个GFS Filesstable block=64KB，它与GFS的block相同sstable block为了压缩和索引(binary search)，GFS block为了checksum Highlightsredo log合并一台机器一个redo log，而不是一个tablet一个redo log(每个机器有100-1000个tablet)，否则GFS受不了group commit 带来的问题：恢复时麻烦了如果一天机器crash了，它上面的tablets会被master分配到很多其他的tabletserver上例如，分配到了100台新tabletserver，他们都会read redo log and filter，这样redo log被读了100次解决办法：利用类似MapReduce机制，在recovery之前先给redo log排序 加速tablet迁移12345sourceTablet.miniorCompaction() // 把memtable里内容dump到GFS的SSTablesourceTablet.stopServe()sourceTablet.miniorCompaction() // 把in-flight commit log对应的操作也持久化到GFS // 这样targetTablet就不需要从commit log recover了master.doSwitch() SSTable由多个64KB的block组成压缩以block为单位，虽然相比在整个SSTable上压缩比小(浪费空间)，但对于随机读，可以只uncompress block而非整个SSTable 经验和教训遇到了新的问题 发现了Chubby的bug network corruption通过给RPC增加checksum解决 delay adding features until clear how it will be used刚开始想给API增加一个通用的事务机制，后来发现大部分人只需要单行事务 不仅监控server，也监控client扩展了RPC，采样detailed trace of important actions 设计和实现都要简单、明了BigTable代码10万行C++tabletserver的membership协议的设计，最初：master给tabletserver发lease结果：在网络出问题时大大降低了可用性(master无法reach tabletserver就只能等expire)改进：实现了更复杂的协议，也利用了Chubby里非常少见的特性结果：大量时间在调试edge case，很多时间在调试Chubby的代码最终：回到简单的设计，只依赖Chubby，而且只使用它通用的特性 Questions按照rowkey来shard，那么可能造成hotspot问题，client端比较难控制 2009 BigTable回顾部署了500+个BigTable集群，最大的机器：70+ PB data; sustained: 10M ops/sec; 30+ GB/s I/O Lots of work on scaling Improved performance isolation Improved protection against corruption Configure on per-table 异地机房复制: 增加了最终一致性模型 CoprocessorReferences http://queue.acm.org/detail.cfm?id=1594206http://google-file-system.wikispaces.asu.edu/http://static.usenix.org/publications/login/2010-08/openpdfs/maltzahn.pdfhttps://stephenholiday.com/notes/bigtable/","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"replication models","slug":"replication","date":"2017-07-19T11:12:26.000Z","updated":"2017-07-27T03:43:36.000Z","comments":true,"path":"2017/07/19/replication/","link":"","permalink":"http://funkygao.github.io/2017/07/19/replication/","excerpt":"","keywords":[],"text":"Models primary-replica(failover, replica readable)/multi-master/chain replica/ec push/pull sync/async 12345678910Dynamo PUSH coordinator -&gt; preference list next nodes in hash ringRaft/ZK PUSH master commit log -&gt; majority ackGFS/Hadoop chain replicationES PUSH primary-replica model, master concurrently dispatch index req to all replicasMySQL PUSH master async PUSH to slaves binlogKafka PULL(Fetch) ISRredisCeph PUSH primary-replica sync model with degrade mode 同步写入SwiftAurora PUSH primary instance R(3)+W(4)&gt;N(6)","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Google container evolution","slug":"Google-container-evolution","date":"2017-07-19T01:45:30.000Z","updated":"2017-07-19T02:25:47.000Z","comments":true,"path":"2017/07/19/Google-container-evolution/","link":"","permalink":"http://funkygao.github.io/2017/07/19/Google-container-evolution/","excerpt":"","keywords":[],"text":"Babbysitter &amp; GlobalWorkQueue -&gt; cgroup -&gt; Borg -&gt; Omega -&gt; Kubernetes Container容器提供的隔离还不完善，OS无法管理的，容器无法隔离(例如CPU Cache，内存带宽)，为了防止(cloud)恶意用户，需要在容器外加一层VM保护 container = isolation + image 数据中心：机器为中心 -&gt; 应用为中心在Application与OS之间增加一层抽象：containercontainer的依赖大部分都存放在image了，除了OS的系统调用实际上，应用仍然会受OS影响，像/proc, ioctl 好处 更高的资源利用率 监控的是应用，而不是混在一起的机器 开发者、管理员不需关心OS load balancer不再balance traffic across machines: across application instances logs are keyed by application, not machine 可以更容易识别是应用的失败还是机器的失败 教训 不要给container端口号Borg做法是物理机上的所有容器共用主机的ip，每个容器分单独的端口号Kubernetes为每个pod分配不同ip，network身份=application身份 不要给container编号用label 不用暴露raw state减少client端的复杂度K8s里，都通过API Service访问状态信息 还没有很好解决的问题 Configuration Dependency management手工配，最终一定不一致自动，无法获取足够的语义信息","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Ceph Internals","slug":"Ceph","date":"2017-07-18T07:27:20.000Z","updated":"2017-07-27T07:50:15.000Z","comments":true,"path":"2017/07/18/Ceph/","link":"","permalink":"http://funkygao.github.io/2017/07/18/Ceph/","excerpt":"","keywords":[],"text":"RADOSKey ideas 把传统文件系统架构分隔成client component and storage componentclient负责更上层的抽象(file, block, object)，storage负责底层object, disk seperation of data and metadata各管各的 ObjectObject是最终落地存储文件的基本单位，可以类比fs block，默认4MB如果一个文件 &gt; object size，文件会被stripped，这个操作是client端完成的 It does not use a directory hierarchy or a tree structure for storageIt is stored in a flat-address space containing billions of objects without any complexity Object组成 key value在fs里用一个文件存储 metadata可以存放在扩展的fs attr里因此对底层文件系统是有要求的，ext4fs对extended attr数量限制的太少，可以用xfs 12345 +---logical------+ +--physical layer -+ | | | |cluster -&gt; pool -&gt; pg ---&gt; osd -&gt; object -&gt; fs files | | +-crush-+ Lookup1234567891011file = inoobj = (ino, ono)obj_hash = hash(ino, ono)pg = obj_hash &amp; (num_pg-1) // num_pg是2整数幂，例如codis里1024 // 官方推荐num_pg是osd总数的数百倍 // num_pg如果变化，会引起大量的数据迁移osds_for_pg = crush(pg) // list of osds，此处是唯一需要全局cluster map的地方 // 其他的，都是算出来的 // osds_for_pg is ordered，len(osds_for_pg)=replicate factorprimary = osds_for_pg[0]replicas = osds_for_pg[1:] crush一个hash算法，目标 数据均匀的分布到集群中 需要考虑各个OSD权重的不同（根据读写性能的差异，磁盘的容量的大小差异等设置不同的权重） 当有OSD损坏需要数据迁移时，数据的迁移量尽可能的少 有点像一致性哈希：failure, addition, removal of nodes result in near-minimal object migration PGPG(placement group)与Couchbase里的vbucket一样，codis里也类似，都是presharding技术 在object_id与osd中间增加一层pg，减少由于osd数量变化造成大量的数据迁移PG使得文件的定位问题，变成了通过crush定位PG的问题，数量大大减少例如，1万个osd，100亿个文件，30万个pg: 100亿 vs 30万 线上尽量不要更改PG的数量，PG的数量的变更将导致整个集群动起来（各个OSD之间copy数据），大量数据均衡期间读写性能下降严重 1Total PGs = (Total_number_of_OSD * 100) / max_replication_count 12345678$ceph osd map pool1 object1osdmap e566 pool &apos;pool1&apos; (10) object &apos;object1&apos; -&gt; pg 10.bac4decb (10.3c) -&gt; up [0,6,3] acting [0,6,3]$// osdmap e566: osd map epoch 566// pool &apos;pool1&apos; (10): pool name and pool id，pool id从0开始，每新建+1// pg 10.bac4decb (10.3c): placement group number 10.3c(pg id是16进制，256个pg，那么他们的id: 0, 1, f, fe, ff) pg id实际上是pool_id.pg_id(pool id=10, pg id=3c)// up [0,6,3]: osd up set contains osd.0, osd.6, osd.3 同一个PG内的osd通过heartbeat相互检查对方状态，大部分情况下不需要mon参与，减少了mon负担 mon相当于Ceph的zookeeper mon quorum负责整个Ceph cluster中所有OSD状态(cluster map)，然后以增量、异步、lazy方式扩散到each OSD和clientmon被动接收osd的上报请求，作为reponse把cluster map返回，不主动push cluster map to osd如果client和它要访问的PG内部各个OSD看到的cluster map不一致，则这几方会首先同步cluster map，然后即可正常访问 MON跟踪cluster状态：OSD, PG, CRUSH maps同一个PG的osd除了向mon发送心跳外，还互相发心跳以及坚持pg数据replica是否正常 Replication是primary-replica model, 强一致性，读和写只能向primary发起请求其中replicas在复制到buffer时就ack，如果某个replica复制时失败，进入degrade状态 2PC Write 123456phase1: client从primay接收ack此时，数据已经replicate到每个replica的内存phase2: client从primary接收commit此时，数据已经replicate到每个replica的磁盘client才把本地的write buffer cache清除 scrub机制read verify Questionsconsistency?strongly consistent Can Ceph replace facebook Haystack?Haystack解决的实际上是metadata访问造成的IO问题，解决的方法是metadata完全内存，无IO每个图片的read，需要NetApp进行3次IO: dir inode, file inode, file datahaystack每个图片read，需要1次IO，metadata保证在内存在一个4TB的SATA盘存储20M个文件，每个inode如果需要500B，那么如果inode都存放内存，需要10GB内存 减少每个metadata的大小去掉大部分不需要的数据，例如uid, gid, atime等xfs里每个metadata占536字节，haystack每个图片需要10字节 减少metadata的总数量多个图片(needle)合并进一个大文件(superblock) 用户上传一张图片，facebook会将其生成4种尺寸的图片，每种存储3份，因此写入量是用户上传量的12倍。 Posix规范里的metadata很多对图片服务器不需要，12345678910111213141516struct inode &#123; loff_t i_size; // 文件大小 struct list_head i_list; // backing dev IO list struct list_head i_devices; blkcnt_t i_blocks; struct super_block *i_sb; struct timespec i_atime; struct timespec i_mtime; struct timespec i_ctime; umode_t i_mode; uid_t i_uid; gid_t i_gid; dev_t i_rdev; unsigned int i_nlink; ...&#125; Ceph解决了一个文件到分布式系统里的定位问题(crush(pg))，但osd并没有解决local store的问题: 如果一个osd上存储过多文件(例如10M)，性能会下降明显 Why EC is slow?Google Colossus采用的是(6,3)EC，存储overhead=(6+3)/6=150%把数据分成6个data block+3个parity block，恢复任意一个数据块需要6个I/O生成parity block和恢复数据时，需要进行额外的encode/decode，造成性能损失 Store small file, waste space?object size是可以设置的 Referenceshttp://ceph.com/papers/weil-crush-sc06.pdfhttp://ceph.com/papers/weil-rados-pdsw07.pdfhttp://ceph.com/papers/weil-ceph-osdi06.pdfhttp://www.xsky.com/tec/ceph72hours/https://blogs.rdoproject.org/6427/ceph-and-swift-why-we-are-not-fightinghttps://users.soe.ucsc.edu/~elm/Papers/sc04.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Dynamo - A flawed architecture","slug":"Dynamo-flaw","date":"2017-07-18T00:35:12.000Z","updated":"2017-07-19T00:23:54.000Z","comments":true,"path":"2017/07/18/Dynamo-flaw/","link":"","permalink":"http://funkygao.github.io/2017/07/18/Dynamo-flaw/","excerpt":"","keywords":[],"text":"AuthorJoydeep Sen Sarma，印度人，07-11在facebook担任Data Infrastructure Lead，目前在一个创业公司任CTO在facebook期间，从0搭建了数千节点的hadoop集群，也是Cassandra的core committer，参与开发Hivehttps://www.linkedin.com/in/joydeeps/ 编写在2009年，Dynamo paper发布于2007 ‘Flaws’R+W&gt;N作者主要以Cassandra的实现来评论Dynamoc论文，从而忽略了vector clock逻辑时钟来跟踪数据版本号来检测数据冲突因此，遭到很多围观群众的声讨 作者的主要思想是：由于hinted handoff，缺少central commit log，缺少resync/rejoin barrier，transient failure会导致stale read Cassandra解决办法，是尝试通过central commit log，那就变成了Raft了https://issues.apache.org/jira/browse/CASSANDRA-225 作者如果提到下面的场景，可能更有说服力12345678910// W=3 R=3 N=5(1,2,3,4,5)Op W R==== ===== =====a=5 1,2,3 3,4,5 // 由于有节点overlap，写入的数据可以读出来2,3crash del(a) 1,4,52,3recover 1,2,3 // 2,3读出a=5，而1没有key(a)，如何处理冲突? 此外，由于sloppy quorum，下面的场景R+W&gt;N也一样无法保证一致性123456789101112// W=3 R=3 N=5(1,2,3,4,5)ring(A, B, C, D, E, F, G, A)client set(k1)=3 // md5(k1)，发现它的coordinator是A，client send req to AA会先写本地磁盘，然后并发向B,C复制，成功后，return to client ok现在A, B, C全部crashset(k1)=8 // 由于hinted handoff，它的coordinator变成DD写盘、复制到E,F，ok。// 当然D,E,F里的数据有hint metadata，表示他们只是代替A,B,C // 等A,B,C活，再transfer to them and cleanup local store然后A, B, C全部recoverget(k1)，读到是3，而不是8// 因为D,E,F的检测A/B/C活以及transfer hinted data是异步的 WAN如果一个node crash，’hinted handoff’会把数据写入一致性哈希环上的下一个node：可能是另外一个DC node等crashed node恢复，如果网络partitioned，这个node会很久无法赶上数据，直到partition解除 但preference list里，已经考虑到了，每个key都会复制到不同的机房 论文里的”facts” 论文提到，商业系统通常通过同步复制保证一致性大部分数据库的复制都是异步的 论文提到，中心化架构降低availability不对，中心化会造成扩展瓶颈，并不会降低可用性，SPOF有非常多的方法解决 Werner Vogels的回复Dynamo SOSP论文有2个目的 展示如何利用多种技术创建一个生产系统 对学术界的一个反馈，学术到生产环境遇到的困难和解决办法 本论文不是一个blueprint，拿它就可以做出一个Dynamo clone的 我认为我的论文真正的贡献，是让人设计系统时的权衡trade-off Dynamo回顾Java实现的，通过md5(key)进行partition，由coordinator node复制read/write/replicate到一致性哈希虚拟节点环上gossip进行membership管理和健康检查，提出了R/W/N的quorum模式 no updates are rejected due to failures or concurrent writes zero-hop DHT为了latency，每个node有全局的路由信息，不产生hop resolve update conflicts，在read时由应用做，write时不检测 这与大部分系统相反，原因是为了always writeable read时，让应用处理冲突，而不是Dynamo store做store做，由于抽象层，只能简单的类似last win的策略应用更懂，可以做类似union等高级处理策略 md5(key) =&gt; target nodeshash conflict? 可以不用考虑，无非就是让一台机器多处理了一个key而已 vector clock[(node, counter), (node, counter), …]get(key)返回的ctx，在put(key, ctx, val)时会带过去：ctx里包含该key的vector clock信息 R W Nget/put latency都取决于R/W里最慢节点的latency get(key)如果没有冲突，返回一个值；发现冲突，返回list返回的每个value都有对应的一个context(vector clock version) replication write coordinatorput(key, ctx, val)根据md5(key)计算出coordinator node，它负责写入本地磁盘，同时向顺时针后面的N-1个alive节点进行复制由于后面N-1节点可能come and go，它是动态的，coordinator盲目地找出后面活着的：这样才能有高可用 sloppy quorum coordinator and replication负责生成新的vector clock，并把(key, val, vc)写入本地同时向顺时针后面的N-1个healthy节点进行复制(dead nodes跳过)但由于虚拟节点的存在，需要保证复制到的节点不在一台机器上preference list是一个key对应的storage node，在ring上，如果顺时针发现有机器重叠就忽略并继续顺时针找 节点的加入和退出手动显示 local persistence engine插件架构，amazon主要使用的是BerkelayDB，但也可以使用mysql/BDB Java等 SLA99.9% 300ms Replication每个key都被复制在N台机器(其中一台是coordinator)，coordinator向顺时针方向的N-1个机器复制preference list is list of nodes that is responsible for storing a particular key，通常&gt;N，它是通过gossip在每个节点上最终一致1234567891011121314okN = 0for node = range preferenceList &#123; // 论文中没有提到复制是顺序还是并发 // 如果顺序，latency是个问题 // 如果并发，为了HA和latency，&gt;N个节点会进行复制，造成空间浪费 if replicateTo(node) == ok &#123; okN++ &#125; // all read/write are performed on the first N healthy nodes from pref list if okN == N &#123; break &#125;&#125; Merkle树123456789node ring(A, B, C, D, E, A), W=3A crash那么本来应该复制到A的数据，会复制到D由于membership change是显示的，D知道这个数据是它临时替A代管的，它会写到临时存储，并携带hint(A) meta info，并定期检查A是否recover如果A ok，D会把本来该写到A的数据transfer给A，成功后，本地删除但，如果D永久crash，而A recover，那么这些hinted data，A就无从transfer了此时，通过Merkle进行检查、增量同步但它的问题是当有node进入、离开时，这个树要重新创建，在key多的时候非常费时，因此，它只能异步后台执行 IDCs只是在preference list上配一下，架构上并没有过多考虑仍然是coordinator负责复制，首先本地机房，然后远程机房 Referenceshttp://jsensarma.com/blog/?p=55http://jsensarma.com/blog/?p=64https://timyang.net/data/dynamo-flawed-architecture-chinese/https://news.ycombinator.com/item?id=915212http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"MapReduce - A major step backwards","slug":"MapReduce-A-major-step-backwards","date":"2017-07-17T06:19:27.000Z","updated":"2017-07-31T01:31:54.000Z","comments":true,"path":"2017/07/17/MapReduce-A-major-step-backwards/","link":"","permalink":"http://funkygao.github.io/2017/07/17/MapReduce-A-major-step-backwards/","excerpt":"","keywords":[],"text":"Author都是数据库领域的权威 Michael Stonebraker美国工程院院士，冯诺依曼奖的获得者，第一届SIGMOD Edgar F. Codd创新奖的得主，曾担任Informix CTO他在1992年提出对象关系数据库模型，在加州伯克利分校任计算机教授达25年，目前是麻省理工学院教授是SQL Server/Sysbase奠基人，87年左右，Sybase联合了微软，共同开发SQL Server。1994年(7y)，两家公司合作终止可以认为，Stonebraker教授是目前主流数据库的奠基人 David J. DeWitt美国工程院院士In 1995, he was named a Fellow of the ACM and received the ACM SIGMOD Innovations AwardIn 2009, ACM recognized the seminal contributions of his Gamma parallel database system project with the ACM Software Systems AwardIEEE awarded him the 2009 IEEE Piore AwardHe was also a Technical Fellow at Microsoft, leading the Microsoft Jim Gray Systems Lab at Madison, Wisconsin. Background2008年1月，一个数据库专栏读者让作者表达一下对MapReduce(OSDI 2004)的看法而引出 ViewpointMapReduce是历史后退从IBM IMS的第一个数据库系统(1968)到现在的40年，数据库领域学到了3个经验，而MapReduce把这些经验完全摒弃，仿佛回到了60年代DBMS还没出现的时候 schema is good防止garbage进入系统MapReduce由于涉及到(k, v)，一定也是可以用schema表达的 要把schema从应用中decouple存放在catalog里，而不是hard coding在应用层 high level query is good70年代就充分讨论过应用通过SQL那样的语言访问DBMS好，还是更底层的语言MapReduce就像是DBMS的汇编语言 MapReduce的实现糟糕他们应该好好学学25年来并行数据库方面的知识 1. Index vs Brute force在非full scan场景下，index必不可少。此外，DBMS里还有query optimizer，而MapReduce里只能brute force full scan MapReduce能提供自动的并行执行，但这些东西80年代就已经在学术界研究了，并在80年代末出现了类似Teradata那样的商业产品 2. Record Skew导致性能问题David J. DeWitt在并行数据库方向已经找到了解决方案，但MapReduce没有使用，好像skew问题根本不存在一样：Map阶段，相同key的记录数量是不均衡的，这就导致Reduce阶段，有的reduce快、有的慢，最终Job的执行时间取决于最慢的那一个 3. 吞吐率和IO问题1234// map=1000 reduce=5001000个map实例，每个map进程都会创建500个本地文件reduce阶段，每个reduce都会到1000个map机器上PULL file这会导致每个map机器上500个文件进行大量的random IO 而并行数据库，采用的是PUSH模型。而MapReduce要修改成PUSH模型是比较困难的，因为它的HA非常依赖它目前的PULL模型MapReduce uses a pull model for moving data between Mappers and Reducers (Jeff:承认了这个问题的存在。但google在实现的时候，通过batching, sorting, grouping of intermediate data and smart scheduling of reads缓解了这个问题只所以使用PULL MODEL，是考虑到容错。大部分MR Job都会遇到几次错误，除了硬件、软件错误外，Google内部的调度系统是抢占式的：它可能会把M/R kill以疼出资源给更高优先级的进程，如果PUSH，那么就需要重新执行所有的Map任务) MapReduce并不新颖他们以为他们发现了一个解决大数据处理的新模式，实际上20年前就有了 “Application of Hash to Data Base Machine and Its Architecture”/1983，是第一个提出把大数据集切分成小数据集的Teradata使用MapReduce同样的技术卖他们的商业产品已经20多年了 就算MapReduce允许用户自己写map/reduce来控制，80年代中期的POSTGRES系统就支持用户自定义函数 缺少DBMS已经具备的功能 Bulk loader UpdateMapReduce is read-only Transaction ViewsHbase处已经提了 Integrity constraintsSchema处已经提了 Referential integrity防止garbase进入系统 与DBMS工具不兼容 BI tools Data mining tools Replication tools ER modelling SQL on hadoop作者已经发行类似Pig那样的系统 Comments作者好像觉得MR是用来替代DBMS的 MapReduce与DBMS的定位不同，它不是OLTPIndex好，但cost也很高，在MR里index只会是成本而没有收益用正确的工具解决不同的问题 Be a lover, not a fighter! Google本身已经证明了它的scalability With really large datasets and distributed sytems the RDBMS paradigms stop workingand that is where a system like Mapreduce is needed 数据库那些人的问题：什么东西都是数据库 作者应该再写个“Airplanes: A major step backward” Jeff Dean在ACM of communication上面的回馈http://cs.smith.edu/dftwiki/images/3/3c/MapReduceFlexibleDataProcessingTool.pdf MR可以通过实现Reader/Writer来支持更多的存储系统而并行数据库，必须要把数据load进系统后才能开始计算 MR可以使用index，它并不一定总是full table scan MR可以完成比SQL更复杂的查询UDF可以帮助DBMS，但目前商业产品上的实现要么缺功能，要么buggy Referenceshttp://database.cs.brown.edu/projects/mapreduce-vs-dbms/","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"conferences and journals","slug":"conferences-and-journals","date":"2017-07-17T05:48:21.000Z","updated":"2017-07-18T05:43:06.000Z","comments":true,"path":"2017/07/17/conferences-and-journals/","link":"","permalink":"http://funkygao.github.io/2017/07/17/conferences-and-journals/","excerpt":"","keywords":[],"text":"Overview按照USnews的分类，Computer Science被分为四个大类：AI, Programming Language, System, Theory PSU的CiteSeer(又名ResearchIndex)，有各个会议的影响因子，算是比较权威的 System包括OS、Architecture、Network、Database等 OSOSDI、SOSP，这两个是OS最好的会议，每两年开一次，轮流开 SOSP: The ACM Symposium on Operating Systems Principles由ACM下属的SIGOPS(special interest group on operation system)于1967年创办，每届收录20篇GFS就是发表在SOSP 2003的, Dynamo是SOSP 2007 OSDI: USENIX Symposium on Operating Systems Design and ImplementationUSENIX是一个于1975年成立的Advanced Computing Systems AssociationOSDI是USENIX 1994创办的，每届会议举行3天，每届收录27篇文章，每个小方向3篇MapReduce是发表在OSDI 2004的 Architecture ISCA HPCA MICRO Database数据库方向的三大顶级会议，SIGMOD和VLDB算是公认第一档次 ACM SIGMOD VLDB:International Conference on Very Large Data Bases ICDE:International Conference on Data Engineering Security IEEE Security and Privacy CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Referenceshttp://blog.sina.com.cn/s/blog_b3b900710102whvj.htmlhttp://sigops.org/sosp/sosp15/current/index.html","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"committee","slug":"committee","date":"2017-07-17T05:48:06.000Z","updated":"2017-07-17T06:16:31.000Z","comments":true,"path":"2017/07/17/committee/","link":"","permalink":"http://funkygao.github.io/2017/07/17/committee/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"LINQ","slug":"LINQ","date":"2017-07-14T01:45:45.000Z","updated":"2017-07-17T06:18:34.000Z","comments":true,"path":"2017/07/14/LINQ/","link":"","permalink":"http://funkygao.github.io/2017/07/14/LINQ/","excerpt":"","keywords":[],"text":"Language Integrated Query，一个C#的语言特性，基本思想是在语言层面增加了query的能力 在通过GraphQL中，应该可以发挥它的能力，来进行聚合、过滤、排序等","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"bigdata story","slug":"bigdata","date":"2017-07-14T00:06:16.000Z","updated":"2017-07-17T09:02:32.000Z","comments":true,"path":"2017/07/14/bigdata/","link":"","permalink":"http://funkygao.github.io/2017/07/14/bigdata/","excerpt":"","keywords":[],"text":"大数据，主要是因为Google的Google File System(2003)， MapReduce(OSDI 2004)，BigTable(2006)三篇论文GFS论文是最经典的，后面google的很多论文都遮遮掩掩的Chubby(2006)Dremal(2010)Borg(2015) 微软在bing项目上的投入，使得微软掌握了大规模data center的建设管理能力，掌握了A/B testing平台，学会了大规模数据分析能力，其中一些成为了Azure的基础，它使得微软顺利从一家软件公司过渡到了云计算公司。微软内部支撑大数据分析的平台Cosmos是狠狠的抄袭了Google的File system却很大程度上摒弃了MapReduce这个框架所谓MapReduce的意思是任何的事情只要都严格遵循Map Shuffle Reduce三个阶段就好。其中Shuffle是系统自己提供的而Map和Reduce则用户需要写代码。Map是一个per record的操作。任何两个record之间都相互独立。Reduce是个per key的操作，相同key的所有record都在一起被同时操作，不同的key在不同的group下面，可以独立运行Map是分类(categorize)操作，Reduce是aggregate To draw an analogy to SQL, map is like the group-by clause of an aggregate query. Reduce is analogous to the aggregate function (e.g., average) that is computed over all the rows with the same group-by attribute. Flume Java本质上来说还是MapReduce，但是它采取了一种delayed execution的方式，把所有相关的操作先存成一个execution DAG,然后一口气执行。这和C#里的LINQ差不多。这种方式就产生了很多optimization的机会了。具体来说大的有两个: 一个是怎么样把若干个Map或者Reduce整成一个 一个是怎么样把一系列的operation整成一个MapReduce job Hadoop三大批发商分别是Cloudera，Hortonworks以及MapR。MapR(印度人CTO是Google GFS项目组，后用C++写了自己的HDFS)、Cloudera(源于BerkeleyDB卖给Oracle)都成立于2009年，Hortonworks(源于Yahoo Hadoop分拆) 2011，目前Cloudera一家独大Hortonworks基本上就是Yahoo里的Hadoop团队减去被Cloudera挖走的Doug Cutting, Hadoop的创始人。这个团队的人做了不少东西，最初的HDFS和Hadoop MapReduce, ZooKeeper,以及Pig Latin。MapR的印度人CTO，目前已经跳槽到了Uber Hortonworks本来是一个发明了Pig的公司。Pig是它们的亲儿子。现在他们作为一个新成立的公司，不像Cloudera或者MapR那样另起炉灶，却决定投入HIVE的怀抱，要把干儿子给捧起来Pig发表在SIGMOD2008，之后Hive也出来了时至今日，我们必须说Pig是被大部分人放弃了：原来做Pig的跑去了Hortonworks，改行做Hive了HIVE已经事实上成为了Hadoop平台上SQL和类SQL的标杆和事实的标准。 2008年Hadoop的一次会议，facebook的2个印度人讲了他们hackathon的项目Hive，他们说SQL的SELECT FROM应该是FROM SELECT2010年Hive论文发表，直到2014年那2个印度人一直在Hive PMC，后来出来创业大数据公司后来，做Pig的人看上了Hive，Hortonworks把自己人塞进PMC，现在那2个印度人已经除名了，Hive基本上是Hortonworks和Cloudera的天下，Cloudera做企业需要的安全方面东西，Hortonworks做性能提升于是，Pig的人成了Hive主力，做Hive的人跑光了 Dynamo: A flawed architecturehttp://jsensarma.com/blog/?p=55 Hbase开始的时候是一个叫Powerset的公司，这个公司是做自然语言搜索的。公司为了能够实现高效率的数据处理，做了HBase。2008年的时候这个公司被卖给了微软 VLDB(Very Large Data Base), 世界数据库业界三大会议之一；另外两个是ICDE和sigmod，另外还有KDD。在数据库领域里面，通常SIGMOD和VLDB算是公认第一档次，ICDE则是给牛人接纳那些被SIGMOD VLDB抛弃的论文的收容所，勉强1.5吧，而且有日渐没落的趋势。CIDR在数据库研究领域是个奇葩的会议，不能说好不能说不好。因为是图灵机得主Michael Stonebraker开的，给大家提供个自娱自乐的场所。所以很多牛人买账，常常也有不错的论文。但是更多的感觉是未完成的作品。 Dremal出来没多久，开源社区，尤其是Hadoop的批发商们都纷纷雀跃而起。Cloudera开始做Impala，MapR则做了Drill，Hortonworks说我们干脆直接improve HIVE好了，所以就有了包括Apache Tez在内的effortDremal的存储是需要做一次ETL这样的工作，把其他的数据通过转化成为它自己的column store的格式之后才能高效率的查询的。 Dremal出来后，Cloudera 2012年做出了Impala，MapR搞了个Drill，Hortonworks也许最忽悠也许最实际，说我们只需要改善 Hive就好 Spark是迄今为止由学校主导的最为成功的开源大数据项目UCBerkeley作为一个传统上非常有名的系统学校，它曾经出过的系统都有一个非常明确的特点，可用性非常高","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"RCFile, Parquet, Dremel","slug":"Parquet","date":"2017-07-13T02:12:54.000Z","updated":"2017-07-31T07:45:31.000Z","comments":true,"path":"2017/07/13/Parquet/","link":"","permalink":"http://funkygao.github.io/2017/07/13/Parquet/","excerpt":"","keywords":[],"text":"Overviewhadoop的存储格式：TextFile, SequenceFile, Hive优化的RCFile, 类似google dremel的Parquet Data placement for MapReduce row store column store hybrid PAX store RCFile - Record Columnar File一种PAX的实现：Table先水平切分成多个row group，每个row group再垂直切分到不同的column这样保证一个row的数据都位于一个block(避免一个row要读取多个block)，列与列的数据在磁盘上是连续的存储块 table = RCFile -&gt; block -&gt; row group -&gt; (sync marker, metadata header(RLE), column store(gzip)) For a table, all row groups have the same size. ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java Usage12345678CREATE TABLE demo_t ( datatime string, section string, domain string, province string, city string, idc string)STORED AS RCFILE; ORCFileParquetReferenceshttp://web.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdfhttp://www.ece.eng.wayne.edu/~sjiang/ECE7650-winter-14/Topic3-RCFile.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"bitcask","slug":"bitcask","date":"2017-07-13T00:02:10.000Z","updated":"2017-07-17T06:27:44.000Z","comments":true,"path":"2017/07/13/bitcask/","link":"","permalink":"http://funkygao.github.io/2017/07/13/bitcask/","excerpt":"","keywords":[],"text":"A Log-Structured Hash Table for KV storageRiak默认采用的存储引擎就是bitcask，在开发bitcask的时候，LevelDB还没有开放出来刚听到的消息，Riak的公司basho，已经离解散不远了，CTO已经离职，公司好像只有1、2人了 Data older data files will be merged and compacted Indexreside in mem 读取file_id对应文件的value_pos开始的value_sz个字节，就得到了我们需要的value值 Write12append to active data fileupdate hash table index in memory Hint File为了冷启动时加快index生成速度：重建hash table时，不需要full scan data files，只scan much smaller hint file Issues 由于index是包含所有key的，因此可以容纳的数据量跟内存有很大关系，每个index至少占40B 不支持range/sort操作如果把hash table变成skiplist，而merge sort older data files，就可以支持range了","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"hashicorp/raft implementation","slug":"raft","date":"2017-07-12T02:48:48.000Z","updated":"2017-08-01T03:01:44.000Z","comments":true,"path":"2017/07/12/raft/","link":"","permalink":"http://funkygao.github.io/2017/07/12/raft/","excerpt":"","keywords":[],"text":"PrimerLog entry = (Term, Index, Command)in RDBMS, Log=WAL/redo log, FSM=records Components FSM LogStore LastIndex FirstIndex StableStore CurrentTerm LastVoteTerm LastVoteCand SnapshotStore PeerStore RPC Protocolmsgpack serializer AppendEntriesLeader发起If got reponse with Success=false，then step down(退位) TermEach term begins with an election Leaderpartition后，old leader在复制日志时，通过它发现new leader，并step down PrevLogIndex, PrevLogTerm它们确保在相同term/index上的log内容完全一致, 而且之前的所有log内容一致：safety LeaderCommitIndex只有committed log才能被FSM Apply []Log Term, Index Type LogCommand复制日志 LogAddPeer LogRemovePeer LogNoop LogBarrier RequestVoteCandidate发起，if not Granted，step down to followerleader/candidate也可能收到RequestVote发起投票时，可能会收到AppendEntries，比较term来决定进入follower状态还是拒绝 Term LastLogIndex, LogLogTerm选民如果发现candidate的log没有自己的新，则拒绝投票阻止一个不包含所有committed entries的candidate成为leader InstallSnapshotLeader发起 2PCLeader’s log is ‘the truth’ 2.1是第一次AppendEntries，3.1后Leader apply to local FSM并update commit index，就可以向client返回了4.2是下一次AppendEntries，leader通知followers最新的commit index，followers才会apply to FSM When the Commit Index is updated, the node will pass all commands between the new and old Commit Index to the state machine. phase2的作用：uncommitted log是可能回滚的12345A(leader),B,C,D,E 5个节点，partition成A(leader)/B，C(leader)/D/EA: apply(set x=9)C: apply(set x=8)由于只与leader(A)心跳，B并不知道已经partition了。如果没有phase2，B会直接更改FSM，造成consensus失败partition修复后，A会退位成follower，同时A/B的(set x=9)这个log entry会回滚 CommitFor a leader to decide an entry is committed must be stored on majority at least one new entry from current term must also be stored on majority 1234567891011121) (term2, index3)，S1 leader，index3收到S2的ack，但没收到S3的ack 即S3.LastIndex=2，此时S1 crash2) S5成为term3 leader(S3,S4 vote for it) term3内S5有3条append，但由于partition了，其他机器term=3 此时重新选举，S1成为term4 leader3) S1恢复，成为term4 leader，并把(term3, index3)发给S3。 此时，不能认为(term2, index3)是可以安全commit的 因为，如果此时S1 crash，currentTerm(S2,S3)=2 S5可能成为term4 leader，&apos;committed&apos; log lost4) 但如果S1在term4内有一个majority ack的log entry (term2, index3)就可以安全commit，因为majority currentTerm=4 那样，S5不不可能成为leader Electionleader要给peers发心跳，阻止新选举(如果一直没有Apply，多久发心跳?) 处理RequestVote RPC123456789101112131415161718192021222324252627282930// 同一个term，只会有一个leader，但由于partition的存在，可能存在多个leader在不同的term：新的leader的term更高if req.Term &lt; r.getCurrentTerm() &#123; // rejected return&#125;if req.Term &gt; r.getCurrentTerm() &#123; // Ensure transition to follower r.setState(Follower) r.setCurrentTerm(req.Term)&#125;// 每个server只会给每个term投一票，按照先来先给原则lastVoteTerm := r.stable.GetUint64(keyLastVoteTerm)if req.Term == lastVoteTerm &#123; return&#125;// 保证被选为新leader的server拥有所有的已经committed的log entrylastIdx, lastTerm := r.getLastEntry()if lastTerm &gt; req.LastLogTerm &#123; return&#125;if lastTerm == req.LastLogTerm &amp;&amp; lastIdx &gt; req.LastLogIndex &#123; return&#125;// 每次投票要持久化r.persistVote(req.Term, req.Candidate)resp.Granted = true // 同意 Replicationleader保存每个followerReplication状态 12345678type followerReplication struct &#123; currentTerm uint64 matchIndex uint64 nextIndex uint64 // AppendEntries时，通过nextIndex--来truncate conflict，直到找到同步点 // 初始值是 1 + LastLogIndex(leader) lastContact time.Time&#125; Repair follower logs delete extraneous entries fill in missing entries startup123456从StableStore里取currentTerm，并设置当前term从LogStore里取last Log，并设置lastLogIndex/lastLogTerm从PeerStore取peers进入Follower状态restoreSnapshot: 通过SnapshotStore取snapshot，并调用FSM.Restore启动3个goroutine: run(runFollower/runCandicate/runLeader)/runFSM/runSnapshot runFSM12执行FSM的3个方法Apply, Restore, Snapshot runLeader1234567891011121314151617181920212223242526272829303132333435363738r.leaderCh &lt;- true// 启动每个peer(除了自己)的异步复制goroutinefor _, peer := range r.peers &#123; r.startReplication(peer)&#125;先发送一个AppendEntries(AddPeer)当做noop心跳// leader loopfor r.getState() == Leader &#123; select &#123; case rpc := &lt;-r.rpcCh: r.processRPC(rpc) case &lt;-r.stepDown: r.setState(Follower) case &lt;-r.commitCh: &lt;--------------------------+ r.processLogs | case log := &lt;-r.applyCh: | r.dispatchLogs &#123; | // 先写本地 | // 如果失败，就step down Follower | LogStore.StoreLogs(logs) | | // 再复制给followers | // inflight在获得majority的时候会通知r.commitCh inflight.StartAll(logs) notify each follower replication goroutine &#125; case &lt;-lease: // 如果LeaderLeaseTimeout内没有contact quorum // 就step down to Follower case &lt;-r.shutdownCh: return &#125;&#125; appendEntries RPC123456789101112131415161718192021222324252627282930313233343536func appendEntries(rpc RPC, a *AppendEntriesRequest) &#123; // 每次执行，无论成功失败，都要给回复 defer rpc.Responde(resp) // 比我的term小，reject if a.Term &lt; r.getCurrentTerm() &#123; return &#125; if a.Term &gt; r.getCurrentTerm() || r.getState() != Follower &#123; r.setState(Follower) a.setCurrentTerm(a.Term) resp.Term = a.Term &#125; r.setLeader(a.Leader) // log conflict detection if a.PrevLogEntry &gt; 0 &#123; 取得本地a.PrevLogEntry上的Log，并比较a.PrevLogTerm与Log.Term是否相同 如果不同，那么表示log conflict, return &#125; // 处理a.Entries // // 1 2 3 4 5 // x y z local Logs // o p q w a.Entries r.logs.DeleteRange(2, 3) 因为冲突 r.logs.StoreLogs(a.Entries) // 处理piggyback phase2的commit if a.LeaderCommitIndex &gt; 0 &amp;&amp; a.LeaderCommitIndex &gt; r.getCommitIndex() &#123; &#125;&#125; runCandidate12electSelf中间，可能会收到RPC，也可能会SetPeers Configuration Change采用2PC方法: C_old -&gt; C_old+new -&gt; C_new http://zookeeper.apache.org/doc/trunk/zookeeperReconfig.htmlzookeeper 3.5.0开始，也有了动态修改cluster的功能 Q &amp; Atime12broadcast time &lt;&lt; election timeout &lt;&lt; MTBF [T, 2T] 为什么每个node上的current term需要持久化?It is best guess, persistent for recovery after crash. 恢复时，从leader拿不行吗？ 为什么每个node上的votedFor要持久化?为了保证election safety: allow at most one winner per termterm1，A vote for B，然后A crash，等A恢复了，如果voteFor不持久化，可能它对term1又vote for C了 成为leader后，立刻发heartbeat还是等heartbeat timeout?如果一个follower的AppendEntries失败，leader怎么处理?一直retry，Leader’s log is ‘the truth’.Log是幂等的，因为有term/index，可以很容易排重 但在client方面，就没有保障了：如果leader crash after executing command but before responding?client如果盲目retry，有可能造成重复执行解决办法：client在发送log时，在每个命令上加入id，确保幂等leader上保存每个follower的index EngineeringConfigHeartbeatTimeout = ElectionTimeout = 1sLeaderLeaseTimeout = 500msCommitTimeout = 50msMaxAppendEntries = 64SnapshotInterval = 2mSnapshotThreshold = 8192 Group Commit0 &lt; MaxAppendEntries &lt;= 1024 12345678910newLog := &lt;-r.applyChready := []*logFuture&#123;newLog&#125;for i := 0; i &lt; r.conf.MaxAppendEntries; i++ &#123; select &#123; case newLog := &lt;-r.applyCh: ready = append(ready, newLog) default: break &#125;&#125; Lease除了follower通过被动接受心跳来检测leader存活，leader本身也通过与majority follower的response来判断自己是否已经被partition了，如果是，进入Follower状态 Pipeline仅仅用于AppendEntries，通过channel实现多次发送RPC给follower而不等待response但如果有错误响应，立刻取消pipeline模式 max outstanding AppendEntries RPC calls = 128 Limitations Apply([]Log)只能在leader上发起，follower没有自动redispatchapplyCh是no buffer的 StableStore.GetUint64如果没有找到key，返回的error必须是”not found” LeaderCh() might lose event Paxos server replication (SR), log replication (LR), synchronisation service (SS), barrier orchestration (BO), service discovery (SD), leader election (LE), metadata management (MM), and Message Queues (Q). CAP证明：在一个network partition的2个节点，现在有两个client分别向他们发送冲突的请求，如果要C，那么必然有一个节点要拒绝：牺牲A；如果要A，必然牺牲C References1985 FLP Impossibility Result1988 Oki and Liskov’s Viewstamped Repication1998 Lamport’s original Paxos paper “Part-Time Parliment”2000 Brewer proposes CAP conjecture during Keynote at PODC2002 Proof of CAP by Gilbert and Lynch(CAP 2年后才被正式证明)2005 Lamport’s Technical Report on Generalized Consensus &amp; Paxos2013 Raft Consensus Paper first available online http://www.cnblogs.com/foxmailed/p/3418143.htmlhttp://raftuserstudy.s3-website-us-west-1.amazonaws.com/study/raft.pptx http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdfhttps://www.cse.buffalo.edu//~demirbas/publications/cloudConsensus.pdfhttp://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-857.pdf","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"Linux 2.6.19 TCP implementation","slug":"Linux-TCP-implementation","date":"2017-06-29T23:52:33.000Z","updated":"2017-06-30T00:27:49.000Z","comments":true,"path":"2017/06/30/Linux-TCP-implementation/","link":"","permalink":"http://funkygao.github.io/2017/06/30/Linux-TCP-implementation/","excerpt":"","keywords":[],"text":"read /proc/sys/net/ipv4/tcp_rmemTCP recv Buffer net.core.netdev_max_backlogmaximum number of packets queued at a device, which are waiting to be processed by the TCP receiving processSocket Backlog, dropped if full write txqueuelenifconfig eth0 txqueuelen","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"an IM implementation","slug":"an-IM-implementation","date":"2017-06-29T05:38:03.000Z","updated":"2017-06-30T06:14:23.000Z","comments":true,"path":"2017/06/29/an-IM-implementation/","link":"","permalink":"http://funkygao.github.io/2017/06/29/an-IM-implementation/","excerpt":"","keywords":[],"text":"ArchitecturePros 通过kafka将上行、下行消息分开处理 通道和逻辑分离 Cons 逻辑服务器对session服务器(router)的感知，通过一致性哈希解决扩容问题 虽然可以进行多IDC部署，但设计上并没有充分考虑目前腾讯其实是所有IDC router 全局同步的，这样各个IDC甚至可以直连本IDC的router模块查询 还没有实现离线消息 实现上对可靠性的处理比较naive，存在多处丢失消息的case 配置上静态服务地址绑定 虽然可以通过VIP LB 每台comet需要配置所有logic addrs 每台logic需要配置所有router addrs 每台job需要配置所有comet addrs 系统组成 接入服务器/前置机(comet)负责消息的接收和投递client只接触到它，通过websocket/http 业务服务器(logic)认证、路由查询、消息kafka持久化 session存储服务器(router)保存路由信息表 push服务器(job)从kafka取消息，通过router里的信息找到对应comet服务器，进行投递 kafka1234567type KafkaMsg struct &#123; OP string RoomId int32 ServerId int32 SubKeys []string Msg []byte &#125; Q &amp; AWhat if comet crash?Client端是有token的，session不会有问题 What if router crash?big trouble What if job crash?job is stateless，会产生kafka consumer group rebalance，不影响。如果只有一个job，重启即可 A发消息给B,C,D，产生几条kafka msg?多条(在push的时候，job-&gt;comet，这里可以合并一些RPC请求) 但向room发，无论room里有多少人，都只产生一条kafka msg. What if client conn broken?send时，就已经把路由信息写到kafka消息了。job push时，采用的是kafka里的路由信息，但这时候可能已经发生变化，造成丢消息","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"geo-distributed storage system","slug":"geo-distributed-storage","date":"2017-06-27T10:22:58.000Z","updated":"2017-06-28T00:04:39.000Z","comments":true,"path":"2017/06/27/geo-distributed-storage/","link":"","permalink":"http://funkygao.github.io/2017/06/27/geo-distributed-storage/","excerpt":"","keywords":[],"text":"PrimerNetwork of network. Why scaleone datacenter too small disaster toleranceprotect data against catastrophes availabilitykeep working after intermittent problems access localityserve users from a nearby datacenter Challanges high latency low bandwidth congestionoverprovisioning prohibitive network partitions Replication123456 read-write | state machine----------------------+-----------------sync ABD | Paxosasync logical clock | CRDTABD(Alike Backup Delegate) CongestionMovivation bandwidth across datacenter is expensive storage usage subject to spikes 如何准备带宽？ 按峰值成本高，利用率低 按均值网络拥塞 Solution weak consistencyasync replication priority messagesshould be small vivace algorithmread-write algorithm based on ABD 算法 把数据和timestamp分离把数据和元数据分离，元数据优先级更高 replicate data locally first replicate timestamp remotely with prioritized msg replicate data remotely in background write1234obtain timestamp tswrite data,ts to f+1 temporary local replicas (big msg)write only ts to f+1 real replicas (small msg, cross datacenter) in background, send data to real replicas (big msg) read123read ts from f+1 replicas (small msg)read data associated with ts from 1 replica (big msg, often local) write only ts to f+1 real replicas (small msg, cross datacenter) Transactionsnapshot isolation(SI)total ordering of update transactions 问题 it orders the commit time of all transactionseven those that do not conflict with each other forbids some scenarios we want to allow for efficiency parallel snapshot isolation(PSI)causality: if T1 commits at site S before T2 starts at site S then T2 does not commit before T1 at any site 1234567SI PSI------------------- ---1 commit time 1 commit time per site1 timeline 1 timeline per siteread from snapshot read from snapshot at siteno w-w conflict no w-w conflict causality property idea1: preferred sites每个key分配一个唯一的preferred site(例如，在长沙的用户它的preferred site就是长沙IDC)，在该site上，可以使用fast commit(without cross-site communication)类似primary/backup，不同的是preferred sites不是强制的，key可以在任何site修改 但问题是 what if many sites often modify a key? no good way to assign a preferred site to key idea2: CRDTserializableSI/PSI都有write skew问题通过transaction chains[sosp 2013]可以有效地提供serializable isolation Referenceshttp://dprg.cs.uiuc.edu/docs/vivace-atc/vivace-atc12.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"chain replication","slug":"chain-replication","date":"2017-06-27T05:21:43.000Z","updated":"2017-06-27T08:56:34.000Z","comments":true,"path":"2017/06/27/chain-replication/","link":"","permalink":"http://funkygao.github.io/2017/06/27/chain-replication/","excerpt":"","keywords":[],"text":"与primary/backup repliation不同，是一种ROWAA(read one, write all available)方法，比quorum有更高的可用性 Referenceshttps://www.usenix.org/legacy/event/osdi04/tech/full_papers/renesse/renesse.pdfhttp://snookles.com/scott/publications/erlang2010-slf.pdfhttps://github.com/CorfuDB/CorfuDB/wiki/White-papers","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"http Preconnect","slug":"http-Preconnect","date":"2017-06-27T02:17:27.000Z","updated":"2017-06-27T02:17:45.000Z","comments":true,"path":"2017/06/27/http-Preconnect/","link":"","permalink":"http://funkygao.github.io/2017/06/27/http-Preconnect/","excerpt":"","keywords":[],"text":"https://www.igvita.com/2015/08/17/eliminating-roundtrips-with-preconnect/","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"TLS Session Resumption","slug":"TLS-Session-Resumption","date":"2017-06-27T02:00:17.000Z","updated":"2017-06-27T02:12:14.000Z","comments":true,"path":"2017/06/27/TLS-Session-Resumption/","link":"","permalink":"http://funkygao.github.io/2017/06/27/TLS-Session-Resumption/","excerpt":"","keywords":[],"text":"There are two mechanisms that can be used to eliminate a round trip for subsequent TLS connections (discussed below): TLS session IDs ServerHello时，server生成一个32字节的session ID给client，后面的TLS握手client可以在它的ClientHello里发送这个id，server就会restore the cached TLS context and avoid the 2nd round trip of TLS handshake nginx支持该方式 ssl_session_cache ssl_session_timeout TLS session tickets与session IDs类似，只是session信息保存在client https dialog","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"AWS Marketplace","slug":"AWS-Marketplace","date":"2017-06-26T11:26:36.000Z","updated":"2017-06-26T11:31:09.000Z","comments":true,"path":"2017/06/26/AWS-Marketplace/","link":"","permalink":"http://funkygao.github.io/2017/06/26/AWS-Marketplace/","excerpt":"","keywords":[],"text":"Why shop here? cloud experienceinstall/deploy that software on your own EC2 with 1-Click Why sell here? AWS已经为ISV创建很好的生态 gain new customers enable usage-based billing","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"NILFS","slug":"NILFS","date":"2017-06-26T10:49:36.000Z","updated":"2017-06-26T11:56:49.000Z","comments":true,"path":"2017/06/26/NILFS/","link":"","permalink":"http://funkygao.github.io/2017/06/26/NILFS/","excerpt":"","keywords":[],"text":"IntroNew Implementation of a Log-Structured File System，included in Linux 2.6.30 kernel take snapshot非常简单，只要记录一下version就可以了 尤其在随机的小文件读写效率更高 在SSD上，NILFS2具有绝对性能优势 123insmod nilfs2.ko mkfs – t nilfs2 /dev/sda8 mount – t nilfs2 /nilfs /dev/sda8 Benchmark vs Journal File SystemJFS保存在日志里的只有metadata，而LFS利用日志记录一切 Referenceshttp://www.linux-mag.com/id/7345/","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"CryptDB","slug":"CryptDB","date":"2017-06-26T08:46:21.000Z","updated":"2017-06-26T08:59:11.000Z","comments":true,"path":"2017/06/26/CryptDB/","link":"","permalink":"http://funkygao.github.io/2017/06/26/CryptDB/","excerpt":"","keywords":[],"text":"一个DB Proxy，对字段名称、记录都加密，Google根据CryptDB的设计开发了Encrypted BigQuery client仍然存在数据泄露问题 Related CryptDB enables most DBMS functionality with a performance overhead of under 30% Arx is built on top of MongoDB and reports a performance overhead of approximately 10% OSPIR-EXT/SisoSPIR support MySQL BlindSeer Referenceshttps://github.com/CryptDB/cryptdbhttp://people.csail.mit.edu/nickolai/papers/raluca-cryptdb.pdfhttps://people.csail.mit.edu/nickolai/papers/popa-cryptdb-tr.pdf","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"Bloom Filter","slug":"Bloom-Filter","date":"2017-06-26T08:03:58.000Z","updated":"2017-06-26T08:07:10.000Z","comments":true,"path":"2017/06/26/Bloom-Filter/","link":"","permalink":"http://funkygao.github.io/2017/06/26/Bloom-Filter/","excerpt":"","keywords":[],"text":"by Burton Bloom in 1970 false positive possible, false negative impossible Dynamo, Postgresql, HBase, Bitcoin都广泛使用","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"Why Do Computers Stop","slug":"Why-Do-Computers-Stop","date":"2017-06-26T06:55:45.000Z","updated":"2017-06-26T08:10:36.000Z","comments":true,"path":"2017/06/26/Why-Do-Computers-Stop/","link":"","permalink":"http://funkygao.github.io/2017/06/26/Why-Do-Computers-Stop/","excerpt":"","keywords":[],"text":"Jim Gray, June 1985, Tandem Technical report 85.7 TermsReliability != Availability availability is doing the right thing within the specified response time Availability = MTBF/(MTBF + MTTR) 分布式系统下，整体的可用性=各个子系统可用性的乘积 模块化使得局部failure不会影响全部，冗余减少MTTR 磁盘的MTBF是1万小时，即1年；如果两张盘完全独立冗余，假设MTBR是24h，那么整体的MTBF是1000年 reliability is not doing the wrong thing Report 设计容错系统的方法process pair Lockstep一个执行失败，就启用另外一个容忍了硬件故障，但没有解决Heisenbugs(难以重现的bug) State Checkpointingprimary通过消息同步把请求发送给backup，如果primary挂了，切到backup通过序列号来排重和发现消息丢失实现起来比较困难 Automatic Checkpointingkernal自动管理checkpoint，而不是让上层应用管理发送的消息很多 Delta Checkpointing发给backup的是logical updates，而不是physical updates性能更好，消息更少，但也更难实现 Persistence失败的时候可能丢失状态需要加入事务来提高可靠性 Fault-tolerent Communication硬件，通过multiple data paths with independent failure modes软件，引入session概念(类似tcp) Fault-tolerent Storage2份复制正好，3份不一定提高MTBF，因为其他失败因素会变为主导分布式复制把数据分片，会限制scope of failure Referenceshttp://www.hpl.hp.com/techreports/tandem/TR-85.7.pdf","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"Amazon Aurora","slug":"Amazon-Aurora","date":"2017-06-23T09:30:31.000Z","updated":"2017-06-26T03:32:19.000Z","comments":true,"path":"2017/06/23/Amazon-Aurora/","link":"","permalink":"http://funkygao.github.io/2017/06/23/Amazon-Aurora/","excerpt":"","keywords":[],"text":"BackgroundLaunched in 2014 for MySQL, and in 2016 for PostgreSQL. Aurora基于shared disk的架构，storage共享来解决一致性问题，把计算节点与存储节点解耦，MySQL本身无状态，一写多读，S3做备份，本质上还是单机数据库 无法访问其binlog automatic storage scaling up to 64 TB, SSD 数据传输通过SSL(AES-256) 支持 100,000 writes/s, 500,000 read/s 费用 8 vCPU/61GB $1.16/h 16vCPU/122GB $2.32/h 32vCPU/244GB $4.62/h每个月相当于2万多人民币 Architecture Why not R(2)+W(2)&gt;N(3) quorum?Aurora采用的是R(3)+W(4)&gt;N(6) 3个AZ(但必须在同一个region)，每个AZ上复制2份它保证 read与write集合是相交的 W&gt;N/2，防止写冲突 原因 写 2+2&gt;3只能容忍一个AZ crash 3+4&gt;6只能容忍一个AZ crash 读 2+2&gt;3只能容忍一个AZ crash 3+4&gt;6能容忍一个AZ crash，此外允许另外一个node crash，即AZ+1为什么这个重要？因为data durability是指写进去的数据能读出来，它提高了durability Why segmented storage如果一个AZ crash了，就会破坏write quorum，降低availability，为了提高availability(99.99%)，他们采用的方法是降低MTTR 类似ES，数据存储(ibdata)被segment化，each 10GB，total max 64TB，每个segment复制6份(3 AZ)，10GB是为了能控制MTTR在10ssegment就成为了independent background noise failure and repair，后台有应用不停地检查、修复segment错误，如果不segment，那么修复成本很高同时，是考虑到底层存储机制，做线性扩容方便 Scale scale write只能把master机器升级到更高的ec2: scale up, not scale out scale readadd more read replicas Referenceshttp://www.allthingsdistributed.com/files/p1041-verbitski.pdfhttps://www.percona.com/blog/2016/05/26/aws-aurora-benchmarking-part-2/http://www.tusacentral.net/joomla/index.php/mysql-blogs/175-aws-aurora-benchmarking-blast-or-splash.html","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"},{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"nagles and delayed ack","slug":"nagles-and-delayed-ack","date":"2017-06-23T07:23:47.000Z","updated":"2017-06-25T23:44:48.000Z","comments":true,"path":"2017/06/23/nagles-and-delayed-ack/","link":"","permalink":"http://funkygao.github.io/2017/06/23/nagles-and-delayed-ack/","excerpt":"","keywords":[],"text":"Case同时开启情况下12345678910client.send(1600B) // 1600&gt;1460，defragment into Packet(1460)+Packet(140)client.sendPacket(1460)server.recv(1460) // no push, server awaiting the next 140// delayed ack works, so no ack sent s-&gt;cclient.sendPacket(140) // because of nagles and has unacked data, wait till 1) data&gt;=1460 or 2) get ack // i,e. will not send packet(140)... // server ack delay timeoutserver.ack(1460)client.recv(ack)client.sendPacket(140) delayed ackLinux最小值20ms，它是根据RTO、RTT动态计算出来的 Nagles 第一次发包，无论多大，立即发送 只要发出的包都被对端ack了就可以发送了，无需等待 如果没有ack，就等buffer里的包凑足MSS一起发，即它只允许1个未ack的包存在于网络，基于字节的“停-等” 1234567891011if there is new data to send if the window size &gt;= MSS and available data is &gt;= MSS send complete MSS segment now else if there is unacked data still in the buffer enqueue data in the buffer until an ack is received else send data immediately end if end ifend if TCP_CORK vs naglescork：塞子 cork是一种加强的nagles算法，但它ignore ack，即使所有ack都已经收到，只要数据包不够大而且时间没到，依然不发送cork是为了提高网络利用率，nagles是为了避免因为过多小包(payload占header比例过小)引起的网络拥堵","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"DB Storage Structures","slug":"DB-Storage-Structures","date":"2017-06-22T09:13:42.000Z","updated":"2017-07-13T01:22:59.000Z","comments":true,"path":"2017/06/22/DB-Storage-Structures/","link":"","permalink":"http://funkygao.github.io/2017/06/22/DB-Storage-Structures/","excerpt":"","keywords":[],"text":"KV任何storage structure，数据都可以用k=&gt;v来表示，不仅NoSQL，RDBMS也一样例如InnoDB的primary key就是1primary_key =&gt; [column1, column2, ...] secondary index也一样，因此在insert/update时有index maintaenance overhead，保持各个index的一致性 B-TreeDesigned for optimal data retrieval performance, not data storage. RDBMS, LMDB, MongoDB 充分利用了read ahead技术 LSM Hash Tablebitcask 不支持range，所有index在内存的hash table里算是一个简化版的LSM-Tree LSM-Tree每个SSTable的bloom filter只能帮助individual key lookup，对range query没用 Fractal-Tree与B-Tree类似，但通过buffer changes/data compression大大降低了disk random IOhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.762&amp;rep=rep1&amp;type=pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Kafka Streams","slug":"KafkaStreams","date":"2017-06-22T07:24:10.000Z","updated":"2017-06-22T07:46:27.000Z","comments":true,"path":"2017/06/22/KafkaStreams/","link":"","permalink":"http://funkygao.github.io/2017/06/22/KafkaStreams/","excerpt":"","keywords":[],"text":"Kafka 0.10提供，可以部分替代SparkStreaming/Storm/Samza/Flink，好处是仅仅依赖kafka，全部通过SDK实现流式处理 通过KeyValueStore interface实现stateful processor，目前有2个实现 in memory RocksDB 其实跟dbus的设计差不多 12345678910111213141516171819202122JsonDeserializer&lt;Purchase&gt; purchaseJsonDeserializer = new JsonDeserializer&lt;&gt;(Purchase.class);JsonSerializer&lt;Purchase&gt; purchaseJsonSerializer = new JsonSerializer&lt;&gt;();JsonSerializer&lt;RewardAccumulator&gt; rewardAccumulatorJsonSerializer = new JsonSerializer&lt;&gt;();JsonSerializer&lt;PurchasePattern&gt; purchasePatternJsonSerializer = new JsonSerializer&lt;&gt;();StringDeserializer stringDeserializer = new StringDeserializer();StringSerializer stringSerializer = new StringSerializer();TopologyBuilder topologyBuilder = new TopologyBuilder();topologyBuilder.addSource(&quot;SOURCE&quot;, stringDeserializer, purchaseJsonDeserializer, &quot;src-topic&quot;) .addProcessor(&quot;PROCESS&quot;, CreditCardAnonymizer::new, &quot;SOURCE&quot;) .addProcessor(&quot;PROCESS2&quot;, PurchasePatterns::new, &quot;PROCESS&quot;) .addProcessor(&quot;PROCESS3&quot;, CustomerRewards::new, &quot;PROCESS&quot;) // kafka(src-topic) -&gt; SOURCE -&gt; PROCESS -+-&gt; PROCESS2 // +-&gt; PROCESS3 .addSink(&quot;SINK&quot;, &quot;patterns&quot;, stringSerializer, purchasePatternJsonSerializer, &quot;PROCESS2&quot;) .addSink(&quot;SINK2&quot;, &quot;rewards&quot;, stringSerializer, rewardAccumulatorJsonSerializer, &quot;PROCESS3&quot;) .addSink(&quot;SINK3&quot;, &quot;purchases&quot;, stringSerializer, purchaseJsonSerializer, &quot;PROCESS&quot;);KafkaStreams streaming = new KafkaStreams(topologyBuilder, streamingConfig);streaming.start(); Referenceshttps://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"2 phase commit failures","slug":"2PC-failures","date":"2017-06-14T08:34:59.000Z","updated":"2017-08-01T01:14:54.000Z","comments":true,"path":"2017/06/14/2PC-failures/","link":"","permalink":"http://funkygao.github.io/2017/06/14/2PC-failures/","excerpt":"","keywords":[],"text":"Node Failure Models fail-stopcrash and never recover fail-recovercrash and later recover byzantine failure Cases2 phase commit，n个节点，那么需要3n个消息交换 coordinator发送proposal后crash 有的node收到，有的没收到 收到Proposal的node被block forever，它可能已经vote commit了不能简单地timeout/abort，因为coordinator可能随时recover并启动phase2 commit这个txn就只能blocked by coordinator，cannot make any progress 解决办法引入coordinator的watchdog机制，它发现coordinator crash后，接管Phase1. 先询问每个participants，已经vote commit还是vote abort还是没有votePhase2. 通知每个participant Commit/Abort但仍有局限，如果有个participant crash了，那么Phase1无法确认 worse casecoordinator本身也是participant 3PC在propose和commit这2个phase中间，加了个prepare to commit 如果coordinator在prepare to commit或者proposal阶段crashtrx aborted 如果coordinator在commit阶段crashnodes will timeout waiting for the commit phase and commit the trx","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"graph database","slug":"graph-database","date":"2017-06-14T06:33:50.000Z","updated":"2017-06-14T06:34:26.000Z","comments":true,"path":"2017/06/14/graph-database/","link":"","permalink":"http://funkygao.github.io/2017/06/14/graph-database/","excerpt":"","keywords":[],"text":"graph indexgraph operations","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"HDD Stats Q1 2017","slug":"HDD-Stats-Q1-2017","date":"2017-06-14T03:14:03.000Z","updated":"2017-06-14T06:05:54.000Z","comments":true,"path":"2017/06/14/HDD-Stats-Q1-2017/","link":"","permalink":"http://funkygao.github.io/2017/06/14/HDD-Stats-Q1-2017/","excerpt":"","keywords":[],"text":"Backblaze Storage Pod把45块SATA盘存放到一台4U机器里，其中15块搞RAID6，ext4，可以空间是裸盘空间的87%，所有访问通过tomcat HTTPS没有iSCSI，没有NFS，没有Fibre Channel180TB成本$9,305，即每GB $0.0517 Backblze Vaults cloud backup service99.999999% annual durability已经存储150PB，由1000个Storage Pod组成，40,000块盘，每天有10块盘损坏 每个Vault由20个Storage Pod组成，每个Pod有45块盘，即每个Vault有900块盘，一块盘如果是4TB，那么每个Vault可以存3.6PB每个磁盘使用ext4文件系统，每个Vault有个7位数字id，例如555-1021，前3位代表data center id，后4位是vault id有个类似name service的服务，client备份前先request name service获取vault id，之后client直接与相应的vault进行backup IO(https) 每个文件被分成20 shards = 17 data shars + 3 parity shards，存放在ext4每个shard有checksum，如果损坏，可以从其他17个shards恢复如果某个Vault有1个pod crash了，backup write的parity会变成2，如果3个pod坏了，那么也可以写，但parity就不存在了，如果此时再坏一个pod，数据无法恢复了 Referenceshttps://www.backblaze.com/blog/vault-cloud-storage-architecture/https://www.backblaze.com/blog/hard-drive-failure-rates-q1-2017/","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Reed-Solomon erasure code","slug":"erasure-coding","date":"2017-06-14T01:20:19.000Z","updated":"2017-06-14T05:28:03.000Z","comments":true,"path":"2017/06/14/erasure-coding/","link":"","permalink":"http://funkygao.github.io/2017/06/14/erasure-coding/","excerpt":"","keywords":[],"text":"Intro纠错码在RAID、备份、冷数据、历史数据存储方面使用广泛也有人利用它把一份数据分散到多个cloud provider(e,g. S3,Azure,Rackspace)，消除某个供应商的依赖: cloud of cloud Usage一份文件，大小x，分成n个数据块+k个校验块，能容忍任意k个数据块或者校验块错误，即至少要n个块是有效的1234567891011121314151617181920212223242526$encode -data 4 -par 2 README.md # n=4 k=2Opening README.mdFile split into 6 data+parity shards with 358 bytes/shard.Writing to README.md.0Writing to README.md.1Writing to README.md.2Writing to README.md.3Writing to README.md.4 # README.md.4 and README.md5 are parity shardsWriting to README.md.5$rm -f README.md.4 README.md.5 # remove the 2 parity shards$decode -out x README.mdOpening README.md.0Opening README.md.1Opening README.md.2Opening README.md.3Opening README.md.4Error reading file open README.md.4: no such file or directoryOpening README.md.5Error reading file open README.md.5: no such file or directoryVerification failed. Reconstructing dataWriting data to x$diff x README.md$ # all the same Algorithm文件内容‘ABCDEFGHIJKLMNOP’，4+2给定n和k，encoding矩阵是不变的 Referenceshttp://pages.cs.wisc.edu/~yadi/papers/yadi-infocom2013-paper.pdf","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"},{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"kafka ext4","slug":"kafka-ext4","date":"2017-06-12T02:03:31.000Z","updated":"2017-06-26T10:56:26.000Z","comments":true,"path":"2017/06/12/kafka-ext4/","link":"","permalink":"http://funkygao.github.io/2017/06/12/kafka-ext4/","excerpt":"","keywords":[],"text":"big file我目前使用ext4作为kafka存储，每个segment 1GB Ext4针对大文件顺序访问的主要优化 replace indirect blocks with extent(盘区) ext3采用多层间接地址映射，操作大文件时产生很多随机访问 one extra block read(seek) every 1024 blocks(4MB) ext4 extent是一组连续的数据块 123456789101112+---------+--------+----------+| logical | length | physical |+---------+--------+----------++ 0 | 1000 | 200 |+---------+--------+----------+type ext4_extent struct &#123; block uint32 // first logical block extent covers len uint16 // number of blocks covered by extent start_hi uint16 // high 16 bits of physical block start uint32 // low 32 bits of physical block&#125; inode preallocinode有很好的locality，同一目录下文件的inode尽量存放一起，加速了目录寻址性能 Multiblock Allocator(MBAlloc) ext3每次只能分配一个4KB(block size)的block ext4支持一次性分配多个block 延迟分配defer block allocation to writeback time online defregmentation e4defrag12345allocate more contiguous blocks in a temporary inoderead a data block from the original inodemove the corresponding block number from the temporary inode to the original inodewrite out the page Benchmark Summary http://www.linux-mag.com/id/7271/","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"fs atomic and ordering","slug":"fs-atomic-and-ordering","date":"2017-06-12T01:46:26.000Z","updated":"2017-06-12T03:37:17.000Z","comments":true,"path":"2017/06/12/fs-atomic-and-ordering/","link":"","permalink":"http://funkygao.github.io/2017/06/12/fs-atomic-and-ordering/","excerpt":"","keywords":[],"text":"应用层通过POSIX系统调用接口访问文件系统，但POSIX只规定了结果，底层的guarantee并没有提供，开发者只能去猜。不同的文件系统的实现不同，那些不“可见”的部分，会造成很大的差异；同时，每个文件系统又有些配置，这些配置也造成理解的困难。 Example12write(f1, &quot;pp&quot;)write(f2, &quot;qq&quot;) 如果write不是原子的，那么可能造成文件大小变化了，但内容没变(State#A)乱序造成State#C size atomicity content atomicity calls out of order Facts atomicity atomic single-sector overwrite目前绝大部分文件系统提供了atomic single-sector overwrite，主要是底层硬盘就已经提供了 atomic append 需要原子地修改inode+data block ext3/ext4/reiserfs writeback不支持 multi-block append目前绝大部分文件系统没有支持 rename/link这类directory operation，基本上是原子的 ordering","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"Wake on LAN","slug":"WOL","date":"2017-06-09T00:11:10.000Z","updated":"2017-07-10T01:25:38.000Z","comments":true,"path":"2017/06/09/WOL/","link":"","permalink":"http://funkygao.github.io/2017/06/09/WOL/","excerpt":"","keywords":[],"text":"WOL是个链路层协议，通过发送UDP广播 magic packet远程power upBIOS可以打开/关闭该功能 1234type MagicPacket struct &#123; header [6]byte // 0xFF payload [16]MACAddress&#125; 在facebook数据中心，开辟了一块单独的“冷存储”，专门保存不再查看的照片、视频(通常都是10年前的)，由于meta/data分离，用户想看，会wake这些存储设备","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Swinging Door Trending","slug":"SDT","date":"2017-06-08T06:52:01.000Z","updated":"2017-06-09T00:46:57.000Z","comments":true,"path":"2017/06/08/SDT/","link":"","permalink":"http://funkygao.github.io/2017/06/08/SDT/","excerpt":"","keywords":[],"text":"SDT旋转门流式压缩，有损算法，对于变化频率不高的时间序列数据，压缩比很高，但可能丢到峰值、波谷数据通过一条由起点和终点确定的直线代替一系列连续数据点通过线性差值“还原”数据，pref sample &lt;–插值–&gt; next sample vs facebook gorillahttp://www.vldb.org/pvldb/vol8/p1816-teller.pdf","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"socket recv buffer","slug":"socket-recv-buffer","date":"2017-06-08T03:03:16.000Z","updated":"2017-06-27T00:32:44.000Z","comments":true,"path":"2017/06/08/socket-recv-buffer/","link":"","permalink":"http://funkygao.github.io/2017/06/08/socket-recv-buffer/","excerpt":"","keywords":[],"text":"1BDP * 4 / 3 use iperf for testing12iperf -s -w 200K # serveriperf -c localhost # client 123456ethtool -k eth0 # segment offload makes tcpdump see a packet bigger than MTUnet.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_congestion_controlnet.ipv4.tcp_moderate_rcvbufnet.ipv4.tcp_slow_start_after_idle","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"golang pkg github pull request","slug":"golang-pkg-github-pull-request","date":"2017-06-08T00:33:57.000Z","updated":"2017-06-08T00:53:08.000Z","comments":true,"path":"2017/06/08/golang-pkg-github-pull-request/","link":"","permalink":"http://funkygao.github.io/2017/06/08/golang-pkg-github-pull-request/","excerpt":"","keywords":[],"text":"12345git remote add upstream https://github.com/funkygao/myrepo.gitgit fetch upstreamgit checkout -b mybranchgit commit -agit push upstream mybranch","categories":[],"tags":[{"name":"misc","slug":"misc","permalink":"http://funkygao.github.io/tags/misc/"}]},{"title":"ElasticSearch internals","slug":"ElasticSearch","date":"2017-06-05T05:35:53.000Z","updated":"2017-08-01T06:57:01.000Z","comments":true,"path":"2017/06/05/ElasticSearch/","link":"","permalink":"http://funkygao.github.io/2017/06/05/ElasticSearch/","excerpt":"","keywords":[],"text":"BasicsStorage Model一种特殊的LSM Tree123456ES LSM======== ==========translog WALbuffer MemTablesegment SSTablemerge merge &amp; compaction Defaults123456node.master: truenode.data: trueindex.number_of_shards: 5index.number_of_replicas: 1path.data: /path/to/data1,/path/to/data2http.max_content_length: 100mb Best Practice内存，一半给ES，一半给pagecache Indexing translog(WAL) 每个shard一个translog，即每个index一个translog 它提供实时的CRUD by docID，先translog然后再segments 默认5s刷盘 translog存放在path.data，可以stripe，但存在与segments竞争资源问题1234$ls nodes/0/indices/twitter/1/drwxr-xr-x 3 funky wheel 102B 6 8 17:23 _statedrwxr-xr-x 5 funky wheel 170B 6 8 17:23 indexdrwxr-xr-x 3 funky wheel 102B 6 8 17:23 translog refresh on segments scheduled periodically(1s)，也可手工 /_refresh 在此触发merge逻辑 refresh后，那么在此之前的所有变更就可以搜索了，in-memory buffer的数据是不能搜索的 产生新segment，但不fsync(close不会触发fsync)，然后open(the new segment) 不保证durability，那是由flush保证的 in-memory buffer清除 flush on translog 30s/200MB/5000 ops by default，会触发commit Any docs in the in-memory buffer are written to a new segment The buffer is cleared A commit point is written to diskcommit point is a file that contains list of segments ready for search The filesystem cache is flushed with an fsync The old translog is deleted commit 把还没有fsync的segments，一一fsync merge policy tiered(default) log_byte_size log_doc 可能把committed和uncommitted segments一起merge 异步，不影响indexing/query 合并后，fsync the merged big segment，之前的small segments被删除 ES内部有throttle机制控制merge进度，防止它占用过多资源: 20MB/s updatedelete, then insert。mysql内部的变长字段update也是这么实现的 Hard commit默认30分钟，或者translog size(200MB)，whoever come early 12345678把in-memory buffer内容生成一个新segment，并把buffer清空把每个还没有commit point的segment进行fsync生成commit point fileIndexSearcher is opened and all documents are searchabletranslog.empty()translog.sequence ++// 如果机器突然停电，可以完全恢复 Merge Happens in parallel to searching. Searcher is changed to new segment. Deleting a document creates a new document and .del file to keep track that document is deleted 由于一个document可能多次update，在不同的segment可能出现多次delete Replicationprimary-backup model master的indexing过程123validate indexing requestlocal indexingconcurrently dispatch the indexing request to all replicas Query local search1234translog.search(q)for s in range segments &#123; s.search(q)&#125; deep paginationsearch(from=50000, size=10)，那么每个shard会创建一个优先队列，队列大小=50010，从每个segment里取结果，直到填满而coordinating node需要创建的优先队列 number_of_shards * 50010 利用scan+scroll可以批量取数据(sorting disabled)，例如在reindex时使用 ClusterIntro所有node彼此相连，n*(n-1)个连接shard = MurMurHash3(document_id) % (num_of_primary_shards) Node1节点用数据的_id计算出数据应该存储在shard0上，通过cluster state信息发现shard0的主分片在Node3节点上，Node1转发请求数据给Node3,Node3完成数据的索引Node3并行转发(PUSH model replication)数据给分配有shard0的副本分片Node1和Node2上。当收到任一节点汇报副本分片数据写入成功以后，Node3即返回给初始的接受节点Node1，宣布数据写入成功。Node1成功返回给客户端。 Scale Rebalance123456789101112131415// 查看shard分布情况curl -XGET http://localhost:9200/_cat/shards// 手工rebalancecurl-XPOST &apos;http://localhost:9200/_cluster/reroute&apos; -d&apos;&#123; &quot;commands&quot;: [ &#123; &quot;move&quot;: &#123; &quot;index&quot;: &quot;mylog-2016-02-08&quot;, &quot;shard&quot;: 6, &quot;from_node&quot;: &quot;192.168.0.1&quot;, &quot;to_node&quot;: &quot;192.168.0.2&quot; &#125; &#125; ]&#125;&apos; 新节点加入过程12345678910// get master node and eligible master nodesfor host = range discovery.zen.ping.unicast.hosts &#123; PingResponse = ZendPing.send(host)&#125;send(&apos;internal:discovery/zen/join&apos;) to mastermaster.reply(&apos;internal:discovery/zen/join/validate&apos;)// 2PC joinmaster.update(ClusterState) and broadcast to all nodes, and wait for minimum_master_nodes ackClusterState change committed and confirmation sent discovery.zen.ping.unicast.hosts其实是种子，5个node配2个，也可以获取全局节点信息：知一点即知天下 Master fault detection默认，每个node每1s ping master，ping_timeout=30s，ping_retries=3失败后，会触发new master election Concurrency每个document有version，乐观锁 Consistency quorum(default) one all Shardallocation1234weightindex(node, index) = indexBalance * (node.numShards(index) – avgShardsPerNode(index))weightnode(node, index) = shardBalance * (node.numShards() – avgShardsPerNode)weightprimary(node, index) = primaryBalance * (node.numPrimaries() – avgPrimariesPerNode)weight(node, index) = weightindex(node, index) + weightnode(node, index) + weightprimary(node, index) 如果计算最后的weight(node, index)大于threshold， 就会发生shard迁移。 在一个已经创立的集群里，shard的分布总是均匀的。但是当你扩容节点的时候，你会发现，它总是先移动replica shard到新节点。这样就导致新节点全部分布的全是副本，主shard几乎全留在了老的节点上。 cluster.routing.allocation.balance参数，比较难找到合适的比例。 初始化1234master通过ClusterState分配一个新shardnode初始化一个空shard，并notify mastermaster mark the shard as startedif this is the first shard with a specific id, it is marked as primary Shrink Indexhttps://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html并不是resharding，开始时可以创建很多shard(e,g. 30)，用来灌数据，等数据好了，可以shrink到5它只是hard link segments，执行时要先把index置为readonly场景，例如tsdb，按照月份建索引，过了这个月，就可以shrink了 Consensuszen discovery(unicast/multicast)，存在脑裂问题，没有去解决，而是通过3台专用master机器，优化master逻辑，减少由于no reponse造成的partition可能性12node1, node2(master), node32-3不通，但1-2, 2-3都通，minimum_master_nodes=2，node3会重新选举自己成为master，而1同意了，2个master 默认ping_interval=1s ping_timeout=3s ping_retries=3 join_timeout=20*ping_interval 没有使用Paxos(zk)的原因: This, to me, is at the heart of our approach to resiliency.Zookeeper, an excellent product, is a separate external system, with its own communication layer.By having the discovery module in Elasticsearch using its own infrastructure, specifically the communication layer, it means that we can use the “liveness” of the cluster to assess its health.Operations happen on the cluster all the time, reads, writes, cluster state updates.By tying our assessment of health to the same infrastructure, we can actually build a more reliable more responsive system.We are not saying that we won’t have a formal Zookeeper integration for those who already use Zookeeper elsewhere. But this will be next to a hardened, built in, discovery module. ClusterStatemaster负责更改，并广播到机器的每个节点，每个节点本地保存如果有很多index，很多fields，很多shard，很多node，那么它会很大，因此它提供了增量广播机制和压缩 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;version&quot; : 11, &quot;master_node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;blocks&quot; : &#123; &#125;, &quot;nodes&quot; : &#123; &quot;sIh5gQcFThCcz3SO6txvvQ&quot; : &#123; &quot;name&quot; : &quot;Max&quot;, &quot;transport_address&quot; : &quot;inet[/162.245.23.194:9301]&quot;, &quot;attributes&quot; : &#123; &#125; &#125;, &quot;-mq1SRuuQoeEq-3S8SdHqw&quot; : &#123; &quot;name&quot; : &quot;Llyron&quot;, &quot;transport_address&quot; : &quot;inet[/162.245.23.194:9300]&quot;, &quot;attributes&quot; : &#123; &#125; &#125; &#125;, &quot;metadata&quot; : &#123; &quot;templates&quot; : &#123; &#125;, &quot;indices&quot; : &#123; &quot;blog&quot; : &#123; &quot;state&quot; : &quot;open&quot;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;uuid&quot; : &quot;UQMz5vbXSBqFU_8U3u4gYQ&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;number_of_shards&quot; : &quot;5&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;1030099&quot; &#125; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;user&quot; : &#123; &quot;properties&quot; : &#123; &quot;name&quot; : &#123; &quot;type&quot; : &quot;string&quot; &#125; &#125; &#125; &#125;, &quot;aliases&quot; : [ ] &#125; &#125; &#125;, &quot;routing_table&quot; : &#123; &quot;indices&quot; : &#123; &quot;blog&quot; : &#123; &quot;shards&quot; : &#123; &quot;4&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;0&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;3&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;1&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;2&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ] &#125; &#125; &#125; &#125;, &quot;routing_nodes&quot; : &#123; &quot;unassigned&quot; : [ ], &quot;nodes&quot; : &#123; &quot;sIh5gQcFThCcz3SO6txvvQ&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;sIh5gQcFThCcz3SO6txvvQ&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ], &quot;-mq1SRuuQoeEq-3S8SdHqw&quot; : [ &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 4, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 0, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 3, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : true, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 1, &quot;index&quot; : &quot;blog&quot; &#125;, &#123; &quot;state&quot; : &quot;STARTED&quot;, &quot;primary&quot; : false, &quot;node&quot; : &quot;-mq1SRuuQoeEq-3S8SdHqw&quot;, &quot;relocating_node&quot; : null, &quot;shard&quot; : 2, &quot;index&quot; : &quot;blog&quot; &#125; ] &#125; &#125;, &quot;allocations&quot; : [ ]&#125; Internal Data Structures and AlgorithmsRoaring bitmapT-Digest PercentilesHDRHistogram PercentilesUse Casesgithub之前用Solr，后来改为elasticsearch，运行在多个集群。其中，存储代码的集群26个data node，8 coordinator node，每个data node有2TB SSD，510 shards with 2 replicas Referenceshttps://www.elastic.co/blog/resiliency-elasticsearchhttps://github.com/elastic/elasticsearch/issues/2488http://blog.mikemccandless.com/2011/02/visualizing-lucenes-segment-merges.htmlhttp://blog.trifork.com/2011/04/01/gimme-all-resources-you-have-i-can-use-them/https://github.com/elastic/elasticsearch/issues/10708https://github.com/blog/1397-recent-code-search-outageshttps://speakerd.s3.amazonaws.com/presentations/d7c3effadc8146a7af6229d3b5640162/friday-colin-g-zach-t-all-about-es-algorithms-and-data-structiures-stage-c.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"Kafka vs Kinesis vs Redis","slug":"Kafka-vs-Kinesis-Redis","date":"2017-06-05T03:15:08.000Z","updated":"2017-06-05T03:29:07.000Z","comments":true,"path":"2017/06/05/Kafka-vs-Kinesis-Redis/","link":"","permalink":"http://funkygao.github.io/2017/06/05/Kafka-vs-Kinesis-Redis/","excerpt":"","keywords":[],"text":"vs KinesisUS East region，要支持10万/秒的吞吐量，Kinesis需要的费用是4787美元/月 https://www.quora.com/Amazon-Kinesis-versus-Apache-Kafka-which-of-them-is-the-most-proven-and-high-performance-oriented vs Redis PubSub redis OOM kafka replay messages, horizontal scale, HA, async Pub","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"facebook Mystery Machine","slug":"facebook-Mystery-Machine","date":"2017-06-05T01:49:00.000Z","updated":"2017-06-05T02:58:05.000Z","comments":true,"path":"2017/06/05/facebook-Mystery-Machine/","link":"","permalink":"http://funkygao.github.io/2017/06/05/facebook-Mystery-Machine/","excerpt":"","keywords":[],"text":"Data Flowconsistent sampling 1local log --&gt; Scribe --&gt; Hive --&gt; UberTrace --&gt; UI Log Schema1234request idhost idhost-local timestampunique event label(event name, task name) Timestamp Normalize 不考虑local clock skew 假设client/server间的RTT是对称的 123456789101112131415Client Server 1|------------&gt;| | |--+1.1 | | | logic | |&lt;-+1.2 2|&lt;------------|1.2 - 1.1 = 0.12 - 1 = 1.0RTT = (1.0 - 0.1)/2 = 0.45clock(1.1) = 1 + 0.45 = 1.45clock(1.2) = 1.45 + 0.1 = 1.55RTT是个经验值，根据大量的trace后稳定下来: 使用最小值","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"storm acker","slug":"storm-ack","date":"2017-06-02T01:03:52.000Z","updated":"2017-06-02T06:36:42.000Z","comments":true,"path":"2017/06/02/storm-ack/","link":"","permalink":"http://funkygao.github.io/2017/06/02/storm-ack/","excerpt":"","keywords":[],"text":"Acker对于Spout产生的每一个tuple, storm都会进行跟踪，利用RotatingMap存放内存，但有保护措施，不会打爆 当Spout触发fail动作时，storm不会自动重发失败的tuple，只是向Spout发送fail消息，触发Spout.fail回调，真正的重发需要在Spout.fail里实现 tuple tree，中间任意一个edge fail，会理解触发Spout的fail，但后面的Bolt的执行不受影响。做无用功？ Spout Executor Bolt Executor","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"monkey patch golang","slug":"monkey-patch-golang","date":"2017-06-02T00:08:25.000Z","updated":"2017-06-02T00:09:22.000Z","comments":true,"path":"2017/06/02/monkey-patch-golang/","link":"","permalink":"http://funkygao.github.io/2017/06/02/monkey-patch-golang/","excerpt":"","keywords":[],"text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( &quot;syscall&quot; &quot;unsafe&quot;)func a() int &#123; return 1 &#125;func b() int &#123; return 2 &#125;func getPage(p uintptr) []byte &#123; return (*(*[0xFFFFFF]byte)(unsafe.Pointer(p &amp; ^uintptr(syscall.Getpagesize()-1))))[:syscall.Getpagesize()]&#125;func rawMemoryAccess(b uintptr) []byte &#123; return (*(*[0xFF]byte)(unsafe.Pointer(b)))[:]&#125;func assembleJump(f func() int) []byte &#123; funcVal := *(*uintptr)(unsafe.Pointer(&amp;f)) return []byte&#123; 0x48, 0xC7, 0xC2, byte(funcVal &gt;&gt; 0), byte(funcVal &gt;&gt; 8), byte(funcVal &gt;&gt; 16), byte(funcVal &gt;&gt; 24), // MOV rdx, funcVal 0xFF, 0x22, // JMP rdx &#125;&#125;func replace(orig, replacement func() int) &#123; bytes := assembleJump(replacement) functionLocation := **(**uintptr)(unsafe.Pointer(&amp;orig)) window := rawMemoryAccess(functionLocation) page := getPage(functionLocation) syscall.Mprotect(page, syscall.PROT_READ|syscall.PROT_WRITE|syscall.PROT_EXEC) copy(window, bytes)&#125;func main() &#123; replace(a, b) print(a())&#125; Referencehttps://software.intel.com/en-us/articles/introduction-to-x64-assemblyhttps://www.hopperapp.com/","categories":[],"tags":[{"name":"hacking","slug":"hacking","permalink":"http://funkygao.github.io/tags/hacking/"}]},{"title":"KafkaSpout","slug":"KafkaSpout","date":"2017-06-01T10:04:04.000Z","updated":"2017-06-02T06:14:53.000Z","comments":true,"path":"2017/06/01/KafkaSpout/","link":"","permalink":"http://funkygao.github.io/2017/06/01/KafkaSpout/","excerpt":"","keywords":[],"text":"configTopology config TOPOLOGY_WORKERS 整个topology在所有节点上的java进程总数 例如，设置成25，parallelism=150，那么每个worker进程会创建150/25=6个线程执行task TOPOLOGY_ACKER_EXECUTORS = 20 不设或者为null，it=TOPOLOGY_WORKERS，即one acker task per worker 设置为0，表示turn off ack/reliability TOPOLOGY_MAX_SPOUT_PENDING = 5000 123(defn executor-max-spout-pending [storm-conf num-tasks] (let [p (storm-conf TOPOLOGY-MAX-SPOUT-PENDING)] (if p (* p num-tasks)))) max in-flight(not ack or fail) spout tuples on a single spout task at once 如果不指定，默认是1 TOPOLOGY_BACKPRESSURE_ENABLE = false TOPOLOGY_MESSAGE_TIMEOUT_SECS30s by default KafkaSpout config123fetchSizeBytes = 1024 * 1024 * 2 // 1048576=1M by default FetchRequestfetchMaxWait = 10000 // by defaultforceFromStart = false emit/ack/fail flow1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class PartitionManager &#123; SortedSet&lt;Long&gt; _pending, failed = new TreeSet(); LinkedList&lt;MessageAndRealOffset&gt; _waitingToEmit = new LinkedList(); func EmitState next(SpoutOutputCollector collector) &#123; if (this._waitingToEmit.isEmpty()) &#123; // 如果内存里数据都发出，就调用kafka consumer一次性批量填充内存_waitingToEmit // 填充时，如果发现failed里有东西，那么就从head of failed(offset) FetchRequest: 重发机制 this.fill(); &#125; // 从LinkedList _waitingToEmit里取一条消息 MessageAndRealOffset toEmit = (MessageAndRealOffset)this._waitingToEmit.pollFirst(); // emit时指定了messageID // BasicBoltExecutor.execute会通过template method自动执行_collector.getOutputter().ack(input) // 即KafkaSpout.ack -&gt; PartitionManager.ack collector.emit(tup, new PartitionManager.KafkaMessageId(this._partition, toEmit.offset)); // 该tuple处于pending state &#125; // Note: a tuple will be acked or failed by the exact same Spout task that created it func ack(Long offset) &#123; this._pending.remove(offset) &#125; func fail(Long offset) &#123; this.failed.add(offset); // kafka consumer会reset offset to the failed msg，重新消费 &#125;&#125;class TopologyBuilder &#123; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125;&#125;class BasicBoltExecutor &#123; public void execute(Tuple input) &#123; _collector.setContext(input); try &#123; _bolt.execute(input, _collector); _collector.getOutputter().ack(input); &#125; catch(FailedException e) &#123; _collector.getOutputter().fail(input); &#125; &#125;&#125; Bolt ackKafkaSpout产生的每个tuple，Bolt必须进行ack，否则30s后KafkaSpout会认为emitted tuple tree not fully processed，进行重发123456class MyBolt &#123; public void execute(Tuple tuple) &#123; _collector.emit(new Values(foo, bar)) _collector.ack(tuple) &#125;&#125; OOM如果消息处理一直不ack，累计的unacked msg越来越多，会不会OOM?NOKafkaSpout只保留offset，不会保存每条emitted but no ack/fail msg spout throttle1.0.0之前，只能用TOPOLOGY_MAX_SPOUT_PENDING控制但这个参数很难控制，它有一些与其他参数配合使用才能生效的机制，而且如果使用Trident语义又完全不同1.0.0之后，可以通过backpressure Storm messaging intra-workerDisruptor inter-worker0MQ/Netty Referenceshttp://www.michael-noll.com/blog/2013/06/21/understanding-storm-internal-message-buffers/http://jobs.one2team.com/apache-storms/http://woodding2008.iteye.com/blog/2335673","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"aliware","slug":"aliware","date":"2017-06-01T07:34:25.000Z","updated":"2017-06-01T08:37:15.000Z","comments":true,"path":"2017/06/01/aliware/","link":"","permalink":"http://funkygao.github.io/2017/06/01/aliware/","excerpt":"","keywords":[],"text":"https://www.aliyun.com/aliware EDAS Enterprise Distributed Application Service RPC framework + elasticjob + qconf + GTS + Dapper + autoscale 鉴权数据下放到服务机器，避免性能瓶颈 MQ DRDS TDDL proxy avg() =&gt; sum()/count() 扩容，切换时client可能会失败，需要client retry 通过/ TDDL: XXX/ sql注释来实现特定SQL路由规则 可以冷热数据分离 ARMSstreaming processing，具有nifi式visualized workflow editor GTS 1234567@GtsTransaction(timeout=1000*60)public void transfer(DataSource db1, DataSource db2) &#123; // 强一致，但涉及的事务比较大时，性能下降非常明显 // 通过soft state MQ的最终一致性吞吐量要好的多 db1.getConnection().execute(&quot;sql1&quot;) db2.getConnection().execute(&quot;sql2&quot;)&#125; SchedulerX CSBcloud service bus，相当于api gateway，包含协议转换","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"TiDB KV Mapping","slug":"TiDB-KV-Mapping","date":"2017-06-01T06:30:58.000Z","updated":"2017-06-01T06:37:51.000Z","comments":true,"path":"2017/06/01/TiDB-KV-Mapping/","link":"","permalink":"http://funkygao.github.io/2017/06/01/TiDB-KV-Mapping/","excerpt":"","keywords":[],"text":"Data12Key： tablePrefix_rowPrefix_tableID_rowIDValue: [col1, col2, col3, col4] Unique Index12Key: tablePrefix_idxPrefix_tableID_indexID_indexColumnsValueValue: rowID Non-Unique Index12Key: tablePrefix_idxPrefix_tableID_indexID_ColumnsValue_rowIDValue: nil 12345var ( tablePrefix = []byte&#123;&apos;t&apos;&#125; recordPrefixSep = []byte(&quot;_r&quot;) indexPrefixSep = []byte(&quot;_i&quot;))","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"SSD vs HDD","slug":"ssd-vs-hdd","date":"2017-06-01T03:27:37.000Z","updated":"2017-06-02T08:17:20.000Z","comments":true,"path":"2017/06/01/ssd-vs-hdd/","link":"","permalink":"http://funkygao.github.io/2017/06/01/ssd-vs-hdd/","excerpt":"","keywords":[],"text":"http://www.pcgamer.com/hard-drive-vs-ssd-performance/2/ 123 RandRead RandWrite SeqRead SeqWriteSSD 50 200 2300 1300HDD 0.6 1.0 200 130","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"kafka internals","slug":"kafka-internals","date":"2017-06-01T03:24:34.000Z","updated":"2017-06-22T07:29:30.000Z","comments":true,"path":"2017/06/01/kafka-internals/","link":"","permalink":"http://funkygao.github.io/2017/06/01/kafka-internals/","excerpt":"","keywords":[],"text":"Overview Controller负责 leadership change of a partitioneach partition leader can independently update ISR new topics; deleted topics replica re-assignment 曾经的设计是没有controller，每个broker要决策时都通过zk，加入controller坏处就是要实现controller failover 1234567class KafkaController &#123; partitionStateMachine PartitionStateMachine replicaStateMachine ReplicaStateMachine // controller选举，并LeaderChangeListener controllerElector ZookeeperLeaderElector &#125; ControllerContext一个全局变量。在选举为controller时，一次性从zk里读入所有状态信息KafkaController.initializeControllerContext 12345678910111213141516171819202122class ControllerContext &#123; epoch, epochZkVersion, correlationId controllerChannelManager // 维护controller与每个broker之间的socket conn // 初始化时从/brokers/ids里取得的，并在BrokerChangeListener里修改 liveBrokersUnderlying: Set[Broker] // 初始化时从/brokers/topics里取得的，并在TopicChangeListener里修改 allTopics: Set[String] // 初始化时从/brokers/topics/$topic里一一取得，并在如下情况下修改 // - assignReplicasToPartitions.assignReplicasToPartitions // - updateAssignedReplicasForPartition // - TopicChangeListener // - ReplicaStateMachine.handleStateChange partitionReplicaAssignment: mutable.Map[TopicAndPartition, Seq[Int]] // AR // 初始化时从/brokers/topics/$topic/$partitionID/state一一取得，如下情况下修改 // - PartitionStateMachine.initializeLeaderAndIsrForPartition // - PartitionStateMachine.electLeaderForPartition partitionLeadershipInfo: mutable.Map[TopicAndPartition, LeaderIsrAndControllerEpoch]&#125; ControllerChannelManagercontroller为每个broker建立一个socket(BlockingChannel)连接和一个thread用于从内存batch取request进行逐条socket send/receivee,g. 1个8节点的机器，controller上为这部分工作，需要建立(7个线程 + 7个socket conn)与每个broker的连接是并发的，互不干扰；对于某一个broker，请求是完全串行的 config controller.socket.timeout.msdefault 30s, socket connect/io timeout broker shutdown/startup/crash?zk.watch(“/brokers/ids”), BrokerChangeListener会调用ControllerChannelManager.addBroker/removeBroker conn broken/timeout?如果socket send/receive失败，那么自动重连重发，backoff=300ms，死循环但如果broker长时间无法reach，它会触发zk.watch(“/brokers/ids”)，removeBroker，死循环退出 ControllerBrokerRequestBatchcontroller -&gt; broker，这里的圣旨请求有3种，都满足幂等性 LeaderAndIsrRequestreplicaManager.becomeLeaderOrFollower UpdateMetadataRequestreplicaManager.maybeUpdateMetadataCache StopReplicaRequest删除topic onBrokerStartup由BrokerChangeListener触发1234567sendUpdateMetadataRequest(newBrokers)// 告诉它它上面的所有partitionsreplicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, OnlineReplica)// 让所有的NewPartition/OfflinePartition进行leader electionpartitionStateMachine.triggerOnlinePartitionStateChange() onBrokerFailure找出受影响的状态，并触发partitionStateMachine、replicaStateMachine的状态切换 StateMachine只有controller那台机器的state machine才会启动 terms12345678910111213141516Replica &#123; broker_id int partition Partition // a Replica belongs to a Partition log Log hw, leo long isLeader bool&#125;Partition &#123; topic string partition_id int leader Replica ISR Set[Replica] AR Set[Replica] // assigned replicas zkVersion long // for CAS&#125; PartitionStateMachine每个partition的状态，负责分配、选举partion的leader1234567891011class PartitionStateMachine &#123; // - NonExistentPartition // - NewPartition // - OnlinePartition // - OfflinePartition partitionState: mutable.Map[TopicAndPartition, PartitionState] = mutable.Map.empty topicChangeListener // childChanges(&quot;/brokers/topics&quot;) addPartitionsListener // all.dataChanges(&quot;/brokers/topics/$topic&quot;) deleteTopicsListener // childChanges(&quot;/admin/delete_topics&quot;)&#125; PartitionLeaderSelector.selectLeader OfflinePartitionLeaderSelector ReassignedPartitionLeaderSelector PreferredReplicaPartitionLeaderSelector ControlledShutdownLeaderSelector ReplicaStateMachine每个partition在assigned replic(AR)上的状态，track每个broker的存活12345678910class ReplicaStateMachine &#123; replicaState: mutable.Map[PartitionAndReplica, ReplicaState] = mutable.Map.empty brokerRequestBatch = new ControllerBrokerRequestBatch // 只有controller关心每个broker的存活，broker自己是不关心的 // 而且broker die只有2种标准，不会因为conn broken认为broker死 // 1. broker id ephemeral znode deleted // 2. broker controlled shutdown brokerChangeListener // childChanges(&quot;/brokers/ids&quot;)&#125; startup对所有replica，如果其对应的broker活，那么就置为OnlineReplica，否则ReplicaDeletionIneligible state transition 校验当前状态与目标状态 维护内存replicaState 必要时通过brokerRequestBatch广播给所有broker Failoverbroker failover每个broker有KafkaHealthcheck，它向/brokers/ids/$id这个ephemeral znode写数据，session expire就会重写，其中timestamp置为当前时间1&#123;&quot;jmx_port&quot;:-1,&quot;timestamp&quot;:&quot;1460677527837&quot;,&quot;host&quot;:&quot;10.1.1.1&quot;,&quot;version&quot;:1,&quot;port&quot;:9002&#125; controller的BrokerChangeListener监视所有broker的存活参考 onBrokerStartup， onBrokerFailure leader failover一个partition的leader broker crash了，controlle选举出新的leader后，该new leader的LEO会成为新的HWleader负责维护/propogate HW，以及每个follower的LEO 存在一个partition多Leader脑裂12345partition0 broker(A, B, C)，A是leaderA GC很久，crontroller认为A死，让B成为leader，写zk ISR znode，正在此时A活了但还没有收到controller发来的RPC，此时A、B都是leader如果client1连接A，clientB连接B，他们都发消息?A活过来的时候，A认为它的ISR=(A,B,C)，clientA发的消息commit条件是A,B,C都ack by fetch request才可以，但B在promoted to leader后会先stop fetching from previous leader。因此，A只有shrink ISR后才可能commit消息，但shrink时它会写zk，通过CAS失败，此时A意识到它已经不是leader controller failovercontroller session expire example1234567891011121314151617181920212223242526272829broker1，是controller 在sessionTimeout*2/3=4s内还没有收到response，就会try next zk server zk connected，发现session(0x35c1876461ec9d3)已经expire了 触发KafkaController.SessionExpirationListener &#123; onControllerResignation() brokerState设置为RunningAsBroker // 之前是RunningAsController controllerElector.elect &#123; switch write(&quot;/controller&quot;) &#123; case ok: onBecomingLeader &#123; // 1 successfully elected as leader // Broker 1 starting become controller state transition controllerEpoch.increment() register listeners replicaStateMachine.startup() partitionStateMachine.startup() brokerState = RunningAsController sendUpdateMetadataRequest(to all live brokers) &#125; case ZkNodeExistsException: read(&quot;/controller&quot;) and get leaderID &#125; &#125; &#125;broker0: ZookeeperLeaderElector.LeaderChangeListener被触发，它watch(&quot;/controller&quot;) &#123; elect &#125; Misc ReplicationUtils.updateLeaderAndIsr Referenceshttps://issues.apache.org/jira/browse/KAFKA-1460https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"scale","slug":"scale","date":"2017-05-31T23:48:41.000Z","updated":"2017-06-02T08:17:07.000Z","comments":true,"path":"2017/06/01/scale/","link":"","permalink":"http://funkygao.github.io/2017/06/01/scale/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"gnatsd","slug":"gnatsd","date":"2017-05-31T07:33:01.000Z","updated":"2017-06-05T03:26:09.000Z","comments":true,"path":"2017/05/31/gnatsd/","link":"","permalink":"http://funkygao.github.io/2017/05/31/gnatsd/","excerpt":"","keywords":[],"text":"2011年用Ruby完成了一个版本，后来用golang重写，目前只维护golang版本NATS = Not Another Tibco Server 协议类似redis持久化通过上层的NATS Streaming完成 竞品是0mq/nanomsg/aeron https://github.com/real-logic/Aeron","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"microservice gateway","slug":"microservice-gateway","date":"2017-05-31T06:11:12.000Z","updated":"2017-05-31T06:24:37.000Z","comments":true,"path":"2017/05/31/microservice-gateway/","link":"","permalink":"http://funkygao.github.io/2017/05/31/microservice-gateway/","excerpt":"","keywords":[],"text":"需要的功能 traffic management failure recovery A/B testing, canary releases discovery, load balance throttling rate limit fault injection observability metrics, monitor, alert, insights policy enforcement access control service identity and security end-to-end authz","categories":[],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://funkygao.github.io/tags/cloud/"}]},{"title":"Database as a Service","slug":"Database-as-a-Service","date":"2017-05-31T05:37:27.000Z","updated":"2017-05-31T05:43:41.000Z","comments":true,"path":"2017/05/31/Database-as-a-Service/","link":"","permalink":"http://funkygao.github.io/2017/05/31/Database-as-a-Service/","excerpt":"","keywords":[],"text":"Trove https://wiki.openstack.org/wiki/Trove","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"DB Schema Management","slug":"DB-Schema-Management","date":"2017-05-31T03:06:12.000Z","updated":"2017-05-31T03:15:29.000Z","comments":true,"path":"2017/05/31/DB-Schema-Management/","link":"","permalink":"http://funkygao.github.io/2017/05/31/DB-Schema-Management/","excerpt":"","keywords":[],"text":"https://github.com/skeema/skeema 统一控制dev/test/staging/prod等环境的scheme 12345678910111213141516$skeemaSkeema is a MySQL schema management tool. It allows you to export a databaseschema to the filesystem, and apply online schema changes by modifying files.Usage: skeema [&lt;options&gt;] &lt;command&gt;Commands: add-environment Add a new named environment to an existing host directory diff Compare a DB instance&apos;s schemas and tables to the filesystem help Display usage information init Save a DB instance&apos;s schemas and tables to the filesystem lint Verify table files and reformat them in a standardized way pull Update the filesystem representation of schemas and tables push Alter tables on DBs to reflect the filesystem representation version Display program version 与mysql在线alter table设计不同，它是higher level的，底层仍旧需要OSC支持:1alter-wrapper=&quot;/usr/local/bin/pt-online-schema-change --execute --alter &#123;CLAUSES&#125; D=&#123;SCHEMA&#125;,t=&#123;TABLE&#125;,h=&#123;HOST&#125;,P=&#123;PORT&#125;,u=&#123;USER&#125;,p=&#123;PASSWORDX&#125;&quot; Referenceshttps://www.percona.com/live/17/sessions/automatic-mysql-schema-management-skeema","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"Cassandra vs ScyllaDB","slug":"Cassandra","date":"2017-05-31T00:16:23.000Z","updated":"2017-07-18T00:35:59.000Z","comments":true,"path":"2017/05/31/Cassandra/","link":"","permalink":"http://funkygao.github.io/2017/05/31/Cassandra/","excerpt":"","keywords":[],"text":"CassandraCassandra 项目诞生于 Facebook，后来团队有人跳到 Amazon 做了另外一个 NoSQL 数据库 DynamoDB。最开始由两个facebook的员工最开始开发出来的，其中一个还直接参与了Amazon的Dynamo的开发。 Dynamo论文发表于2007年，用于shopping cartCassandra在2008年被facebook开源，用于inbox searchUber现在有全球最大的Cassandra data center Features CQL(Cassandra Query Language) 1,000 node clusters multi-data center out-of-the-box replication ecosystem with Spark Internals LSM Tree Gossip P2P DHT consistency same as DynamoDB ONE QUORUM ALL read repair Thrift ScyllaDBKVM核心人员用C++写的Cassandra(Java) clone，单机性能提高了10倍，主要原因是： DPDK, bypass kernel O_DIRECT IO, bypass pagecache, cache由scylla自己管理 pagecahce的格式必须是文件的格式(sstable)，而app level cache更有效，更terse compaction的时候，pagecache讲是个累赘，它可能造成很多热点数据的淘汰 把一个node看做是多个cpu core组成的cluster, share nothing sharding at the cpu core instead of node更充分利用多核，减少contention，充分利用cpu cache, NUMA friendly 在需要core间交换数据时，使用explicit message passing avoid JVM GC Referenceshttps://db-engines.com/en/rankinghttps://github.com/scylladb/scyllahttp://www.scylladb.com/http://www.seastar-project.org/https://www.reddit.com/r/programming/comments/3lzz56/scylladb_cassandra_rewritten_in_c_claims_to_be_up/https://news.ycombinator.com/item?id=10262719","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"etcd3 vs zookeeper","slug":"etcd3-vs-zookeeper","date":"2017-05-25T09:07:34.000Z","updated":"2017-06-06T01:54:53.000Z","comments":true,"path":"2017/05/25/etcd3-vs-zookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/25/etcd3-vs-zookeeper/","excerpt":"","keywords":[],"text":"etcd v3独有的特性 get and watch by prefix, by interval lease based TTL for key sets runtime reconfiguration point in time backup extensive metrics 获取历史版本数据(这个非常有用)multi-version mini transation DSL 123456789Tx.If( Compare(Value(&quot;foo&quot;), &quot;&gt;&quot;, &quot;bar&quot;), Compare(Value(Version(&quot;foo&quot;), &quot;=&quot;, 2), ...).Then( Put(&quot;ok&quot;, &quot;true&quot;)...).Else( Put(&quot;ok&quot;, &quot;false&quot;)...).Commit() leases 1234l = CreateLeases(15*second)Put(foo, bar, l)l.KeepAlive()l.Revoke() watcher功能丰富 streaming watch 支持index参数，不会lose event recursive off-heap内存中只保留index，大部分数据通过mmap映射到boltdb file incremental snapshot zk独有的特性 ephemeral znode non-blocking full fuzzy snapshotToo busy to snap, skipping key支持在N Millions on-heap etcd2 etcd2的key支持在10K量级，etcd3支持1M量级 原因在于snapshot成本，可能导致0 qps，甚至reelection comparisonmemory footprint2M 256B keys123etcd2 10GBzk 2.4GBetcd3 0.8GB Referenceshttps://coreos.com/blog/performance-of-etcd.html","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"2017 kafka report","slug":"2017-kafka-report","date":"2017-05-25T01:30:27.000Z","updated":"2017-05-25T01:54:10.000Z","comments":true,"path":"2017/05/25/2017-kafka-report/","link":"","permalink":"http://funkygao.github.io/2017/05/25/2017-kafka-report/","excerpt":"","keywords":[],"text":"调查来自47个国家的388个组织(公司)26%受访者年销售额10亿美金以上15%受访者每天处理10亿消息/天43%受访者在公有云上使用kafka，其中60%是AWS Referenceshttps://www.confluent.io/wp-content/uploads/2017-Apache-Kafka-Report.pdf","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"zookeeper processor","slug":"zookeeper-processor","date":"2017-05-25T00:39:35.000Z","updated":"2017-05-26T07:11:03.000Z","comments":true,"path":"2017/05/25/zookeeper-processor/","link":"","permalink":"http://funkygao.github.io/2017/05/25/zookeeper-processor/","excerpt":"","keywords":[],"text":"Chain of Responsibility为了实现各种服务器的代码结构的高度统一，不同角色的server对应不同的processor chain 123interface RequestProcessor &#123; void processRequest(Request request) throws RequestProcessorException;&#125; LeaderZooKeeperServer.java FollowerZooKeeperServer.java ZooKeeperServer.java1234567func processPacket() &#123; submitRequest()&#125;func submitRequest(req) &#123; firstProcessor.processRequest(req)&#125;","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"kafka redesign","slug":"kafka-redesign","date":"2017-05-24T02:21:06.000Z","updated":"2017-05-24T08:15:10.000Z","comments":true,"path":"2017/05/24/kafka-redesign/","link":"","permalink":"http://funkygao.github.io/2017/05/24/kafka-redesign/","excerpt":"","keywords":[],"text":"Goals support many topics needle in haystack IO optimization R/W isolation index file leads to random sync write","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"apache bookeeper","slug":"apache-bookeeper","date":"2017-05-23T09:04:32.000Z","updated":"2017-05-24T08:13:14.000Z","comments":true,"path":"2017/05/23/apache-bookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/23/apache-bookeeper/","excerpt":"","keywords":[],"text":"Features 没有topic/partition概念，一个stream是由多个ledger组成的，每个ledger是有边界的createLedger(int ensSize, int writeQuorumSize, int ackQuorumSize) ledger只有int id，没有名字 每个entry(log)是有unique int64 id的 striped write: 交错存储 各个存储节点bookie之间没有明确的主从关系 shared WAL single writer Quorum投票复制，通过存储在zk里的LastAddConfirmedId确保read consistency bookie does not communicate with other bookies，由client进行并发broadcast/quorum VS Kafka createLedger时，客户端决定如何placement(org.apache.bookkeeper.client.EnsemblePlacementPolicy)，然后存放在zookeeper例如，5个bookie，createLedger(3, 3, 2) IO Model 读写分离 Disk 1: Journal(WAL) Device {timestamp}.txn Disk 2: Ledger Device 数据存放在多个ledger目录 LastLogMark表示index+data在此之前的都已经持久化到了Ledger Device，之前的WAL可以删除 异步写 而且是顺序写 所有的active ledger共用一个entry logger 读的时候利用ledger index/cache [Disk 3]: Index Device 默认Disk2和Disk3是在一起的 在写入Memtable后，就可以向client ack了 IO被分成4种类型，分别优化 sync sequential write: shared WAL async random write: group commit from Memtable tail read: from Memtable random read: from (index + os pagecache) Referenceshttps://github.com/ivankelly/bookkeeper-tutorialhttps://github.com/twitter/DistributedLog","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"SSD","slug":"SSD","date":"2017-05-22T06:31:39.000Z","updated":"2017-07-26T03:39:13.000Z","comments":true,"path":"2017/05/22/SSD/","link":"","permalink":"http://funkygao.github.io/2017/05/22/SSD/","excerpt":"","keywords":[],"text":"PrimerPhysical unit of flash memory Pageunit for read &amp; write Blockunit for erase 物理特性 Erase before re-write Sequential write within a block Cost: 17-32x more expensive per GB than disk Optimal I/O for SSD I/O request size越好越好 要符号物理特性 page or block对齐 segmented sequential write within a block http://codecapsule.com/2014/02/12/coding-for-ssds-part-6-a-summary-what-every-programmer-should-know-about-solid-state-drives/http://www.open-open.com/lib/view/open1423106687217.html","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"oklog","slug":"oklog","date":"2017-05-22T03:22:58.000Z","updated":"2017-05-23T01:35:42.000Z","comments":true,"path":"2017/05/22/oklog/","link":"","permalink":"http://funkygao.github.io/2017/05/22/oklog/","excerpt":"","keywords":[],"text":"injecter负责write优化(WAL)，让storage node负责read优化与RocketMQ类似: CQRS injecter = commit log storage node = consume log 不同在于：storage node是通过pull mode replication机制实现，可以与injecter位于不同机器而RocketMQ的commit log与consume log是在同一台broker上的 kafka couples R/W，无法独立scale CQRS decouples R/W，可以独立scale produceProducer通过forwarder连接到多个injecter上，injecter间通过gossip来负载均衡，load高的会通过与forwarder协商进行redirect distribution queryscatter-gather","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"db trigger","slug":"db-trigger","date":"2017-05-22T02:40:14.000Z","updated":"2017-05-22T02:50:00.000Z","comments":true,"path":"2017/05/22/db-trigger/","link":"","permalink":"http://funkygao.github.io/2017/05/22/db-trigger/","excerpt":"","keywords":[],"text":"触发器的缺陷 如何监控 代码的版本控制 test 部署 性能损耗 多租户 资源隔离 无法频繁发布，如何应付频繁的需求变更","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"materialized view","slug":"materialized-view","date":"2017-05-22T02:05:02.000Z","updated":"2017-05-22T02:57:51.000Z","comments":true,"path":"2017/05/22/materialized-view/","link":"","permalink":"http://funkygao.github.io/2017/05/22/materialized-view/","excerpt":"","keywords":[],"text":"物化试图，可以理解为cache of query results, derived result觉得用“异构表”可能更贴切 与试图不同，它是物理存在的，并由数据库来确保与主库的一致性它是随时可以rebuilt from source store，应用是从来不会更新它的: readonly MySQL没有提供该功能，但通过dbus可以方便构造materialized viewPostgreSQL提供了materialized view Referenceshttps://docs.microsoft.com/en-us/azure/architecture/patterns/materialized-view","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"cache invalidation","slug":"cache-invalidation","date":"2017-05-22T01:52:37.000Z","updated":"2017-05-22T02:59:27.000Z","comments":true,"path":"2017/05/22/cache-invalidation/","link":"","permalink":"http://funkygao.github.io/2017/05/22/cache-invalidation/","excerpt":"","keywords":[],"text":"1234567891011// read dataval = cache.get(key)if val == nil &#123; val = db.get(key) cache.put(key, val)&#125;return val// write datadb.put(key, val)cache.put(key, val) 这会造成dual write conflict 如果需要的只是eventaul consistency，那么通过dbus来进行cache invalidation是最有效的 https://martinfowler.com/bliki/TwoHardThings.html","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"Linkedin Esprosso","slug":"Linked-Esprosso","date":"2017-05-22T00:07:48.000Z","updated":"2017-06-29T23:53:38.000Z","comments":true,"path":"2017/05/22/Linked-Esprosso/","link":"","permalink":"http://funkygao.github.io/2017/05/22/Linked-Esprosso/","excerpt":"","keywords":[],"text":"WhatDistributed Document Store RESTful API MySQL作为存储 Helix负责集群 Databus异步replicate不同数据中心commit log Schema存放在zookeeper，通过Avro的兼容性实现schema evolution Referenceshttps://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store https://nonprofit.linkedin.com/content/dam/static-sites/thirdPartyJS/github-gists?e_origin=https://engineering.linkedin.com&amp;e_channel=resource-iframe-embed-4","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"Multi-Data Center Consistency","slug":"Multi-Data-Center-Consistency","date":"2017-05-19T07:18:33.000Z","updated":"2017-05-19T08:09:06.000Z","comments":true,"path":"2017/05/19/Multi-Data-Center-Consistency/","link":"","permalink":"http://funkygao.github.io/2017/05/19/Multi-Data-Center-Consistency/","excerpt":"","keywords":[],"text":"MDCC提供了跨机房的分布式数据库强一致性模型 Referenceshttp://mdcc.cs.berkeley.edu/","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"asynchronous distributed snapshot","slug":"distributed-snapshot","date":"2017-05-19T02:32:48.000Z","updated":"2017-05-19T09:31:46.000Z","comments":true,"path":"2017/05/19/distributed-snapshot/","link":"","permalink":"http://funkygao.github.io/2017/05/19/distributed-snapshot/","excerpt":"","keywords":[],"text":"如何给分布式系统做个全局逻辑一致的快照?Node State + Channel State 发送规则12345node.recordState()for conn in allConns &#123; // before any conn&apos;s outbound msg conn.send(marker)&#125; 接收规则123456789101112msg = conn.recv()if msg.isMarker() &#123; t1 = now() if !node.stateRecorded() &#123; node.recordState() Channel(conn) = [] &#125; else &#123; Channel(conn) = msgsBetween(now(), t1) // in-flight msgs not applied on state node.state.apply(msgs before the marker) &#125;&#125; Demo 12345678910111213141516171819202122232425a)P为自己做快照P(red, green, blue)在Channel(PQ)上 send(marker)b)P把绿球送给Q，这个消息是在marker后面以此同时，Q把自己的橙色球送给P，此时Q(brown, pink)c) Q在Channel(PQ)上收到marker // Q是接收者Q为自己做快照Q(brown, pink)Channel(PQ) = []// 因为之前Q把自己的橙色球送给了P，因此Q也是发送者在Channel(QP)上 send(marker)d)P收到橙色球，然后是marker由于P已经记录了state, Channel(QP)=[orange, ]最终的分布式系统的snapshot:P(red, green, blue)Channel(PQ) []Q(brown, pink)Channel(QP) = [orange, ] FAQ如何发起发起global distributed snapshot的节点，可以是一台，也可以多台并发 如何结束所有节点上都完成了snapshot 用途故障恢复 与Apache Storm的基于记录的ack不同，Apache Flink的failure recovery采用了改进的Chandy-Lamport算法checkpoint coordinator是JobManager data sources periodically inject markers into the data stream.123val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(4)env.enableCheckpointing(1000) // 数据源每1s发送marker(barrier) Whenever an operator receives such a marker, it checkpoints its internal state.1234567891011121314151617181920212223242526272829303132class StateMachineMapper extends FlatMapFunction[Event, Alert] with Checkpointed[mutable.HashMap[Int, State]] &#123; private[this] val states = new mutable.HashMap[Int, State]() override def flatMap(t: Event, out: Collector[Alert]): Unit = &#123; // get and remove the current state val state = states.remove(t.sourceAddress).getOrElse(InitialState) val nextState = state.transition(t.event) if (nextState == InvalidTransition) &#123; // 报警 out.collect(Alert(t.sourceAddress, state, t.event)) &#125; else if (!nextState.terminal) &#123; // put back to states states.put(t.sourceAddress, nextState) &#125; &#125; override def snapshotState(checkpointId: Long, timestamp: Long): mutable.HashMap[Int, State] = &#123; // barrier(marker) injected from data source and flows with the records as part of the data stream // // snapshotState()与flatMap()一定是串行执行的 // 此时operator已经收到了barrier(marker) // 在本方法返回后，flink会自动把barrier发给我的output streams // 再然后，保存states(默认是JobManager内存，也可以HDFS) states &#125; override def restoreState(state: mutable.HashMap[Int, State]): Unit = &#123; // 出现故障后，flink会停止dataflow，然后重启operator(StateMachineMapper) states ++= state &#125;&#125; Referenceshttp://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdfhttps://arxiv.org/abs/1506.08603https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.htmlhttps://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"https","slug":"https","date":"2017-05-19T00:21:27.000Z","updated":"2017-06-02T00:17:45.000Z","comments":true,"path":"2017/05/19/https/","link":"","permalink":"http://funkygao.github.io/2017/05/19/https/","excerpt":"","keywords":[],"text":"curl https://baidu.comHow the 270ms passed1234567891011121314151617181920212223242526272829303132333435363738394041424344451 1 0.0721 (0.0721) C&gt;S Handshake ClientHello Version 3.1 cipher suites TLS_EMPTY_RENEGOTIATION_INFO_SCSV TLS_DHE_RSA_WITH_AES_256_CBC_SHA TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 TLS_DHE_DSS_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA TLS_RSA_WITH_AES_256_CBC_SHA256 TLS_DHE_RSA_WITH_AES_128_CBC_SHA TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 TLS_DHE_DSS_WITH_AES_128_CBC_SHA TLS_RSA_WITH_RC4_128_SHA TLS_RSA_WITH_RC4_128_MD5 TLS_RSA_WITH_AES_128_CBC_SHA TLS_RSA_WITH_AES_128_CBC_SHA256 TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA TLS_RSA_WITH_3DES_EDE_CBC_SHA compression methods NULL1 2 0.1202 (0.0480) S&gt;C Handshake ServerHello Version 3.1 session_id[32]= b3 ea 99 ee 5a 4c 03 e8 e0 74 95 09 f1 11 09 2a 9d f5 8f 2a 26 7a d3 7f 71 ff dc 39 62 66 b0 f9 cipherSuite TLS_RSA_WITH_AES_128_CBC_SHA compressionMethod NULL1 3 0.1205 (0.0002) S&gt;C Handshake Certificate1 4 0.1205 (0.0000) S&gt;C Handshake ServerHelloDone1 5 0.1244 (0.0039) C&gt;S Handshake ClientKeyExchange1 6 0.1244 (0.0000) C&gt;S ChangeCipherSpec1 7 0.1244 (0.0000) C&gt;S Handshake1 8 0.1737 (0.0492) S&gt;C ChangeCipherSpec1 9 0.1737 (0.0000) S&gt;C Handshake1 10 0.1738 (0.0001) C&gt;S application_data1 11 0.2232 (0.0493) S&gt;C application_data1 12 0.2233 (0.0001) C&gt;S Alert1 0.2234 (0.0000) C&gt;S TCP FIN1 0.2709 (0.0475) S&gt;C TCP FIN","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"hybrid logical clock","slug":"hlc","date":"2017-05-18T23:51:28.000Z","updated":"2017-05-27T00:07:48.000Z","comments":true,"path":"2017/05/19/hlc/","link":"","permalink":"http://funkygao.github.io/2017/05/19/hlc/","excerpt":"","keywords":[],"text":"分布式事务，为了性能，目前通常提供SI/SSI级别的isolation，通过乐观冲突检测而非2PC悲观方式实现，这就要求实现事务的causality，通常都是拿逻辑时钟实现total order例如vector clock就是一种，zab里的zxid也是；google percolator里的total order算是另外一种逻辑时钟，但这种方法由于有明显瓶颈，也增加了一次消息传递 但逻辑时钟无法反应物理时钟，因此有人提出了混合时钟，wall time + logical time，分别是给人看和给机器看，原理比较简单，就是在交互消息时，接收方一定sender event happens before receiver 但wall time本身比较脆弱，例如一个集群，有台机器ntp出现问题，管理员调整时间的时候出现人为错误，本来应该是2017-09-09 10:00:00，结果typo成2071-09-09 10:00:00，后果是它会传染给集群内所有机器，hlc里的wall time都会变成2071年，人工无法修复，除非允许丢弃历史数据，只有等到2071年那一天系统会自动恢复，wall time部分也就失去了意义 要解决这个问题，可以加入epoch1234HLC+-------+-----------+--------------+| epoch | wall time | logical time |+-------+-----------+--------------+ 修复2071问题时，只需把epoch+1","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"可靠性金字塔 SRE","slug":"SRE","date":"2017-05-18T05:55:21.000Z","updated":"2017-10-09T01:34:20.000Z","comments":true,"path":"2017/05/18/SRE/","link":"","permalink":"http://funkygao.github.io/2017/05/18/SRE/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"MySQL B+ Tree","slug":"MySQL-B-Tree","date":"2017-05-18T01:32:32.000Z","updated":"2017-05-18T01:34:07.000Z","comments":true,"path":"2017/05/18/MySQL-B-Tree/","link":"","permalink":"http://funkygao.github.io/2017/05/18/MySQL-B-Tree/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"batch insert(mysql)","slug":"batch-insert","date":"2017-05-18T00:46:17.000Z","updated":"2017-05-18T00:58:45.000Z","comments":true,"path":"2017/05/18/batch-insert/","link":"","permalink":"http://funkygao.github.io/2017/05/18/batch-insert/","excerpt":"","keywords":[],"text":"1234567// case1INSERT INTO T(v) VALUES(1), (2), (3), (4), (5)// case2for i=1; i&lt;=5; i++ &#123; INSERT INTO T(v) VALUES(i);&#125; case1和2有什么影响？假设auto_commit 好处 减少与mysql server的交互 减少SQL解析(如果statement则没区别) query cache打开时，只会invalidate cache一次，提高cache hit 坏处 可能变成一个大事务batch insert的时候，batch不能太大","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"cannot have exactly-once delivery","slug":"cannot-have-exactly-once-delivery","date":"2017-05-17T02:46:26.000Z","updated":"2017-05-18T06:12:44.000Z","comments":true,"path":"2017/05/17/cannot-have-exactly-once-delivery/","link":"","permalink":"http://funkygao.github.io/2017/05/17/cannot-have-exactly-once-delivery/","excerpt":"","keywords":[],"text":"http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"RocketMQ解读","slug":"RocketMQ","date":"2017-05-17T02:38:32.000Z","updated":"2017-06-19T11:48:46.000Z","comments":true,"path":"2017/05/17/RocketMQ/","link":"","permalink":"http://funkygao.github.io/2017/05/17/RocketMQ/","excerpt":"","keywords":[],"text":"Features Producer Group发送事务消息时，作为TC，要多机，保存事务状态表{offset: P/C/R} Broker tag-based message filter 定时消息，不支持任意精度，只是特定level: 5s, 10s, 1m等 queueID=delayLevel-1因此，应该不支持message revoke 区分commit log和consume log，有点类似WAL和table关系可以把它们放在不同FS下，但没有更细粒度的增加了一个分发步骤的好处：可以不分发 Commit Log1234$&#123;rocketmq.home&#125;\\store\\commitlog\\$&#123;fileName&#125;fileName[n] = fileName[n-1] + mappedFileSize为了保证mappedFileSize相同，在每个file tail加padding，默认1GB 每条消息12345678QueueOffset针对普通消息，存的是consume log里的offset；如果事务消息，是事务状态表的offset+---------+-------+-----+---------+------+-------------+----------------+----------------+| MsgSize | Magic | CRC | QueueID | Flag | QueueOffset | PhysicalOffset | SysFlag(P/C/R) |+---------+-------+-----+---------+------+-------------+----------------+----------------++--------------+------------------+-----------+---------------+----+------+-------+------+| ProducedTime | ProduderHostPort | StoreTime | StoreHostPort | .. | Body | Topic | Prop |+--------------+------------------+-----------+---------------+----+------+-------+------+ 每次append commit log，会同步调用dispatch分发到consume queue和索引服务1234567new DispatchRequest(topic, queueId, result.getWroteOffset(), result.getWroteBytes(), tagsCode, msg.getStoreTimestamp(), result.getLogicsOffset(), msg.getKeys(), // Transaction msg.getSysFlag(), msg.getPreparedTransactionOffset()); queue仅仅是逻辑概念，可以通过它来参与producer balance，类似一致哈希里的虚拟节点每台broker上的commitlog被本机所有的queue共享，不做任何区分 1234567broker1: queue0, queue2broker2: queue0, then, topicA has 3 queues:broker1_queue0, broker1_queue2, broker2_queue0producer.selectOneMessageQueue(&quot;topicA&quot;, &quot;broker1&quot;, &quot;queue0&quot;) 消息的局部顺序由producer client保证 Question 如何实现retention by topic: 没有实现仅仅根据commit log file的mtime来判断是否过期，虽然里面混杂多topics 如何I/O balancing 如何压缩 如果CRC出错，那么所有topic都受影响? 为什么要存StoreHostPort？如何迁移topic：无法迁移 写commit log需要加锁，这个锁粒度太大，相当于db level lock，而非table level broker的脑裂问题 failover topic的commit log是分散在所有broker上的 Consume Queue1$&#123;rocketmq.home&#125;/store/consumequeue/$&#123;topicName&#125;/$&#123;queueId&#125;/$&#123;fileName&#125; 读一条消息，先读consume queue(类似mysql的secondary index)，再读commit log(clustered index) 没有采用sendfile，而是通过mmap：因为random read 123+---------------------+-----------------+------------------------+| CommitLogOffset(8B) | MessageSize(4B) | MessageTagHashcode(8B) |+---------------------+-----------------+------------------------+ 虽然消费时，consume queue是顺序的，但接下来的commit log几乎都是random read，此外如何优化压缩？光靠pagecache+readahead是远远不够的 Producer123TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); // from local cache or name serverMessageQueue mq = topicPublishInfo.selectOneMessageQueue(lastBrokerName);sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); Transaction123456789101112131415// 2PC，2 messages// Phase1producer group write redologproducer group send a message(type=TransactionPreparedType) to brokerbroker append it to CommitLog and return MessageIdbroker will not append it to consume queue// Phase2producer group write redologproducer group send a message(type=TransactionCommitType, msgId=$msgId) to brokerbroker find the message with msgId in CommitLog and clone it and append it to CommitLog(type=TransactionCommitType|TransactionRollbackType)if type == TransactionCommitType &#123; broker append commit log offset to consume queue&#125; State Table保存在broker，默认1m扫一次 1234567824B, mmap+-----------------+------+-----------+-----------------------+--------------+| CommitLogOffset | Size | Timestamp | ProducerGroupHashcode | State(P/C/R) |+-----------------+------+-----------+-----------------------+--------------+prepare消息，insert tablecommit/rollback消息，update table 对于未决事务，根据随机向Producer Group里的一台发请求CHECK_TRANSACTION_STATEProducer Group根据redolog(mmap)定位状态Producer Group信息存放在namesvr Problems Producer不再是普通的client，它已经变成server(TC)，而且要求不能随便shutdown Producer Group里写redolog的机器死了怎么办 HA粒度只控制在Broker，而kafka是在partition上","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"architecture design checklist","slug":"architecture-design-checklist","date":"2017-05-17T01:00:00.000Z","updated":"2017-06-02T08:16:55.000Z","comments":true,"path":"2017/05/17/architecture-design-checklist/","link":"","permalink":"http://funkygao.github.io/2017/05/17/architecture-design-checklist/","excerpt":"","keywords":[],"text":"archeck","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"kateway replay messages","slug":"kateway-replay-messages","date":"2017-05-16T23:52:34.000Z","updated":"2017-05-17T00:27:21.000Z","comments":true,"path":"2017/05/17/kateway-replay-messages/","link":"","permalink":"http://funkygao.github.io/2017/05/17/kateway-replay-messages/","excerpt":"","keywords":[],"text":"Issueconsumer有需求回放/快进消息，目前kateway具有该功能：用户在web console上把offset设置到指定位置 但由于机器里kateway正在消费源源不断的消息，checkpoint会overwrite这个指定的offset这就要求用户先关闭消费进程，然后web console上操作，再启动消费进程: not user friendly在不影响性能前提下，对其进行改进 Solution12345678910111213141516171819202122232425_, stat, err := cg.kz.conn.Get(path)if cg.lastVer == -1 &#123; // 第一次commit offset cg.lastVer = stat.Version&#125; else if cg.lastVer != stat.Version &#123; // user manually reset the offset checkpoint return ErrRestartConsumerGroup&#125;// 也可能在Get后，用户恰好操作了“回放”，通过CAS解决这个问题switch err &#123;case zk.ErrNoNode: return cg.kz.create(path, data, false)case nil: newStat, err := cg.kz.conn.Set(path, data, stat.Version) if err != nil &#123; cg.lastVer = newStat.Version &#125; return errdefault: return err&#125;","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"how DBMS works","slug":"how-DBMS-works","date":"2017-05-16T08:31:29.000Z","updated":"2017-06-01T06:09:13.000Z","comments":true,"path":"2017/05/16/how-DBMS-works/","link":"","permalink":"http://funkygao.github.io/2017/05/16/how-DBMS-works/","excerpt":"","keywords":[],"text":"Basics Undo log Oracle和MySQL机制类似 MS SQL Server里，称为transaction log PostgreSQL里没有undo log，它通过mvcc系统表实现，每一行存储多个版本 Redo log Oracle和MySQL机制类似 MS SQL Server里，称为transaction log PostgreSQL里称为WAL Query Optimization大部分是基于Selinger的论文，动态规划算法，把这个问题拆解成3个子问题 cost estimation以I/O和CPU的成本衡量 relational equivalences that define a search space cost-based search Concurrency ControlGray论文 区分细粒度和粗粒度的锁数据库是个分层结构 hierarchical structure 提出了多种隔离级别最初都是2PL实现的serializable isolation Database RecoveryIBM的ARIES算法(1992)，Algorithm for Recovery and Isolation Exploiting SemanticsARIES can only update the data in-place after the log reaches storage确保在恢复时，已经commit的事务要redo，未commit的事务要undoredo log是物理的，undo log是逻辑的 No Force, Steal database need not write dirty pages to disk at commit time由于有redo log，update pages are written to disk lazily after commitNo Force database can flush dirty pages to disk at any time由于有undo log，uncommitted(dirty) pages can be written to disk by the buffer managerSteal ARIES为each page保存LSN，disk page是数据管理和恢复的基本单位，page write是原子的 ARIES crash recovery分成3步 analysis phase从前向后，determine winners &amp; losers redo phase如果是Force(在commit前刷dirty pages)，就不需要redo stage了repeat history undo phase从后向前，undo losers ARIES数据结构 xaction table dirty page table checkpoint Example123456789101112131415161718192021222324252627282930After a crash, we find the following log:0 BEGIN CHECKPOINT5 END CHECKPOINT (EMPTY XACT TABLE AND DPT)10 T1: UPDATE P1 (OLD: YYY NEW: ZZZ)15 T1: UPDATE P2 (OLD: WWW NEW: XXX)20 T2: UPDATE P3 (OLD: UUU NEW: VVV)25 T1: COMMIT30 T2: UPDATE P1 (OLD: ZZZ NEW: TTT)Analysis phase:Scan forward through the log starting at LSN 0.LSN 5: Initialize XACT table and DPT to empty.LSN 10: Add (T1, LSN 10) to XACT table. Add (P1, LSN 10) to DPT.LSN 15: Set LastLSN=15 for T1 in XACT table. Add (P2, LSN 15) to DPT.LSN 20: Add (T2, LSN 20) to XACT table. Add (P3, LSN 20) to DPT.LSN 25: Change T1 status to &quot;Commit&quot; in XACT tableLSN 30: Set LastLSN=30 for T2 in XACT table.Redo phase:Scan forward through the log starting at LSN 10.LSN 10: Read page P1, check PageLSN stored in the page. If PageLSN&lt;10, redo LSN 10 (set value to ZZZ) and set the page&apos;s PageLSN=10.LSN 15: Read page P2, check PageLSN stored in the page. If PageLSN&lt;15, redo LSN 15 (set value to XXX) and set the page&apos;s PageLSN=15.LSN 20: Read page P3, check PageLSN stored in the page. If PageLSN&lt;20, redo LSN 20 (set value to VVV) and set the page&apos;s PageLSN=20.LSN 30: Read page P1 if it has been flushed, check PageLSN stored in the page. It will be 10. Redo LSN 30 (set value to TTT) and set the page&apos;s PageLSN=30.Undo phase:T2 must be undone. Put LSN 30 in ToUndo.Write Abort record to log for T2LSN 30: Undo LSN 30 - write a CLR for P1 with &quot;set P1=ZZZ&quot; and undonextLSN=20. Write ZZZ into P1. Put LSN 20 in ToUndo.LSN 20: Undo LSN 20 - write a CLR for P3 with &quot;set P3=UUU&quot; and undonextLSN=NULL. Write UUU into P3. ARIES是为传统硬盘设计的，顺序写，但成本也明显：修改1B，需要redo 1B+undo 1B+page 1B=3Bwhat if in-place update with SSD? 分布式mid-1970s 2PC 一票否决 Referenceshttps://blog.acolyer.org/2016/01/08/aries/http://cseweb.ucsd.edu/~swanson/papers/SOSP2013-MARS.pdfhttps://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"scalability papers","slug":"scalability-papers","date":"2017-05-16T08:28:29.000Z","updated":"2017-06-02T08:17:57.000Z","comments":true,"path":"2017/05/16/scalability-papers/","link":"","permalink":"http://funkygao.github.io/2017/05/16/scalability-papers/","excerpt":"","keywords":[],"text":"http://www.perfdynamics.com/Manifesto/USLscalability.html","categories":[],"tags":[{"name":"architecture","slug":"architecture","permalink":"http://funkygao.github.io/tags/architecture/"}]},{"title":"PostgreSQL MVCC","slug":"PostgreSQL-MVCC","date":"2017-05-16T03:54:13.000Z","updated":"2017-05-16T08:23:45.000Z","comments":true,"path":"2017/05/16/PostgreSQL-MVCC/","link":"","permalink":"http://funkygao.github.io/2017/05/16/PostgreSQL-MVCC/","excerpt":"","keywords":[],"text":"InternalsMySQL通过undo log记录uncommitted changes，与此不同，PostgreSQL store all row versions in table data structure. 每个row有2个隐藏字段 Tmin insert时的trx id Tmax delete时的trx id INSERT DELETE DELETE操作并不会马上物理删除，而是VACUUM进程调度进行purge UPDATE","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"VNC protocol","slug":"VNC-protocol","date":"2017-05-16T02:32:46.000Z","updated":"2017-05-17T00:51:09.000Z","comments":true,"path":"2017/05/16/VNC-protocol/","link":"","permalink":"http://funkygao.github.io/2017/05/16/VNC-protocol/","excerpt":"","keywords":[],"text":"WebRTCWebRTC提供了direct data and media stream transfer between two browsers without external server involved: P2P 浏览器上点击“Screen share”按钮后 12345678// sender利用OS的API获取screenshot，并以一定的FPS来进行发送// 优化：把屏幕分成chunk，在把timer之间有变化的chunk生成frame发送时，frame被编码成H.264或VP8通过HTTPS发送// receiver对接收到的frame解码并显示 通过WebRTC实现的是只读的屏幕分享，receiver不能控制sender屏幕 实现123456789101112131415161718192021222324252627&lt;body&gt; &lt;p&gt;&lt;input type=&quot;button&quot; id=&quot;share&quot; value=&quot;Screen share&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;video id=&quot;video&quot; autoplay /&gt;&lt;/p&gt;&lt;/body&gt;&lt;script&gt;navigator.getUserMedia = navigator.webkitGetUserMedia || navigator.getUserMedia;$(&apos;#share&apos;).click(function() &#123; navigator.getUserMedia(&#123; audio: false , video: &#123; mandatory: &#123; chromeMediaSource: &apos;screen&apos; , maxWidth: 1280 , maxHeight: 720 &#125; , optional: [ ] &#125; &#125;, function(stream) &#123; // we&apos;ve got media stream // so the received stream can be transmitted via WebRTC the same way as web camera and easily played in &lt;video&gt; component on the other side document.getElementById(&apos;video&apos;).src = window.URL.createObjectURL(stream); &#125; , function() &#123; alert(&apos;Error. Try in latest Chrome with Screen sharing enabled in about:flags.&apos;); &#125;)&#125;)&lt;/script&gt; VNCRemote Frame Buffer，支持X11, Windows, Mac远程终端用户使用机器（比如显示器、键盘、鼠标）的叫做客户端，提供帧缓存变化的被称为服务器 显示协议pixel(x, y) =&gt; 像素数据编码 C-&gt;S消息类型SetEncodingsRaw, CopyRect, RRE, Hextile, TRLE, ZRLE FramebufferUpdateRequest最重要的显示消息，表示client要server传回哪些区域的图像 1234client.send(messageType, incremental, x, y, width, height) =&gt; server// incremental&gt;0，表示该区域内容变化了才发给client；没有变化，就不用发server.reply(messageType, rectangleN, [&#123;x, y, with, height, color&#125;, ...]) =&gt; client KeyEventclient端的键盘动作 PointerEventclient端的鼠标动作 vnc browserhttp://guacamole.incubator.apache.org/https://github.com/novnc/noVNC Referenceshttp://www.tuicool.com/articles/Rzqumuhttps://github.com/macton/htermchrome://flags/#enable-usermedia-screen-capture","categories":[],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://funkygao.github.io/tags/protocol/"}]},{"title":"LSM for SSD","slug":"LSM","date":"2017-05-16T01:33:59.000Z","updated":"2017-06-01T11:37:56.000Z","comments":true,"path":"2017/05/16/LSM/","link":"","permalink":"http://funkygao.github.io/2017/05/16/LSM/","excerpt":"","keywords":[],"text":"Basics (immutable)memtable: sorted skiplist SSTable: sorted string table L0里的SSTable们，key可能会overlap，因为他们是直接从immutable memtable刷出来的 SSTable file123+-------------------+-------------------------+------------+| index block(16KB) | bloom-filter block(4KB) | data block |+-------------------+-------------------------+------------+ Get(key)12345678910111213locate key in memtableif found then returnlocate key in immutable memtableif found then returnfor level:=0; level&lt;=6; level++ &#123; // 对于level0，需要遍历所有SSTable，因为keys overlap // 但leveldb为了解决这个问题，除了Bloomfilter，也限制了L0的文件数量，一旦超过8，就compact(L0-&gt;L1) // 对于其他level，由于已经sorted，可以直接定位SSTable // // 最坏情况下，Get(key)需要读8个L0以及L1-L6，共14个文件 locate key in SSTable in $level if found then return&#125; SSD主流SSD，例如Samsung 960 Pro，可以提供440K/s random readwith block size=4KB LSM是为传统硬盘设计的，在SSD下，可以做优化，不必过分担心随机读 优化LSM-Tree的主要成本都在compaction(merge sort)，造成IO放大(50倍) 读很多文件到内存 排序 再写回到磁盘 要优化compaction，可以把LSM Tree变小，RocksDB是通过压缩实现的 在SSD下，可以考虑把key和value分离，在LSM Tree里只保存sorted key和pointer(value)，value直接保存在WAL里 key: 16Bpointer(value): 16B 2M个k/v，需要64MB2B个k/v，需要64GB Referencehttps://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf","categories":[],"tags":[{"name":"storage","slug":"storage","permalink":"http://funkygao.github.io/tags/storage/"}]},{"title":"InnoDB MVCC","slug":"InnoDB-MVCC","date":"2017-05-16T00:17:17.000Z","updated":"2017-07-13T06:24:37.000Z","comments":true,"path":"2017/05/16/InnoDB-MVCC/","link":"","permalink":"http://funkygao.github.io/2017/05/16/InnoDB-MVCC/","excerpt":"","keywords":[],"text":"BasicInnoDB中通过Undo log实现了txn rollback和MVCC，而并发控制(isolation)通过锁来实现Undo log分为insert undo和update undo(delete是一种特殊的update)，回滚时 insert只要把insert undo log丢弃即可 update需要通过DB_ROLL_PTR DB_TRX_ID找到事务修改前的版本并恢复 与redo log不同的是，磁盘上不存在单独的undo log文件，所有的undo log均存放在主ibd数据文件中（表空间），即使客户端设置了每表一个数据文件也是如此 内部存储InnoDB为每行row都reserved了隐藏字段(system column) DB_ROW_ID DB_TRX_ID DB_ROLL_PTR 12345typedef ib_uint64_t ib_id_t;typedef ib_id_t row_id_t;typedef ib_id_t trx_id_t;typedef ib_id_t roll_ptr_t; Undo log实现方式 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） Demo Innodb中存在purge线程，它会查询那些比现在最老的活动事务还早的undo log，并删除它们 Issues回滚成本当事务正常提交时Innbod只需要更改事务状态为COMMIT即可，不需做其他额外的工作而Rollback如果事务影响的行非常多，回滚则可能成本很高 write skewInnoDB通过Undo log实现的MVCC在修改单行记录是没有问题的，但多行时就可能出问题1234begin;update table set col1=2 where id=1; // 成功，创建了undo logupdate table set col2=3 where id=2; // 失败rollback; 回滚row(id=1)时，由于它没有被lock，此时可能已经被另外一个txn给修改了，这个回滚会破坏已经commit的事务 如果要解决这个问题，需要应用层进行控制 另外一个例子123456国家规定一个家庭最多养3只宠物(constraint)，Alice和Bob是一家人，他们现在有dog(1)+cat(1)如果并发地，Alice再买一只猫，而Bob再买一只狗，这2个事务就会write skew因为repeatable read：一个事务提交之后才visible by other txnTxnAlice TxnBobcat=cat+1 dog=dog+1这2个事务都成功了，但却破坏了constraint 锁RR隔离级别下，普通select不加锁，使用MVCC进行一致性读取，即snapshot readupdate, insert, delete, select … for update, select … lock in share mode都会进行加锁，并且读取的是当前版本: READ COMMITTED读除了lock in share mode是S锁，其他都是X锁 Referenceshttps://dev.mysql.com/doc/refman/5.7/en/innodb-locks-set.htmlhttps://blog.jcole.us/innodb/http://jimgray.azurewebsites.net/WICS_99_TP/","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"microservice transaction","slug":"microservice-transaction","date":"2017-05-15T23:02:29.000Z","updated":"2017-05-16T02:27:33.000Z","comments":true,"path":"2017/05/16/microservice-transaction/","link":"","permalink":"http://funkygao.github.io/2017/05/16/microservice-transaction/","excerpt":"","keywords":[],"text":"Referenceshttps://docs.microsoft.com/en-us/azure/architecture/patterns/compensating-transaction","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"zookeeper internals","slug":"zookeeper","date":"2017-05-15T02:26:14.000Z","updated":"2017-07-04T05:13:22.000Z","comments":true,"path":"2017/05/15/zookeeper/","link":"","permalink":"http://funkygao.github.io/2017/05/15/zookeeper/","excerpt":"","keywords":[],"text":"BasicsModelFile api without partial R/WNo rename operation zab通过TCP+zxid实现事务的totally ordersequential consistency一致性模型，保证the real execution looks to clients like some sequential execution in which the operations of every client appear in the order they were submitted ImplementationZKDatabase1234567// zk的内存数据库class ZKDatabase &#123; DataTree dataTree LinkedList&lt;Proposal&gt; committedLog FileTxnSnapLog snapLog ConcurrentHashMap&lt;Long, Integer&gt; sessionsWithTimeouts // &#123;sessionID: timeout&#125;&#125; DataTree12345678910111213141516171819202122class DataTree &#123; ConcurrentHashMap&lt;String, DataNode&gt; nodes // &#123;path: znode&#125;, flat ConcurrentHashMap&lt;Long, HashSet&lt;String&gt;&gt; ephemerals // &#123;sessionID: [path, ]&#125; WatchManager dataWatches, childWatches func createNode(path, data) &#123; parent = nodes.get(parentPath) parent.Lock() // check NodeExistsException // set stat of the new znode and its parent child = new DataNode(parent, data, stat) // the new znode parent.addChild(child) nodes.put(path, child) if ephemeralOwner != nil &#123; ephemerals.get(ephemeralOwner).add(path) &#125; parent.Unlock() dataWatches.triggerWatch(path, NodeCreated) childWatches.triggerWatch(parentPath, NodeChildrenChanged) &#125;&#125; DataNode1234567class DataNode &#123; DataNode parent Set&lt;String&gt; children StatPersisted stat []byte data&#125; n(n-1)/2 conns只允许id比较大的server发起主动连接：由于任意server在启动时都会主动向其他server发起连接，如果这样，任意两台server之间就拥有两条连接，这明显是没有必要的123456789======= ======= ======= ======= ======= ========sid 1 2 3 4 5======= ======= ======= ======= ======= ========1 &lt;&gt; &lt; &lt; &lt; &lt;2 &lt;&gt; &lt; &lt; &lt;3 &lt;&gt; &lt; &lt;4 &lt;&gt; &lt;5 &lt;&gt;======= ======= ======= ======= ======= ======== 成为 leader 的条件 选epoch最大的 epoch相等，选 zxid 最大的 epoch和zxid都相等，选择server id最大的 1234(newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId)))) 何时选举进入LOOKING状态 刚启动时 稳定运行中，任何的异常都会让本机进入LOOKING态123catch (Exception e) &#123; setPeerState(LOOKING)&#125; 2PC是个简化的2PC，因为不存在abort/rollback，只有commit 传统2PC里coordinator crash的硬伤，zk是怎么解决的？重新选举，recover from txnlog 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class LearnerHandler extends Thread &#123; queuedPackets = new LinkedBlockingQueue&lt;QuorumPacket&gt;()&#125;class Leader &#123; func processAck(long zxid) &#123; Proposal p = outstandingProposals.get(zxid) p.ackSet.add(1) if p.ackSet.verifyQuorum() &#123; // 大多数返回ack了 outstandingProposals.remove(zxid) commit(zxid) &#123; // 通知all followers QuorumPacket qp = new QuorumPacket(Leader.COMMIT, zxid, null, null) sendPacket(qp) // 异步 &#125; inform(p) &#123; // 通知all observers &#125; &#125; &#125; func propose(req Request) &#123; Proposal p = new Proposal(req) outstandingProposals.put(zxid, p) sendPacket(p) &#123; for LearnerHandler f = range followers &#123; f.queuePacket(p) &#125; &#125; &#125;&#125;class Follower &#123; func processPacket(QuorumPacket qp) &#123; switch qp.type &#123; case Leader.PROPOSAL: // proposal只是记录txnlog，不改变database FollowerZooKeeperServer.logRequest() &#123; pendingTxns.add(req) txnLog.append(req) &#125; case Leader.COMMIT: FollowerZooKeeperServer.commit(qp.zxid) &#123; commitProcessor.commit(pendingTxns.remove()) &#123; committedRequests.add(req) &#123; FinalRequestProcessor.processRequest(req) &#123; ZooKeeperServer.processTxn -&gt; ZKDatabase().processTxn 根据req.type来创建response，并发送 &#125; &#125; &#125; &#125; &#125; &#125;&#125; ConstraintsMany ZooKeeper write requests are conditional in nature: a znode can only be deleted if it does not have any children a znode can be created with a name and a sequence number appended to it a change to data will only be applied if it is at an expected version Quorum1234567891011121314func isQuorum(type) &#123; // zk的请求有2种 1. 事务请求 2. 只读请求 switch (type) &#123; case OpCode.exists, getACL, getChildren, getChildren2, getData: // 本地执行，不需要proposal return false case OpCode.error, closeSession, create, createSession, delete, setACL, setData, check, multi: return true default: return false &#125;&#125; 注意：session部分，也会走txn multi是原子操作，multi里的每个op都使用相同的zxid WatchWatches are maintained locally at the ZooKeeper server to which the client is connected.它是不走proposal quorum的 Watcher只会告诉客户端发生了什么类型的事件，而不会说明事件的具体内容例如，NodeDataChanged，watcher只会通知client：在你watch的path上，发生了NodeDataChanged这个事件但最新的数据是什么，不在event里，而需要client主动重新去get Watch的通知，由WatchManager完成，它先从内存里删除这个watcher，然后回调watcher.process后者在NIOServerCnxn，即watches are sent asynchronously to watchers(client).But ZooKeeper guarantees that a client will see a watch event for a znode it is watching before seeing the new data that corresponds to that znode. 1234567891011121314151617181920212223class WatchManager &#123; HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable func triggerWatch(path, event) &#123; synchronized(this) &#123; watchers = watchTable.remove(path) // so one time trigger &#125; // Watch机制本身是非常轻量级的，对服务器不会有多大开销： // 它都是local zk server在内存中处理 // 但如果一个path的watcher很多，那么这个O(n)循环 for watcher = range watchers &#123; w.process(event) &#125; &#125;&#125;func process(WatchedEvent event) &#123; h = new ReplyHeader(-1, -1L, 0) sendResponse(h, event) // if IOException, close the client conn // sock.write(非阻塞) async write // sendResponse对同一个client是串行的，顺序的&#125; 顺序123456client.get(path, watch=true)// 此时数据发生变化zk保证的顺序：client先拿到watch event，之后才能看到最新的数据client如果watch很多时间，那么得到这些event的顺序与server端发生的顺序是完全一致的 watch的保持zk client连接zk1, 并get with watch，突然zk1 crash，client连接到zk2，那么watch是如何保持的?这是client端实现的，内存里记录watches，在pick new server后，sendSetWatches watch的事件会丢吗client2能获取到每次的set事件吗?12client1不停地set(node, newValue)client2 get with watch 不一定：因为是one time trigger获取event后，要重新watch，在此过程中可能产生新的事件: 期间事件lost 此外，zk与client的conn断开后，client会连接下一个zk，在此期间的事件lost例如，watch NodeCreated事件，在client重新连接期间，该node created，那么client将永远无法获取该事件 watch同一个znode多次，会收到几条event?由于WatchManager的实现，相同类型的watch在一个path上被set多次，只会触发一次 12create(&quot;/foo&quot;, EPHEMERAL_SEQUENTIAL)exists(&quot;/foo&quot;, watcher) // 那么这个watch事件是永远不会trigger的，因为path不同，见WatchManager的实现 ClientrecvLoop里任意的错误，都会pick next server and authentication，进入新的循环 conn reset by peer conn EOF receive packet timeout session expire time conn.recvTimeout = sessionTimeout * 2 / 3 ping interval = sessionTimeout / 3 例如，sessionTimeout=30s，那么client在等待20s还得不到response，就会try next server恰好赶在ping的窗口期 10+20=30 Q/Aclient ping是如何保持住session的?client连接s1，定期ping，但s1 crash后client连接s2，为什么session能保持住?123456connect(s2)send(ConnectRequest&#123;LastZxidSeen, SessionID&#125;) // SessionID是s1当时分配的var r ConnectResponse = recv()if r.SessionID == 0 &#123; // session expire&#125; createSession会通过txn，因此client可以failoverserver在sessionTimeout内没有收到ping，就会closeSession，它也通过txn session idzk session id assigned by server, global unique 123456func initializeNextSession(id=1) &#123; long nextSid = 0; nextSid = (System.currentTimeMillis() &lt;&lt; 24) &gt;&gt;&gt; 8; nextSid = nextSid | (id &lt;&lt;56); return nextSid;&#125; 后面的session id就是这个种子基础上 increment by 1 SnapshotdataLogDir(txn log) and dataDir(snapshot) should be placed in 2 disk devices如果txn log和snapshot处于同一块硬盘，异步的snapshot可能会block txn log，连锁反应就是把proposal阻塞，进而造成follower重新选举 when System.getProperty(“zookeeper.snapCount”), 默认值100,000 takeSnapshot的时间在50,000 ~ 100,0000 之间的随机值 txn数量超过snapCount+随机数 roll txn log 创建一个线程，异步执行takeSnapshot。但前面的takeSnapshot线程未完成，则放弃Too busy to snap, skipping 1234567891011121314151617181920212223Request si = getRequest()if (zks.getZKDatabase().append(si)) &#123; // txn log ok logCount++; // logCount就是txn的数量 if (logCount &gt; (snapCount / 2 + randRoll)) &#123; randRoll = r.nextInt(snapCount/2); // 为了防止集群内所有节点同时takeSnapshot加入随机 zks.getZKDatabase().rollLog(); // txn log will roll if (snapInProcess != null &amp;&amp; snapInProcess.isAlive()) &#123; LOG.warn(&quot;Too busy to snap, skipping&quot;); &#125; else &#123; snapInProcess = new Thread(&quot;Snapshot Thread&quot;) &#123; public void run() &#123; try &#123; zks.takeSnapshot(); &#125; catch(Exception e) &#123; LOG.warn(&quot;Unexpected exception&quot;, e); &#125; &#125; &#125;; snapInProcess.start(); &#125; logCount = 0; &#125;&#125; size每个znode meta data至少76+path+data，如果1M znodes，平均size(path+data)=100，那么snapshot文件长度至少200MB我的一个生产环境zk，znode 3万，snapshot文件15MB；即，如果300万个znodes，那么snapshot文件将是1.5GB 123456path(len4, path)node data(len4, data) acl8 meta60 czxid8, mzxid8, ctime8, mtime8, version4, cversion4, aversion4, ephemeralOwner8, pzxid8 checksumAdler32 Edge casesleader electionLOOKING后，把自己的zxid广播，是得到大多数同意就成为leader?是，不需要等全部ack async commit[S1(leader), S2, S3]S1 Propose set(a)=5，在得到majority Ack(proposal)后，向所有机器发送Commit，Q1. S1需要在得到majority Ack(commit)后才return OK to client?Q2. 如果S1发送Commit给所有机器前恰好挂了，new leader会恢复这个事务吗？ leader在得到majority Ack(proposal)后，majority servers已经记录下了txnlog，leader发送Commit只是为了让serversmake the txn visibile to client，Commit消息是不会记录txnlog的leader处理Commit是异步的，不需要等待Commit的ack，即Q1: no，Q2: yes ZAB makes the guarantee that a proposal which has been logged by a quorum of followers will eventually be committedany uncommited proposals from a previous epoch seen by a new leader will be committed by that leader before it becomes active. 换个角度看这个问题：S1得到请求set(a)=5，commit locally，但commit还没有发送给S2，S3，crash!这时候，一个client可能发请求 get(a)，如果它连接的是S1，在S1 crash前，get(a)=2所以，这个commit必须让新leader知道 sync proposal[S1(leader), S2, S3, S4, S5]S1 Propose set(a)=2，发送给了S2 Proposal，但S3-5还没有收到Proposal，此时S1 crash，那么这个proposal在new leader上会被恢复吗? 即重新选举后，get(a)=2? 不一定！ 1234567S1(leader), S2, S3, S4, S5现在propose set(a)=b，S1确认了，但其他还没有确认，此时全部crash然后启动S2-S5，等一会儿再启动S1，那么S2-S5他们的txid相同，会选出S5 leader等S1启动时，它的txid是最大的，a=b可能会丢：如果S1启动慢了200ms内，可能不会丢；否则，例如慢了1分钟，则丢了，S1变成follower后会把该txid truncate: txnlog seek FastLeaderElection.java123456789101112131415161718192021222324252627282930finalizeWait=200ms // 在得到majority确认后，但还没有得到全部确认，wait before make leadersendNotifications()for i.am.looking &#123; n = recvqueue.poll() switch n.state &#123; case LOOKING: compare my proposal with n and update my proposal if every node agrees &#123; // got the leader! return &#125; if quorum agrees &#123; // Verify if there is any change in the proposed leader for &#123; n = recvqueue.poll(200ms) if n == nil &#123; break &#125; &#125; &#125; case FOLLOWING, LEADING: if leader is not me &#123; // confirm I have recv notification from leader // stating that he is leader &#125; &#125;&#125; Referenceshttps://issues.apache.org/jira/browse/ZOOKEEPER-1813https://issues.apache.org/jira/browse/ZOOKEEPER-417https://issues.apache.org/jira/browse/ZOOKEEPER-1674https://issues.apache.org/jira/browse/ZOOKEEPER-1642http://blog.csdn.net/pwlazy/article/details/8080626","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"delay and schedule message delivery","slug":"delay-and-schedule-message-delivery","date":"2017-05-15T00:11:24.000Z","updated":"2017-05-15T00:26:43.000Z","comments":true,"path":"2017/05/15/delay-and-schedule-message-delivery/","link":"","permalink":"http://funkygao.github.io/2017/05/15/delay-and-schedule-message-delivery/","excerpt":"","keywords":[],"text":"使用场景 业务需要 通过它可以实现XA的prepare/commit/rollback，从而实现与其他系统的原子提交 实现kateway通过mysql作为WAL，并通过background worker(actor)来实现调度/commit/rollback 优先队列以message due time作为优先级进行存储，配合workermessage rollback可以通过发送一个tombstone message实现但由于worker的async，无法在rollback时判断是否真正rollback成功：一条消息要5分钟后发送，在5分钟到达时，client可能恰好要取消，这时候，rollback与worker之间存在race condition，需要正常处理这个一致性：要么，取消失败，消息被发出要么，取消成功，消息不发出不能，取消成功，消息被发出1234567891011// workerfor &#123; if msg := peek(queue); msg.due() &#123; msg = pop(queue) if msg.isTombstone() &#123; // the msg is cancelled &#125; else &#123; publish(msg) &#125; &#125;&#125;","categories":[],"tags":[{"name":"PubSub","slug":"PubSub","permalink":"http://funkygao.github.io/tags/PubSub/"}]},{"title":"mysql repeatable read write skew","slug":"repeatable-read-write-skew","date":"2017-05-12T01:18:24.000Z","updated":"2017-05-15T09:25:14.000Z","comments":true,"path":"2017/05/12/repeatable-read-write-skew/","link":"","permalink":"http://funkygao.github.io/2017/05/12/repeatable-read-write-skew/","excerpt":"","keywords":[],"text":"Isolation教科书里的4种isolation read uncommitted: 即dirty read，可能读到其他rollback的数据 read committed: 即non-repeatable read，同一个txn内读一条数据多次，结果可能不同 repeatable read: 一个txn内读一条数据多次结果相同，但不保证读多个数据的时候也相同(phantom read) serialized 但它只是从锁的实现来描述的，不适用于MVCC。各个数据库产品，虽然采用了这些isolation名字，但语义各不相同，很多与教科书里的定义不符 MySQL不会出现phantom read。MySQL里的RR其实是Snapshot Isolation，只有Serialized是完全基于锁 PostgreSQL实际上只有2个隔离级别：Read Committed和Serialized而Serialized是基于MVCC的无锁实现，即Serialized Snapshot MVCC Snapshot存在write skew问题 甲在一个银行有两张信用卡，分别是A和B。银行给这两张卡总的信用额度是2000，即A透支的额度和B透支的额度相加必须不大于2000：A+B&lt;=2000。 A账号扣款1234567begin;a = select credit from ab = select credit from bif (a + b) + amount &lt;= 2000 &#123; update a set credit = credit + amount&#125;commit B账号扣款1234567begin;a = select credit from ab = select credit from bif (a + b) + amount &lt;= 2000 &#123; update b set credit = credit + amount&#125;commit 假设现在credit(a)=1000, credit(b)=500, 1500&lt;=2000甲同时用a账号消费400，b账号消费300在mysql RR下，2个事务都成功，但2个事务结束后credit(a)=1400, credit(b)=700, 2100&gt;2000 如果是serialized隔离级别，则没有问题：一个事务会失败 在mysql RR下，可以通过应用层加约束来避免write skew 结论for mysql不能期望加了一个事务就万事大吉，而要了解每种隔离级别的语义。 涉及单行数据事务的话，只要 Read Committed ＋ 乐观锁就足够保证不丢写 涉及多行数据的事务的话，Serializable 隔离环境的话不会出错，但是你不会开 如果开 Repeatable Read （Snapshot）隔离级别，那么可能会因为 Write Skew 而丢掉写 如果是金融业务，尽量不要用MySQL","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"dual write conflict","slug":"dual-write-conflict","date":"2017-05-11T23:58:30.000Z","updated":"2017-06-14T07:27:18.000Z","comments":true,"path":"2017/05/12/dual-write-conflict/","link":"","permalink":"http://funkygao.github.io/2017/05/12/dual-write-conflict/","excerpt":"","keywords":[],"text":"Issues race condition partial failure","categories":[],"tags":[{"name":"一致性","slug":"一致性","permalink":"http://funkygao.github.io/tags/一致性/"}]},{"title":"shard scales","slug":"shard-scales","date":"2017-05-11T23:58:16.000Z","updated":"2017-06-02T00:16:47.000Z","comments":true,"path":"2017/05/12/shard-scales/","link":"","permalink":"http://funkygao.github.io/2017/05/12/shard-scales/","excerpt":"","keywords":[],"text":"","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://funkygao.github.io/tags/algorithm/"}]},{"title":"mysql group replication","slug":"mysql-group-replication","date":"2017-05-11T08:50:06.000Z","updated":"2017-05-15T02:12:36.000Z","comments":true,"path":"2017/05/11/mysql-group-replication/","link":"","permalink":"http://funkygao.github.io/2017/05/11/mysql-group-replication/","excerpt":"","keywords":[],"text":"简介GR是个mysql插件，通过原子广播协议、乐观事务冲突检测实现了高可用的多master集群每个master都有全量数据，client side load balance write workload或者使用ProxySQL读事务都是本地执行的有2种模式 单主，自动选主 多主，active active master 与PXC是完全的竞争产品 Requirements and Limitations InnoDB engine only, rollback uncommitted changes turn on binlog RBR GTID enabled each table MUST have a primary key或者not null unique key no concurrent DDL 至少3台master，至多9台，不需要slave auto_increment字段通过offset把各个master隔离开，避免冲突 cascading foreign key not supported 只是校验write set，serializable isolation NOT supported 存在stale read问题，如果write/read不在一台member savepoints可能有问题 Performancehttp://mysqlhighavailability.com/an-overview-of-the-group-replication-performance/ 80% throughput of a standalone MySQL server Internals XCOMeXtended COMmunications，一个Paxos系统 确保消息在所有member上相同顺序分发 动态成员，成员失效检测 理论基础 Database State Machine 事务的Update操作都在一个成员上执行，在Commit时把write-set以total order发送消息给每个成员；每个成员上的certification进程检查事务冲突(first commit wins)，完成最终提交或回滚 Commit时的Paxos有2个作用 certification，检测事务冲突 propagate Group Replication ensures that a transaction only commits after a majority of the members in a group have received itand agreed on the relative order between all transactions that were sent concurrently. 与multi-paxos不同，XCOM是multi-leader/multi-proposer：每个member都是leader of its own slots Certificationgroup_replication_group_name就是GTID里的UUIDGTID就是database versionmysql&gt; select @@global.gtid_executed transaction write set: [{updated_row_pk: GTID_EXECUTED}, {updated_row_pk: GTID_EXECUTED}, …] GTID是由certification模块负责的，由它来负责GTID GNO的increment所有member会定期交换GTID_EXECUTED，所有member已经committed事务的交集：Stable Set. Transaction Distributed Recovery向group增加新成员的过程: 获取missing data，同时cache正在发生的新事务，最后catch up 123456789101112131415161718192021222324252627282930// 从现有member里通过mysql backup工具(mysqldump等)搞个backup instance// phase 0: joinJoiner.join(), 通过total order broadcast发给每个member生成view change binlog event: $viewIDgroup里每个member(包括Joiner)都会收到该view event每个online member会把该binlog event排队到现有transaction queue里// phase 1: row copyJoiner pick a live member from the group as Donor // Donor可能会有lagDoner transmits all data up to the joining moment: master/slave connectionfor &#123; if binlog.event.view_id == $viewID &#123; Joiner.SQLThread.stop() break &#125; if Doner.dies &#123; reselect Donner goto restart &#125;&#125;// phase 2: catch upjoining moment后发生的binlogDonor发给Joiner，Joiner applycatch up同步完成后，declare Joiner online，开始对外服务// Joiner.leave()类似的过程// crash过程会被detector发现，自动执行Joiner.leave() 这个过程与mysql在线alter table设计原理类似 binlog view change markersgroup里member变化，会产生一种新的binlog event: view change log event.view id就是一种logicl clock，在member变化时inrement123+-----------------+| epoch | counter |+-----------------+ epoch在第一个加入group的member生成，作用是为了解决all members crash问题: avoid dup counter certification based replication通过group communication和total order transaction实现synchronous replication 事务在单节点乐观运行，在commit时，通过广播和冲突检测实现全局数据一致性它需要 transactional database来rollback uncommitted changes primary keys to generate broadcast write-set atomic changes global ordering replication events Config123456789101112131415[mysqld]log-binbinlog-format=rowbinlog-checksum=NONEgtid-mode=ONenforce-gtid-consistencylog-slave-updatesmaster-info-repository=TABLErelay-log-info-repository=TABLEtransaction-write-set-extraction=MURMUR32// GRgroup_replication_group_name=&quot;da7bad5b-daed-da7a-ba44-da7aba5e7ab&quot;group_replication_local_address=&quot;host2:24901&quot;group_replication_group_seeds=&quot;host1:24901,host2:24901,host3:24901&quot; FAQGR是同步还是异步？replication分为5步12345master locally applymaster generate binlog eventmaster sending the event to slave(s)slave IO thread add event to relay logslave SQL thread apply the event from relay log GR下，只有3是同步的: 把write set广播并得到majority certify confirm广播时发送消息是同步的，但apply write set还是异步的:12345678910member1: DELETE FROM a; // a table with many rowsmember1: 产生一个非常大的binlog eventmember1: group communicate the binlog event to all members(包括它自己)其他member确认ok，那么member1就返回ok给clientclient访问member1，那么数据是一致的但其他member在异步apply binlog event，可能花很长时间，这时候client访问member2，可能不一致：delete的数据仍然能读出来 Referenceshttp://lefred.be/content/mysql-group-replication-about-ack-from-majority/http://lefred.be/content/mysql-group-replication-synchronous-or-asynchronous-replication/http://lefred.be/content/galera-replication-demystified-how-does-it-work/http://www.tocker.ca/2014/12/30/an-easy-way-to-describe-mysqls-binary-log-group-commit.htmlhttp://mysqlhighavailability.com/tag/mysql-group-replication/http://mysqlhighavailability.com/mysql-group-replication-transaction-life-cycle-explained/","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"mysql在线alter table设计","slug":"osc","date":"2017-05-11T07:42:46.000Z","updated":"2017-05-31T03:05:36.000Z","comments":true,"path":"2017/05/11/osc/","link":"","permalink":"http://funkygao.github.io/2017/05/11/osc/","excerpt":"","keywords":[],"text":"主要逻辑1234567891011121314151617181920212223// 合法性检查// 包括：是否有外键、用户权限、表是否合法、是否有唯一键等// 创建变更记录表CREATE /* changelog table */ TABLE _tbs_c// 创建影子表CREATE /* shadow table */ TABLE _tbl_s LIKE tbl// 在影子表上应用alter语句ALTER TABLE _tbl_s STATEMENT// 开始行拷贝线程 tbl -&gt; _tbl_s// 开始binlog接收和应用线程 binlog -&gt; _tbl_s// 等待行拷贝线程完成// 通知binlog线程收工// 等待binlog线程结束// 开始切换LOCK TABLES tbl WRITERENAME TABLE tbl TO _tbl_old, _tbl_s TO tbl UNLOCK TABLES 确定行拷贝chunk范围123select id from (select id from a where id&gt;=0 and id&lt;=3001 order by id asc limit 1000) select_osc_chunk order by id desc limit 1; 行拷贝in chunk1234567begin;insert ignore into `a`.`_a_gho` (`id`, `value`) (select `id`, `value` from `a`.`a` force index (`PRIMARY`) where (((`id` &gt; ?) or ((`id` = ?))) and ((`id` &lt; ?) or ((`id` = ?)))) lock in share mode )commit; 关键点async binlog worker如何判断所有数据变更已经完成binlog worker向changelog table发一行记录，在收到这个记录时，即表示完成 RENAME race condition with DMLmysql内部保证，LOCK TABLE后，如果有DML与RENAME并发操作，那么在UNLOCK TABLES时，RENAME一定获取最高优先级，即：RENAME一定会先执行。否则，会丢数据:1234567LOCK TABLE WRITEINSERT // blockedRENAME // blockedUNLOCK TABLEINERT // 如果INSERT先执行，那么它会插入原表RENAME // 原表被rename to tbl_old，刚才INSERT的数据丢失: 存放在了tbl_old LOCK, RENAME如果在一个mysql连接内执行LOCK; RENAME，那么会失败解决办法：创建2个mysql连接，分别执行LOCK和RENAME","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]},{"title":"两段锁 2PL","slug":"2PL","date":"2017-05-11T07:33:41.000Z","updated":"2017-05-16T08:51:42.000Z","comments":true,"path":"2017/05/11/2PL/","link":"","permalink":"http://funkygao.github.io/2017/05/11/2PL/","excerpt":"","keywords":[],"text":"事务开始后就处于加锁阶段，一直到执行ROLLBACK和COMMIT之前都是加锁阶段。ROLLBACK和COMMIT使事务进入解锁阶段 事务遵守两段锁协议是可串行化调度的充分条件，而不是必要条件 MS SQL Server默认采用2PL实现isolation，Oralce/PostgreSQL/MySQL InnoDB默认使用MVCC MySQL 2PL提供2种锁 shared(read) exclusive(write) 读写互斥，但读读不互斥 MySQL serialized isolation","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://funkygao.github.io/tags/database/"}]}]}